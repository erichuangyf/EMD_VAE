{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef50705",
   "metadata": {},
   "source": [
    "# Import and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43264681",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a505b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 02:14:22.046003: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import os\n",
    "\n",
    "from pfn_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45667e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "train, val, test = 0.6, 0.3, 0.1\n",
    "Phi_sizes, F_sizes = (256, 256, 256), (256, 256, 256)\n",
    "num_epoch = 500\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505a2894",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_dir = \"/global/home/users/yifengh3/VAE/vec_data/recon_data\"\n",
    "raw_b_signals = np.load(os.path.join(data_base_dir, \"fixed_reconstructed_B_signal_vector.npz\")) \n",
    "raw_hv_signals = np.load(os.path.join(data_base_dir, \"reconstructed_hv_vector.npz\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3813124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'recon', 'beta']\n"
     ]
    }
   ],
   "source": [
    "print(list(raw_b_signals.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5aa5d4",
   "metadata": {},
   "source": [
    "# Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25276321",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal1 = raw_b_signals[\"data\"]\n",
    "signal2 = raw_hv_signals[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b559b182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 02:14:24.781698: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-09-28 02:14:24.782755: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-09-28 02:14:24.816651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: Quadro RTX 6000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2022-09-28 02:14:24.816690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-09-28 02:14:24.817971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-09-28 02:14:24.818000: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-09-28 02:14:24.819474: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-09-28 02:14:24.819683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-09-28 02:14:24.821020: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-09-28 02:14:24.821825: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-09-28 02:14:24.824981: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-09-28 02:14:24.827590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-09-28 02:14:24.827996: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-28 02:14:24.829788: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-09-28 02:14:24.831086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: Quadro RTX 6000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2022-09-28 02:14:24.831109: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-09-28 02:14:24.831126: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-09-28 02:14:24.831137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-09-28 02:14:24.831148: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-09-28 02:14:24.831158: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-09-28 02:14:24.831167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-09-28 02:14:24.831177: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-09-28 02:14:24.831187: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-09-28 02:14:24.833597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-09-28 02:14:24.833642: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-09-28 02:14:25.348731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-09-28 02:14:25.348776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-09-28 02:14:25.348786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-09-28 02:14:25.352638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22475 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 02:14:25.780477: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-09-28 02:14:25.780920: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2994530000 Hz\n",
      "2022-09-28 02:14:26.170238: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 - 3s - loss: 9.1701 - acc: 0.8363 - val_loss: 0.4810 - val_acc: 0.9037\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.3377 - acc: 0.8992 - val_loss: 0.3212 - val_acc: 0.9123\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.2276 - acc: 0.9204 - val_loss: 0.1876 - val_acc: 0.9304\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2105 - acc: 0.9261 - val_loss: 0.2165 - val_acc: 0.9224\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1881 - acc: 0.9316 - val_loss: 0.1861 - val_acc: 0.9342\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1919 - acc: 0.9305 - val_loss: 0.2080 - val_acc: 0.9290\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1812 - acc: 0.9344 - val_loss: 0.1760 - val_acc: 0.9357\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1761 - acc: 0.9361 - val_loss: 0.1683 - val_acc: 0.9395\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1670 - acc: 0.9387 - val_loss: 0.1793 - val_acc: 0.9361\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1734 - acc: 0.9370 - val_loss: 0.1620 - val_acc: 0.9407\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1687 - acc: 0.9382 - val_loss: 0.1662 - val_acc: 0.9391\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1703 - acc: 0.9372 - val_loss: 0.1632 - val_acc: 0.9403\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1606 - acc: 0.9411 - val_loss: 0.1621 - val_acc: 0.9395\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1636 - acc: 0.9399 - val_loss: 0.1662 - val_acc: 0.9378\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1612 - acc: 0.9406 - val_loss: 0.1613 - val_acc: 0.9403\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1598 - acc: 0.9417 - val_loss: 0.1894 - val_acc: 0.9311\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1568 - acc: 0.9425 - val_loss: 0.1548 - val_acc: 0.9431\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1569 - acc: 0.9426 - val_loss: 0.1555 - val_acc: 0.9439\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1593 - acc: 0.9412 - val_loss: 0.1531 - val_acc: 0.9438\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1540 - acc: 0.9431 - val_loss: 0.1560 - val_acc: 0.9430\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1541 - acc: 0.9433 - val_loss: 0.1597 - val_acc: 0.9413\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1539 - acc: 0.9435 - val_loss: 0.1586 - val_acc: 0.9426\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1520 - acc: 0.9440 - val_loss: 0.1541 - val_acc: 0.9434\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1556 - acc: 0.9421 - val_loss: 0.1538 - val_acc: 0.9429\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1449 - acc: 0.9463 - val_loss: 0.1501 - val_acc: 0.9436\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1457 - acc: 0.9458 - val_loss: 0.1456 - val_acc: 0.9456\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1462 - acc: 0.9459 - val_loss: 0.1524 - val_acc: 0.9442\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1444 - acc: 0.9466 - val_loss: 0.1459 - val_acc: 0.9458\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1476 - acc: 0.9454 - val_loss: 0.1451 - val_acc: 0.9458\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1483 - acc: 0.9452 - val_loss: 0.1634 - val_acc: 0.9405\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1456 - acc: 0.9464 - val_loss: 0.1550 - val_acc: 0.9419\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1428 - acc: 0.9477 - val_loss: 0.1778 - val_acc: 0.9328\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1431 - acc: 0.9472 - val_loss: 0.1489 - val_acc: 0.9456\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1472 - acc: 0.9458 - val_loss: 0.1598 - val_acc: 0.9434\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1400 - acc: 0.9481 - val_loss: 0.1409 - val_acc: 0.9475\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1385 - acc: 0.9488 - val_loss: 0.1432 - val_acc: 0.9473\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1406 - acc: 0.9480 - val_loss: 0.1421 - val_acc: 0.9475\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1375 - acc: 0.9492 - val_loss: 0.1504 - val_acc: 0.9442\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1380 - acc: 0.9484 - val_loss: 0.1466 - val_acc: 0.9464\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1386 - acc: 0.9485 - val_loss: 0.1442 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1343 - acc: 0.9505 - val_loss: 0.1454 - val_acc: 0.9468\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1326 - acc: 0.9505 - val_loss: 0.1406 - val_acc: 0.9475\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1338 - acc: 0.9499 - val_loss: 0.1502 - val_acc: 0.9441\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1334 - acc: 0.9504 - val_loss: 0.1379 - val_acc: 0.9485\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1332 - acc: 0.9504 - val_loss: 0.1388 - val_acc: 0.9486\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1319 - acc: 0.9508 - val_loss: 0.1381 - val_acc: 0.9487\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1340 - acc: 0.9503 - val_loss: 0.1394 - val_acc: 0.9481\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1334 - acc: 0.9503 - val_loss: 0.1391 - val_acc: 0.9482\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1329 - acc: 0.9502 - val_loss: 0.1384 - val_acc: 0.9480\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1270 - acc: 0.9526 - val_loss: 0.1353 - val_acc: 0.9493\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.1277 - acc: 0.9523 - val_loss: 0.1349 - val_acc: 0.9499\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.1263 - acc: 0.9527 - val_loss: 0.1403 - val_acc: 0.9470\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.1267 - acc: 0.9527 - val_loss: 0.1364 - val_acc: 0.9489\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.1276 - acc: 0.9520 - val_loss: 0.1347 - val_acc: 0.9503\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.1264 - acc: 0.9529 - val_loss: 0.1395 - val_acc: 0.9480\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.1250 - acc: 0.9534 - val_loss: 0.1321 - val_acc: 0.9510\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.1233 - acc: 0.9541 - val_loss: 0.1487 - val_acc: 0.9453\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.1223 - acc: 0.9544 - val_loss: 0.1383 - val_acc: 0.9484\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.1229 - acc: 0.9538 - val_loss: 0.1344 - val_acc: 0.9504\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.1208 - acc: 0.9548 - val_loss: 0.1339 - val_acc: 0.9500\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.1209 - acc: 0.9548 - val_loss: 0.1339 - val_acc: 0.9509\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.1166 - acc: 0.9562 - val_loss: 0.1282 - val_acc: 0.9528\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.1159 - acc: 0.9565 - val_loss: 0.1292 - val_acc: 0.9521\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.1144 - acc: 0.9569 - val_loss: 0.1265 - val_acc: 0.9530\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.1144 - acc: 0.9571 - val_loss: 0.1345 - val_acc: 0.9500\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.1128 - acc: 0.9575 - val_loss: 0.1252 - val_acc: 0.9536\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.1119 - acc: 0.9581 - val_loss: 0.1339 - val_acc: 0.9505\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.1110 - acc: 0.9583 - val_loss: 0.1233 - val_acc: 0.9540\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.1101 - acc: 0.9589 - val_loss: 0.1338 - val_acc: 0.9509\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.1106 - acc: 0.9585 - val_loss: 0.1238 - val_acc: 0.9545\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.1087 - acc: 0.9592 - val_loss: 0.1206 - val_acc: 0.9552\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.1085 - acc: 0.9595 - val_loss: 0.1284 - val_acc: 0.9536\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.1076 - acc: 0.9600 - val_loss: 0.1219 - val_acc: 0.9555\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.1053 - acc: 0.9604 - val_loss: 0.1294 - val_acc: 0.9518\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.1067 - acc: 0.9601 - val_loss: 0.1237 - val_acc: 0.9552\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.1036 - acc: 0.9614 - val_loss: 0.1239 - val_acc: 0.9553\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.0996 - acc: 0.9628 - val_loss: 0.1178 - val_acc: 0.9571\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.0990 - acc: 0.9629 - val_loss: 0.1166 - val_acc: 0.9569\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.0989 - acc: 0.9628 - val_loss: 0.1202 - val_acc: 0.9564\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.0975 - acc: 0.9634 - val_loss: 0.1166 - val_acc: 0.9572\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.0970 - acc: 0.9636 - val_loss: 0.1182 - val_acc: 0.9566\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.0976 - acc: 0.9631 - val_loss: 0.1202 - val_acc: 0.9560\n",
      "Epoch 83/200\n",
      "198/198 - 2s - loss: 0.0958 - acc: 0.9640 - val_loss: 0.1224 - val_acc: 0.9546\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 - 2s - loss: 0.0927 - acc: 0.9651 - val_loss: 0.1172 - val_acc: 0.9568\n",
      "Epoch 85/200\n",
      "198/198 - 2s - loss: 0.0922 - acc: 0.9654 - val_loss: 0.1150 - val_acc: 0.9577\n",
      "Epoch 86/200\n",
      "198/198 - 2s - loss: 0.0916 - acc: 0.9656 - val_loss: 0.1166 - val_acc: 0.9571\n",
      "Epoch 87/200\n",
      "198/198 - 2s - loss: 0.0914 - acc: 0.9656 - val_loss: 0.1151 - val_acc: 0.9583\n",
      "Epoch 88/200\n",
      "198/198 - 2s - loss: 0.0907 - acc: 0.9662 - val_loss: 0.1164 - val_acc: 0.9580\n",
      "Epoch 89/200\n",
      "198/198 - 2s - loss: 0.0904 - acc: 0.9662 - val_loss: 0.1147 - val_acc: 0.9585\n",
      "Epoch 90/200\n",
      "198/198 - 2s - loss: 0.0901 - acc: 0.9662 - val_loss: 0.1143 - val_acc: 0.9580\n",
      "Epoch 91/200\n",
      "198/198 - 2s - loss: 0.0889 - acc: 0.9664 - val_loss: 0.1184 - val_acc: 0.9568\n",
      "Epoch 92/200\n",
      "198/198 - 2s - loss: 0.0887 - acc: 0.9665 - val_loss: 0.1178 - val_acc: 0.9570\n",
      "Epoch 93/200\n",
      "198/198 - 2s - loss: 0.0883 - acc: 0.9668 - val_loss: 0.1128 - val_acc: 0.9586\n",
      "Epoch 94/200\n",
      "198/198 - 2s - loss: 0.0873 - acc: 0.9671 - val_loss: 0.1139 - val_acc: 0.9587\n",
      "Epoch 95/200\n",
      "198/198 - 2s - loss: 0.0877 - acc: 0.9670 - val_loss: 0.1184 - val_acc: 0.9570\n",
      "Epoch 96/200\n",
      "198/198 - 2s - loss: 0.0872 - acc: 0.9672 - val_loss: 0.1136 - val_acc: 0.9588\n",
      "Epoch 97/200\n",
      "198/198 - 2s - loss: 0.0860 - acc: 0.9675 - val_loss: 0.1134 - val_acc: 0.9591\n",
      "Epoch 98/200\n",
      "198/198 - 2s - loss: 0.0857 - acc: 0.9680 - val_loss: 0.1137 - val_acc: 0.9588\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 99/200\n",
      "198/198 - 2s - loss: 0.0839 - acc: 0.9686 - val_loss: 0.1145 - val_acc: 0.9584\n",
      "Epoch 100/200\n",
      "198/198 - 2s - loss: 0.0835 - acc: 0.9689 - val_loss: 0.1128 - val_acc: 0.9593\n",
      "Epoch 101/200\n",
      "198/198 - 2s - loss: 0.0835 - acc: 0.9687 - val_loss: 0.1124 - val_acc: 0.9596\n",
      "Epoch 102/200\n",
      "198/198 - 2s - loss: 0.0829 - acc: 0.9689 - val_loss: 0.1163 - val_acc: 0.9581\n",
      "Epoch 103/200\n",
      "198/198 - 2s - loss: 0.0826 - acc: 0.9690 - val_loss: 0.1133 - val_acc: 0.9594\n",
      "Epoch 104/200\n",
      "198/198 - 2s - loss: 0.0830 - acc: 0.9691 - val_loss: 0.1159 - val_acc: 0.9581\n",
      "Epoch 105/200\n",
      "198/198 - 2s - loss: 0.0821 - acc: 0.9693 - val_loss: 0.1192 - val_acc: 0.9573\n",
      "Epoch 106/200\n",
      "198/198 - 2s - loss: 0.0818 - acc: 0.9693 - val_loss: 0.1131 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 1.5848932274101303e-05.\n",
      "Epoch 107/200\n",
      "198/198 - 2s - loss: 0.0808 - acc: 0.9697 - val_loss: 0.1129 - val_acc: 0.9595\n",
      "Epoch 108/200\n",
      "198/198 - 2s - loss: 0.0805 - acc: 0.9701 - val_loss: 0.1127 - val_acc: 0.9596\n",
      "Epoch 109/200\n",
      "198/198 - 2s - loss: 0.0807 - acc: 0.9698 - val_loss: 0.1191 - val_acc: 0.9573\n",
      "Epoch 110/200\n",
      "198/198 - 2s - loss: 0.0803 - acc: 0.9699 - val_loss: 0.1127 - val_acc: 0.9600\n",
      "Epoch 111/200\n",
      "198/198 - 2s - loss: 0.0799 - acc: 0.9701 - val_loss: 0.1128 - val_acc: 0.9599\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1.0000000608891671e-05.\n",
      "Epoch 00111: early stopping\n"
     ]
    }
   ],
   "source": [
    "pfn_original, hist1, original_training_data = train_pfn(signal1, signal2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e472d473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PFN AUC: 0.9925298176618182\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/home/users/yifengh3/VAE/EMD_VAE/PFN/pfn_utils.py:147: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  plt.plot(pfn_tp, 1/pfn_fp, '-', color='black', label='PFN')\n",
      "/global/home/users/yifengh3/VAE/EMD_VAE/PFN/pfn_utils.py:148: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  plt.plot(pfn_tp, 1/pfn_tp, '-', color='red', label='random')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "analysis(pfn_original,original_training_data[1][2], original_training_data[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe8f2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_preds = pfn_original.predict(original_training_data[1][2], batch_size=10000)\n",
    "original_roc_info = roc_curve(original_training_data[1][-1][:,1], ori_preds[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d35f4",
   "metadata": {},
   "source": [
    "# Test of recon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5594be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_betas = raw_b_signals[\"beta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a706d425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14285715 -0.2857143  -0.42857143 -0.5714286  -0.71428573 -0.85714287\n",
      " -1.         -1.1428572  -1.2857143  -1.4285715  -1.5714285  -1.7142857\n",
      " -1.8571428  -2.         -2.142857   -2.2857144  -2.4285715  -2.5714285\n",
      " -2.7142856  -2.857143   -3.         -3.142857   -3.2857144  -3.4285715\n",
      " -3.5714285  -3.7142856  -3.857143   -4.         -4.142857   -4.285714\n",
      " -4.428571   -4.571429   -4.714286   -4.857143  ]\n"
     ]
    }
   ],
   "source": [
    "beta_idx = np.logical_and(log_betas<0, log_betas>-5)\n",
    "new_betas = log_betas[beta_idx]\n",
    "new_betas = new_betas\n",
    "print(new_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a5c948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_1_recons = raw_b_signals[\"recon\"][beta_idx]\n",
    "signal_2_recons = raw_hv_signals[\"recon\"][beta_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929e7557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 11.1246 - acc: 0.8396 - val_loss: 0.9260 - val_acc: 0.8724\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.5318 - acc: 0.8936 - val_loss: 0.2945 - val_acc: 0.9239\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.3193 - acc: 0.9078 - val_loss: 0.2732 - val_acc: 0.9299\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2251 - acc: 0.9215 - val_loss: 0.2626 - val_acc: 0.9096\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1982 - acc: 0.9276 - val_loss: 0.2361 - val_acc: 0.9187\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.2079 - acc: 0.9248 - val_loss: 0.1789 - val_acc: 0.9367\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1868 - acc: 0.9314 - val_loss: 0.1715 - val_acc: 0.9375\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.4470 - acc: 0.9039 - val_loss: 0.4621 - val_acc: 0.8781\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.2063 - acc: 0.9265 - val_loss: 0.1833 - val_acc: 0.9353\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1746 - acc: 0.9356 - val_loss: 0.1725 - val_acc: 0.9375\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1702 - acc: 0.9376 - val_loss: 0.2080 - val_acc: 0.9236\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1720 - acc: 0.9374 - val_loss: 0.1683 - val_acc: 0.9404\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1680 - acc: 0.9386 - val_loss: 0.1806 - val_acc: 0.9355\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1686 - acc: 0.9380 - val_loss: 0.1682 - val_acc: 0.9395\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1669 - acc: 0.9391 - val_loss: 0.1639 - val_acc: 0.9415\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1654 - acc: 0.9393 - val_loss: 0.1700 - val_acc: 0.9376\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1698 - acc: 0.9379 - val_loss: 0.1690 - val_acc: 0.9400\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1658 - acc: 0.9392 - val_loss: 0.1667 - val_acc: 0.9408\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1669 - acc: 0.9389 - val_loss: 0.1856 - val_acc: 0.9331\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1684 - acc: 0.9387 - val_loss: 0.1804 - val_acc: 0.9326\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1634 - acc: 0.9398 - val_loss: 0.1681 - val_acc: 0.9393\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1639 - acc: 0.9395 - val_loss: 0.1830 - val_acc: 0.9326\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1618 - acc: 0.9412 - val_loss: 0.1794 - val_acc: 0.9358\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1621 - acc: 0.9403 - val_loss: 0.1880 - val_acc: 0.9328\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1606 - acc: 0.9412 - val_loss: 0.1623 - val_acc: 0.9417\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1641 - acc: 0.9394 - val_loss: 0.1658 - val_acc: 0.9388\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1622 - acc: 0.9403 - val_loss: 0.1649 - val_acc: 0.9423\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1643 - acc: 0.9402 - val_loss: 0.1737 - val_acc: 0.9385\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1625 - acc: 0.9408 - val_loss: 0.1702 - val_acc: 0.9377\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1632 - acc: 0.9402 - val_loss: 0.1748 - val_acc: 0.9370\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1596 - acc: 0.9414 - val_loss: 0.1625 - val_acc: 0.9424\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1594 - acc: 0.9415 - val_loss: 0.1607 - val_acc: 0.9424\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1599 - acc: 0.9413 - val_loss: 0.1602 - val_acc: 0.9431\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1608 - acc: 0.9412 - val_loss: 0.1610 - val_acc: 0.9420\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1623 - acc: 0.9401 - val_loss: 0.1635 - val_acc: 0.9416\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1614 - acc: 0.9411 - val_loss: 0.1603 - val_acc: 0.9420\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1622 - acc: 0.9409 - val_loss: 0.1635 - val_acc: 0.9426\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1601 - acc: 0.9412 - val_loss: 0.1615 - val_acc: 0.9422\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1588 - acc: 0.9418 - val_loss: 0.1587 - val_acc: 0.9431\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1586 - acc: 0.9417 - val_loss: 0.1603 - val_acc: 0.9429\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1587 - acc: 0.9421 - val_loss: 0.1648 - val_acc: 0.9411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1581 - acc: 0.9422 - val_loss: 0.1632 - val_acc: 0.9418\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1600 - acc: 0.9415 - val_loss: 0.1657 - val_acc: 0.9407\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1584 - acc: 0.9423 - val_loss: 0.1666 - val_acc: 0.9393\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1563 - acc: 0.9427 - val_loss: 0.1641 - val_acc: 0.9405\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1549 - acc: 0.9428 - val_loss: 0.1647 - val_acc: 0.9419\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1553 - acc: 0.9430 - val_loss: 0.1612 - val_acc: 0.9426\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1561 - acc: 0.9427 - val_loss: 0.1586 - val_acc: 0.9426\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1576 - acc: 0.9418 - val_loss: 0.1691 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1543 - acc: 0.9431 - val_loss: 0.1585 - val_acc: 0.9433\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.1534 - acc: 0.9435 - val_loss: 0.1602 - val_acc: 0.9430\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.1542 - acc: 0.9433 - val_loss: 0.1591 - val_acc: 0.9429\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.1541 - acc: 0.9436 - val_loss: 0.1691 - val_acc: 0.9409\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.1548 - acc: 0.9432 - val_loss: 0.1586 - val_acc: 0.9422\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.1541 - acc: 0.9434 - val_loss: 0.1576 - val_acc: 0.9433\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.1535 - acc: 0.9438 - val_loss: 0.1578 - val_acc: 0.9434\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.1546 - acc: 0.9429 - val_loss: 0.1560 - val_acc: 0.9442\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.1535 - acc: 0.9440 - val_loss: 0.1605 - val_acc: 0.9428\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.1552 - acc: 0.9431 - val_loss: 0.1561 - val_acc: 0.9440\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.1539 - acc: 0.9433 - val_loss: 0.1581 - val_acc: 0.9434\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.1537 - acc: 0.9435 - val_loss: 0.1565 - val_acc: 0.9435\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.1539 - acc: 0.9436 - val_loss: 0.1626 - val_acc: 0.9417\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.1524 - acc: 0.9437 - val_loss: 0.1559 - val_acc: 0.9438\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.1527 - acc: 0.9439 - val_loss: 0.1566 - val_acc: 0.9431\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.1522 - acc: 0.9441 - val_loss: 0.1552 - val_acc: 0.9442\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.1520 - acc: 0.9439 - val_loss: 0.1591 - val_acc: 0.9429\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.1515 - acc: 0.9445 - val_loss: 0.1586 - val_acc: 0.9426\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.1522 - acc: 0.9441 - val_loss: 0.1552 - val_acc: 0.9444\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.1516 - acc: 0.9446 - val_loss: 0.1599 - val_acc: 0.9425\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.1536 - acc: 0.9438 - val_loss: 0.1601 - val_acc: 0.9425\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.1511 - acc: 0.9444 - val_loss: 0.1541 - val_acc: 0.9450\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.1505 - acc: 0.9446 - val_loss: 0.1551 - val_acc: 0.9446\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.1506 - acc: 0.9447 - val_loss: 0.1547 - val_acc: 0.9444\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.1502 - acc: 0.9447 - val_loss: 0.1648 - val_acc: 0.9401\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.1516 - acc: 0.9444 - val_loss: 0.1580 - val_acc: 0.9435\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.1505 - acc: 0.9447 - val_loss: 0.1549 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.1500 - acc: 0.9449 - val_loss: 0.1539 - val_acc: 0.9449\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.1499 - acc: 0.9447 - val_loss: 0.1566 - val_acc: 0.9438\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.1492 - acc: 0.9453 - val_loss: 0.1579 - val_acc: 0.9433\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.1494 - acc: 0.9451 - val_loss: 0.1559 - val_acc: 0.9440\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.1493 - acc: 0.9451 - val_loss: 0.1543 - val_acc: 0.9446\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.1491 - acc: 0.9453 - val_loss: 0.1546 - val_acc: 0.9446\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 1.5848932274101303e-05.\n",
      "Epoch 83/200\n",
      "198/198 - 2s - loss: 0.1485 - acc: 0.9454 - val_loss: 0.1554 - val_acc: 0.9439\n",
      "Epoch 84/200\n",
      "198/198 - 2s - loss: 0.1485 - acc: 0.9452 - val_loss: 0.1579 - val_acc: 0.9440\n",
      "Epoch 85/200\n",
      "198/198 - 2s - loss: 0.1483 - acc: 0.9454 - val_loss: 0.1539 - val_acc: 0.9451\n",
      "Epoch 86/200\n",
      "198/198 - 2s - loss: 0.1485 - acc: 0.9453 - val_loss: 0.1547 - val_acc: 0.9444\n",
      "Epoch 87/200\n",
      "198/198 - 2s - loss: 0.1484 - acc: 0.9455 - val_loss: 0.1539 - val_acc: 0.9449\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.0000000608891671e-05.\n",
      "Epoch 88/200\n",
      "198/198 - 2s - loss: 0.1479 - acc: 0.9455 - val_loss: 0.1541 - val_acc: 0.9447\n",
      "Epoch 89/200\n",
      "198/198 - 2s - loss: 0.1481 - acc: 0.9454 - val_loss: 0.1532 - val_acc: 0.9450\n",
      "Epoch 90/200\n",
      "198/198 - 2s - loss: 0.1481 - acc: 0.9456 - val_loss: 0.1551 - val_acc: 0.9444\n",
      "Epoch 91/200\n",
      "198/198 - 2s - loss: 0.1479 - acc: 0.9455 - val_loss: 0.1532 - val_acc: 0.9450\n",
      "Epoch 92/200\n",
      "198/198 - 2s - loss: 0.1479 - acc: 0.9454 - val_loss: 0.1530 - val_acc: 0.9452\n",
      "Epoch 93/200\n",
      "198/198 - 2s - loss: 0.1478 - acc: 0.9456 - val_loss: 0.1531 - val_acc: 0.9451\n",
      "Epoch 94/200\n",
      "198/198 - 2s - loss: 0.1477 - acc: 0.9456 - val_loss: 0.1537 - val_acc: 0.9452\n",
      "Epoch 95/200\n",
      "198/198 - 2s - loss: 0.1476 - acc: 0.9456 - val_loss: 0.1529 - val_acc: 0.9454\n",
      "Epoch 96/200\n",
      "198/198 - 2s - loss: 0.1476 - acc: 0.9459 - val_loss: 0.1533 - val_acc: 0.9451\n",
      "Epoch 97/200\n",
      "198/198 - 2s - loss: 0.1476 - acc: 0.9456 - val_loss: 0.1540 - val_acc: 0.9450\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 98/200\n",
      "198/198 - 2s - loss: 0.1476 - acc: 0.9459 - val_loss: 0.1533 - val_acc: 0.9451\n",
      "Epoch 99/200\n",
      "198/198 - 2s - loss: 0.1476 - acc: 0.9457 - val_loss: 0.1531 - val_acc: 0.9454\n",
      "Epoch 100/200\n",
      "198/198 - 2s - loss: 0.1475 - acc: 0.9457 - val_loss: 0.1535 - val_acc: 0.9452\n",
      "Epoch 101/200\n",
      "198/198 - 2s - loss: 0.1474 - acc: 0.9460 - val_loss: 0.1534 - val_acc: 0.9452\n",
      "Epoch 102/200\n",
      "198/198 - 2s - loss: 0.1476 - acc: 0.9457 - val_loss: 0.1526 - val_acc: 0.9454\n",
      "Epoch 103/200\n",
      "198/198 - 2s - loss: 0.1474 - acc: 0.9457 - val_loss: 0.1534 - val_acc: 0.9454\n",
      "Epoch 104/200\n",
      "198/198 - 2s - loss: 0.1476 - acc: 0.9459 - val_loss: 0.1526 - val_acc: 0.9453\n",
      "Epoch 105/200\n",
      "198/198 - 2s - loss: 0.1473 - acc: 0.9456 - val_loss: 0.1539 - val_acc: 0.9449\n",
      "Epoch 106/200\n",
      "198/198 - 2s - loss: 0.1473 - acc: 0.9458 - val_loss: 0.1531 - val_acc: 0.9453\n",
      "Epoch 107/200\n",
      "198/198 - 2s - loss: 0.1472 - acc: 0.9460 - val_loss: 0.1532 - val_acc: 0.9453\n",
      "Epoch 108/200\n",
      "198/198 - 2s - loss: 0.1473 - acc: 0.9460 - val_loss: 0.1540 - val_acc: 0.9455\n",
      "Epoch 109/200\n",
      "198/198 - 2s - loss: 0.1472 - acc: 0.9460 - val_loss: 0.1525 - val_acc: 0.9452\n",
      "Epoch 110/200\n",
      "198/198 - 2s - loss: 0.1471 - acc: 0.9458 - val_loss: 0.1525 - val_acc: 0.9455\n",
      "Epoch 111/200\n",
      "198/198 - 2s - loss: 0.1473 - acc: 0.9457 - val_loss: 0.1535 - val_acc: 0.9453\n",
      "Epoch 112/200\n",
      "198/198 - 2s - loss: 0.1471 - acc: 0.9461 - val_loss: 0.1525 - val_acc: 0.9453\n",
      "Epoch 113/200\n",
      "198/198 - 2s - loss: 0.1471 - acc: 0.9460 - val_loss: 0.1533 - val_acc: 0.9450\n",
      "Epoch 114/200\n",
      "198/198 - 2s - loss: 0.1470 - acc: 0.9459 - val_loss: 0.1534 - val_acc: 0.9454\n",
      "Epoch 115/200\n",
      "198/198 - 2s - loss: 0.1471 - acc: 0.9458 - val_loss: 0.1548 - val_acc: 0.9449\n",
      "Epoch 116/200\n",
      "198/198 - 2s - loss: 0.1471 - acc: 0.9458 - val_loss: 0.1532 - val_acc: 0.9452\n",
      "Epoch 117/200\n",
      "198/198 - 2s - loss: 0.1470 - acc: 0.9460 - val_loss: 0.1523 - val_acc: 0.9457\n",
      "Epoch 118/200\n",
      "198/198 - 2s - loss: 0.1472 - acc: 0.9459 - val_loss: 0.1529 - val_acc: 0.9449\n",
      "Epoch 119/200\n",
      "198/198 - 2s - loss: 0.1468 - acc: 0.9462 - val_loss: 0.1526 - val_acc: 0.9457\n",
      "Epoch 120/200\n",
      "198/198 - 2s - loss: 0.1469 - acc: 0.9460 - val_loss: 0.1534 - val_acc: 0.9453\n",
      "Epoch 121/200\n",
      "198/198 - 2s - loss: 0.1469 - acc: 0.9460 - val_loss: 0.1529 - val_acc: 0.9454\n",
      "Epoch 122/200\n",
      "198/198 - 2s - loss: 0.1469 - acc: 0.9459 - val_loss: 0.1542 - val_acc: 0.9450\n",
      "Epoch 123/200\n",
      "198/198 - 2s - loss: 0.1468 - acc: 0.9459 - val_loss: 0.1532 - val_acc: 0.9455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/200\n",
      "198/198 - 2s - loss: 0.1466 - acc: 0.9461 - val_loss: 0.1532 - val_acc: 0.9452\n",
      "Epoch 125/200\n",
      "198/198 - 2s - loss: 0.1469 - acc: 0.9461 - val_loss: 0.1528 - val_acc: 0.9455\n",
      "Epoch 126/200\n",
      "198/198 - 2s - loss: 0.1468 - acc: 0.9460 - val_loss: 0.1533 - val_acc: 0.9451\n",
      "Epoch 127/200\n",
      "198/198 - 2s - loss: 0.1467 - acc: 0.9459 - val_loss: 0.1527 - val_acc: 0.9453\n",
      "Epoch 00127: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 9.1385 - acc: 0.8323 - val_loss: 0.8466 - val_acc: 0.9023\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.5516 - acc: 0.8975 - val_loss: 0.2807 - val_acc: 0.9302\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.2507 - acc: 0.9197 - val_loss: 0.4187 - val_acc: 0.8424\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2220 - acc: 0.9279 - val_loss: 0.1536 - val_acc: 0.9452\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1780 - acc: 0.9381 - val_loss: 0.1883 - val_acc: 0.9388\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1711 - acc: 0.9398 - val_loss: 0.1776 - val_acc: 0.9389\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1752 - acc: 0.9392 - val_loss: 0.1620 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1540 - acc: 0.9455 - val_loss: 0.1474 - val_acc: 0.9478\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1563 - acc: 0.9442 - val_loss: 0.1913 - val_acc: 0.9264\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1571 - acc: 0.9444 - val_loss: 0.1587 - val_acc: 0.9417\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1561 - acc: 0.9445 - val_loss: 0.1505 - val_acc: 0.9475\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1555 - acc: 0.9450 - val_loss: 0.1652 - val_acc: 0.9401\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1506 - acc: 0.9465 - val_loss: 0.1415 - val_acc: 0.9506\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1501 - acc: 0.9468 - val_loss: 0.1624 - val_acc: 0.9411\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1591 - acc: 0.9437 - val_loss: 0.1431 - val_acc: 0.9490\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1474 - acc: 0.9477 - val_loss: 0.1502 - val_acc: 0.9468\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1468 - acc: 0.9478 - val_loss: 0.1413 - val_acc: 0.9496\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1442 - acc: 0.9483 - val_loss: 0.1405 - val_acc: 0.9500\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 1.9503 - acc: 0.8868 - val_loss: 0.1981 - val_acc: 0.9256\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1636 - acc: 0.9411 - val_loss: 0.1468 - val_acc: 0.9480\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1472 - acc: 0.9473 - val_loss: 0.1443 - val_acc: 0.9488\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1438 - acc: 0.9486 - val_loss: 0.1406 - val_acc: 0.9494\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1436 - acc: 0.9485 - val_loss: 0.1400 - val_acc: 0.9503\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1403 - acc: 0.9496 - val_loss: 0.1367 - val_acc: 0.9512\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1397 - acc: 0.9499 - val_loss: 0.1369 - val_acc: 0.9511\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1424 - acc: 0.9490 - val_loss: 0.1429 - val_acc: 0.9484\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1388 - acc: 0.9503 - val_loss: 0.1386 - val_acc: 0.9497\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1389 - acc: 0.9500 - val_loss: 0.1373 - val_acc: 0.9510\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1383 - acc: 0.9506 - val_loss: 0.1346 - val_acc: 0.9519\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1369 - acc: 0.9507 - val_loss: 0.1367 - val_acc: 0.9510\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1395 - acc: 0.9498 - val_loss: 0.1467 - val_acc: 0.9469\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1366 - acc: 0.9510 - val_loss: 0.1335 - val_acc: 0.9522\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1379 - acc: 0.9503 - val_loss: 0.1490 - val_acc: 0.9467\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1388 - acc: 0.9501 - val_loss: 0.1515 - val_acc: 0.9449\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1370 - acc: 0.9508 - val_loss: 0.1680 - val_acc: 0.9373\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1383 - acc: 0.9502 - val_loss: 0.1350 - val_acc: 0.9517\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1361 - acc: 0.9513 - val_loss: 0.1343 - val_acc: 0.9513\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1326 - acc: 0.9523 - val_loss: 0.1363 - val_acc: 0.9509\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1321 - acc: 0.9521 - val_loss: 0.1311 - val_acc: 0.9531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1334 - acc: 0.9518 - val_loss: 0.1337 - val_acc: 0.9523\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1336 - acc: 0.9516 - val_loss: 0.1463 - val_acc: 0.9479\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1335 - acc: 0.9518 - val_loss: 0.1303 - val_acc: 0.9532\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1346 - acc: 0.9515 - val_loss: 0.1427 - val_acc: 0.9478\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1363 - acc: 0.9507 - val_loss: 0.1334 - val_acc: 0.9517\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1320 - acc: 0.9523 - val_loss: 0.1312 - val_acc: 0.9531\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1325 - acc: 0.9522 - val_loss: 0.1324 - val_acc: 0.9529\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1326 - acc: 0.9521 - val_loss: 0.1328 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1297 - acc: 0.9529 - val_loss: 0.1298 - val_acc: 0.9533\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1308 - acc: 0.9526 - val_loss: 0.1298 - val_acc: 0.9535\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1297 - acc: 0.9532 - val_loss: 0.1395 - val_acc: 0.9486\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.1308 - acc: 0.9525 - val_loss: 0.1304 - val_acc: 0.9536\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.1307 - acc: 0.9525 - val_loss: 0.1312 - val_acc: 0.9530\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.1304 - acc: 0.9525 - val_loss: 0.1292 - val_acc: 0.9537\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.1288 - acc: 0.9530 - val_loss: 0.1338 - val_acc: 0.9524\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.1295 - acc: 0.9527 - val_loss: 0.1284 - val_acc: 0.9537\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.1282 - acc: 0.9533 - val_loss: 0.1315 - val_acc: 0.9525\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.1298 - acc: 0.9532 - val_loss: 0.1340 - val_acc: 0.9530\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.1294 - acc: 0.9529 - val_loss: 0.1286 - val_acc: 0.9535\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.1295 - acc: 0.9530 - val_loss: 0.1337 - val_acc: 0.9520\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.1286 - acc: 0.9533 - val_loss: 0.1549 - val_acc: 0.9427\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.1275 - acc: 0.9537 - val_loss: 0.1330 - val_acc: 0.9511\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.1270 - acc: 0.9535 - val_loss: 0.1286 - val_acc: 0.9535\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.1253 - acc: 0.9545 - val_loss: 0.1284 - val_acc: 0.9538\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.1253 - acc: 0.9543 - val_loss: 0.1262 - val_acc: 0.9546\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.1270 - acc: 0.9535 - val_loss: 0.1283 - val_acc: 0.9538\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.1251 - acc: 0.9544 - val_loss: 0.1295 - val_acc: 0.9540\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.1259 - acc: 0.9543 - val_loss: 0.1283 - val_acc: 0.9538\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.1261 - acc: 0.9539 - val_loss: 0.1334 - val_acc: 0.9522\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.1255 - acc: 0.9541 - val_loss: 0.1487 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.1236 - acc: 0.9549 - val_loss: 0.1258 - val_acc: 0.9547\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.1230 - acc: 0.9549 - val_loss: 0.1300 - val_acc: 0.9530\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.1228 - acc: 0.9549 - val_loss: 0.1267 - val_acc: 0.9544\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.1225 - acc: 0.9550 - val_loss: 0.1276 - val_acc: 0.9539\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.1229 - acc: 0.9549 - val_loss: 0.1271 - val_acc: 0.9544\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.1226 - acc: 0.9549 - val_loss: 0.1273 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.1211 - acc: 0.9558 - val_loss: 0.1255 - val_acc: 0.9552\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.1211 - acc: 0.9556 - val_loss: 0.1252 - val_acc: 0.9549\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.1200 - acc: 0.9559 - val_loss: 0.1250 - val_acc: 0.9552\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.1199 - acc: 0.9559 - val_loss: 0.1250 - val_acc: 0.9551\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.1200 - acc: 0.9556 - val_loss: 0.1255 - val_acc: 0.9549\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.1207 - acc: 0.9556 - val_loss: 0.1246 - val_acc: 0.9554\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.1196 - acc: 0.9556 - val_loss: 0.1246 - val_acc: 0.9552\n",
      "Epoch 83/200\n",
      "198/198 - 2s - loss: 0.1197 - acc: 0.9557 - val_loss: 0.1269 - val_acc: 0.9544\n",
      "Epoch 84/200\n",
      "198/198 - 2s - loss: 0.1196 - acc: 0.9558 - val_loss: 0.1253 - val_acc: 0.9550\n",
      "Epoch 85/200\n",
      "198/198 - 2s - loss: 0.1193 - acc: 0.9557 - val_loss: 0.1251 - val_acc: 0.9552\n",
      "Epoch 86/200\n",
      "198/198 - 2s - loss: 0.1191 - acc: 0.9559 - val_loss: 0.1251 - val_acc: 0.9555\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 87/200\n",
      "198/198 - 2s - loss: 0.1173 - acc: 0.9566 - val_loss: 0.1244 - val_acc: 0.9553\n",
      "Epoch 88/200\n",
      "198/198 - 2s - loss: 0.1174 - acc: 0.9565 - val_loss: 0.1249 - val_acc: 0.9552\n",
      "Epoch 89/200\n",
      "198/198 - 2s - loss: 0.1170 - acc: 0.9564 - val_loss: 0.1255 - val_acc: 0.9551\n",
      "Epoch 90/200\n",
      "198/198 - 2s - loss: 0.1175 - acc: 0.9566 - val_loss: 0.1241 - val_acc: 0.9556\n",
      "Epoch 91/200\n",
      "198/198 - 2s - loss: 0.1169 - acc: 0.9568 - val_loss: 0.1255 - val_acc: 0.9549\n",
      "Epoch 92/200\n",
      "198/198 - 2s - loss: 0.1169 - acc: 0.9565 - val_loss: 0.1241 - val_acc: 0.9553\n",
      "Epoch 93/200\n",
      "198/198 - 2s - loss: 0.1169 - acc: 0.9568 - val_loss: 0.1265 - val_acc: 0.9546\n",
      "Epoch 94/200\n",
      "198/198 - 2s - loss: 0.1173 - acc: 0.9566 - val_loss: 0.1267 - val_acc: 0.9544\n",
      "Epoch 95/200\n",
      "198/198 - 2s - loss: 0.1167 - acc: 0.9568 - val_loss: 0.1254 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 96/200\n",
      "198/198 - 2s - loss: 0.1156 - acc: 0.9571 - val_loss: 0.1238 - val_acc: 0.9554\n",
      "Epoch 97/200\n",
      "198/198 - 2s - loss: 0.1154 - acc: 0.9571 - val_loss: 0.1251 - val_acc: 0.9548\n",
      "Epoch 98/200\n",
      "198/198 - 2s - loss: 0.1154 - acc: 0.9571 - val_loss: 0.1241 - val_acc: 0.9555\n",
      "Epoch 99/200\n",
      "198/198 - 2s - loss: 0.1154 - acc: 0.9571 - val_loss: 0.1247 - val_acc: 0.9553\n",
      "Epoch 100/200\n",
      "198/198 - 2s - loss: 0.1152 - acc: 0.9571 - val_loss: 0.1254 - val_acc: 0.9550\n",
      "Epoch 101/200\n",
      "198/198 - 2s - loss: 0.1154 - acc: 0.9572 - val_loss: 0.1237 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 102/200\n",
      "198/198 - 2s - loss: 0.1143 - acc: 0.9574 - val_loss: 0.1239 - val_acc: 0.9555\n",
      "Epoch 103/200\n",
      "198/198 - 2s - loss: 0.1142 - acc: 0.9575 - val_loss: 0.1242 - val_acc: 0.9555\n",
      "Epoch 104/200\n",
      "198/198 - 2s - loss: 0.1141 - acc: 0.9574 - val_loss: 0.1239 - val_acc: 0.9554\n",
      "Epoch 105/200\n",
      "198/198 - 2s - loss: 0.1143 - acc: 0.9574 - val_loss: 0.1239 - val_acc: 0.9555\n",
      "Epoch 106/200\n",
      "198/198 - 2s - loss: 0.1141 - acc: 0.9575 - val_loss: 0.1239 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 1.5848932274101303e-05.\n",
      "Epoch 107/200\n",
      "198/198 - 2s - loss: 0.1139 - acc: 0.9576 - val_loss: 0.1236 - val_acc: 0.9558\n",
      "Epoch 108/200\n",
      "198/198 - 2s - loss: 0.1136 - acc: 0.9577 - val_loss: 0.1235 - val_acc: 0.9556\n",
      "Epoch 109/200\n",
      "198/198 - 2s - loss: 0.1136 - acc: 0.9576 - val_loss: 0.1235 - val_acc: 0.9558\n",
      "Epoch 110/200\n",
      "198/198 - 2s - loss: 0.1134 - acc: 0.9576 - val_loss: 0.1238 - val_acc: 0.9554\n",
      "Epoch 111/200\n",
      "198/198 - 2s - loss: 0.1134 - acc: 0.9576 - val_loss: 0.1255 - val_acc: 0.9547\n",
      "Epoch 112/200\n",
      "198/198 - 2s - loss: 0.1133 - acc: 0.9576 - val_loss: 0.1236 - val_acc: 0.9555\n",
      "Epoch 113/200\n",
      "198/198 - 2s - loss: 0.1135 - acc: 0.9576 - val_loss: 0.1241 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00113: ReduceLROnPlateau reducing learning rate to 1.0000000608891671e-05.\n",
      "Epoch 114/200\n",
      "198/198 - 2s - loss: 0.1130 - acc: 0.9579 - val_loss: 0.1250 - val_acc: 0.9552\n",
      "Epoch 115/200\n",
      "198/198 - 2s - loss: 0.1129 - acc: 0.9578 - val_loss: 0.1237 - val_acc: 0.9555\n",
      "Epoch 116/200\n",
      "198/198 - 2s - loss: 0.1129 - acc: 0.9578 - val_loss: 0.1243 - val_acc: 0.9553\n",
      "Epoch 117/200\n",
      "198/198 - 2s - loss: 0.1128 - acc: 0.9580 - val_loss: 0.1236 - val_acc: 0.9555\n",
      "Epoch 118/200\n",
      "198/198 - 2s - loss: 0.1127 - acc: 0.9580 - val_loss: 0.1238 - val_acc: 0.9554\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 119/200\n",
      "198/198 - 2s - loss: 0.1128 - acc: 0.9578 - val_loss: 0.1235 - val_acc: 0.9556\n",
      "Epoch 00119: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 8.4831 - acc: 0.8464 - val_loss: 0.6014 - val_acc: 0.8727\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.5015 - acc: 0.9010 - val_loss: 0.5402 - val_acc: 0.9003\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.3391 - acc: 0.9142 - val_loss: 0.2614 - val_acc: 0.9324\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.1879 - acc: 0.9377 - val_loss: 0.1652 - val_acc: 0.9455\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1975 - acc: 0.9348 - val_loss: 0.1581 - val_acc: 0.9431\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1579 - acc: 0.9446 - val_loss: 0.1480 - val_acc: 0.9462\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1600 - acc: 0.9438 - val_loss: 0.1451 - val_acc: 0.9479\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1483 - acc: 0.9477 - val_loss: 0.1442 - val_acc: 0.9507\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1608 - acc: 0.9436 - val_loss: 0.2358 - val_acc: 0.9222\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1665 - acc: 0.9423 - val_loss: 0.1391 - val_acc: 0.9513\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1428 - acc: 0.9492 - val_loss: 0.1357 - val_acc: 0.9526\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1424 - acc: 0.9493 - val_loss: 0.1404 - val_acc: 0.9510\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1472 - acc: 0.9480 - val_loss: 0.1666 - val_acc: 0.9383\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1409 - acc: 0.9497 - val_loss: 0.1335 - val_acc: 0.9530\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1436 - acc: 0.9490 - val_loss: 0.1494 - val_acc: 0.9470\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1412 - acc: 0.9499 - val_loss: 0.1623 - val_acc: 0.9432\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1394 - acc: 0.9502 - val_loss: 0.1337 - val_acc: 0.9536\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1413 - acc: 0.9494 - val_loss: 0.1682 - val_acc: 0.9395\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1345 - acc: 0.9521 - val_loss: 0.1306 - val_acc: 0.9543\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1346 - acc: 0.9519 - val_loss: 0.1338 - val_acc: 0.9524\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1354 - acc: 0.9517 - val_loss: 0.1322 - val_acc: 0.9529\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1408 - acc: 0.9493 - val_loss: 0.1373 - val_acc: 0.9512\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1349 - acc: 0.9520 - val_loss: 0.1406 - val_acc: 0.9527\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1357 - acc: 0.9515 - val_loss: 0.1404 - val_acc: 0.9493\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1281 - acc: 0.9538 - val_loss: 0.1247 - val_acc: 0.9555\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1270 - acc: 0.9541 - val_loss: 0.1520 - val_acc: 0.9437\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1295 - acc: 0.9534 - val_loss: 0.1321 - val_acc: 0.9515\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1280 - acc: 0.9536 - val_loss: 0.1320 - val_acc: 0.9529\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1319 - acc: 0.9527 - val_loss: 0.1381 - val_acc: 0.9506\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1303 - acc: 0.9528 - val_loss: 0.1280 - val_acc: 0.9554\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1237 - acc: 0.9557 - val_loss: 0.1340 - val_acc: 0.9541\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1233 - acc: 0.9554 - val_loss: 0.1273 - val_acc: 0.9546\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1248 - acc: 0.9551 - val_loss: 0.1269 - val_acc: 0.9547\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1237 - acc: 0.9555 - val_loss: 0.1257 - val_acc: 0.9557\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1239 - acc: 0.9552 - val_loss: 0.1525 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 00035: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 - 2s - loss: 10.4423 - acc: 0.8272 - val_loss: 0.6270 - val_acc: 0.8883\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.4024 - acc: 0.9029 - val_loss: 0.4409 - val_acc: 0.8553\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.2527 - acc: 0.9243 - val_loss: 1.2100 - val_acc: 0.7786\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2297 - acc: 0.9297 - val_loss: 0.1646 - val_acc: 0.9426\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1688 - acc: 0.9404 - val_loss: 0.1520 - val_acc: 0.9463\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1709 - acc: 0.9396 - val_loss: 0.1590 - val_acc: 0.9429\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1624 - acc: 0.9428 - val_loss: 0.1426 - val_acc: 0.9488\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1595 - acc: 0.9433 - val_loss: 0.1463 - val_acc: 0.9503\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1637 - acc: 0.9421 - val_loss: 0.1648 - val_acc: 0.9439\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1588 - acc: 0.9439 - val_loss: 0.1475 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1486 - acc: 0.9468 - val_loss: 0.1542 - val_acc: 0.9453\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1508 - acc: 0.9464 - val_loss: 0.1518 - val_acc: 0.9441\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1373 - acc: 0.9510 - val_loss: 0.1348 - val_acc: 0.9518\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1364 - acc: 0.9506 - val_loss: 0.1357 - val_acc: 0.9519\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1368 - acc: 0.9512 - val_loss: 0.2038 - val_acc: 0.9250\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1396 - acc: 0.9501 - val_loss: 0.1402 - val_acc: 0.9495\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1476 - acc: 0.9478 - val_loss: 0.1445 - val_acc: 0.9485\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1417 - acc: 0.9495 - val_loss: 0.1597 - val_acc: 0.9458\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1330 - acc: 0.9525 - val_loss: 0.1327 - val_acc: 0.9527\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1324 - acc: 0.9524 - val_loss: 0.1326 - val_acc: 0.9523\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1359 - acc: 0.9513 - val_loss: 0.1336 - val_acc: 0.9518\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1330 - acc: 0.9526 - val_loss: 0.1347 - val_acc: 0.9521\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1336 - acc: 0.9519 - val_loss: 0.1365 - val_acc: 0.9513\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1340 - acc: 0.9520 - val_loss: 0.1356 - val_acc: 0.9513\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1339 - acc: 0.9521 - val_loss: 0.1350 - val_acc: 0.9520\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1313 - acc: 0.9526 - val_loss: 0.1305 - val_acc: 0.9535\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1290 - acc: 0.9537 - val_loss: 0.1292 - val_acc: 0.9539\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1300 - acc: 0.9533 - val_loss: 0.1515 - val_acc: 0.9454\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1323 - acc: 0.9522 - val_loss: 0.1357 - val_acc: 0.9505\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1308 - acc: 0.9531 - val_loss: 0.1290 - val_acc: 0.9542\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1324 - acc: 0.9527 - val_loss: 0.1310 - val_acc: 0.9533\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1313 - acc: 0.9533 - val_loss: 0.1362 - val_acc: 0.9510\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1312 - acc: 0.9528 - val_loss: 0.1398 - val_acc: 0.9501\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1315 - acc: 0.9531 - val_loss: 0.1583 - val_acc: 0.9426\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1324 - acc: 0.9527 - val_loss: 0.1478 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1265 - acc: 0.9544 - val_loss: 0.1303 - val_acc: 0.9531\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1275 - acc: 0.9543 - val_loss: 0.1387 - val_acc: 0.9499\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1267 - acc: 0.9548 - val_loss: 0.1313 - val_acc: 0.9537\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1271 - acc: 0.9545 - val_loss: 0.1325 - val_acc: 0.9520\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1294 - acc: 0.9535 - val_loss: 0.1305 - val_acc: 0.9533\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 00040: early stopping\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for signal_1, signal_2 in zip(signal_1_recons,signal_2_recons):\n",
    "    result.append(train_pfn(signal_1, signal_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3697547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruct PFN AUC(beta = -1.14):0.9851579449233723\n",
      "reconstruct PFN AUC(beta = -1.86):0.9893310627342753\n",
      "reconstruct PFN AUC(beta = -2.57):0.9899304178316589\n",
      "reconstruct PFN AUC(beta = -3.29):0.988313560883888\n"
     ]
    }
   ],
   "source": [
    "roc_res = []\n",
    "for i, (pfn, hist, data) in enumerate(result):\n",
    "    preds = pfn.predict(data[1][2], batch_size=10000)\n",
    "    pfn_fp, pfn_tp, threshs = roc_curve(data[1][-1][:,1], preds[:,1])\n",
    "    roc_res.append([pfn_fp, pfn_tp, threshs])\n",
    "    auc = roc_auc_score(data[1][-1][:,1], preds[:,1])\n",
    "    print('reconstruct PFN AUC(beta = {:.2f}):{}'.format(new_betas[i],auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4fc3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"auc.npz\", beta= new_betas, roc=roc_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
