{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef50705",
   "metadata": {},
   "source": [
    "# Import and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43264681",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a505b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 11:51:30.255438: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import os\n",
    "\n",
    "from pfn_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45667e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "train, val, test = 0.6, 0.3, 0.1\n",
    "Phi_sizes, F_sizes = (256, 256, 256), (256, 256, 256)\n",
    "num_epoch = 500\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505a2894",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_dir = \"/global/home/users/yifengh3/VAE/vec_data/recon_data\"\n",
    "raw_b_signals = np.load(os.path.join(data_base_dir, \"pfn_bsignal.npz\")) \n",
    "raw_hv_signals = np.load(os.path.join(data_base_dir, \"pfn_hv_signal.npz\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3813124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'recon', 'beta']\n"
     ]
    }
   ],
   "source": [
    "print(list(raw_b_signals.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5aa5d4",
   "metadata": {},
   "source": [
    "# Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25276321",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal1 = raw_b_signals[\"data\"]\n",
    "signal2 = raw_hv_signals[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b559b182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 11:51:33.882794: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-09-28 11:51:33.885686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-09-28 11:51:33.910287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: Quadro RTX 6000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2022-09-28 11:51:33.910311: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-09-28 11:51:33.926008: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-09-28 11:51:33.926040: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-09-28 11:51:33.938448: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-09-28 11:51:33.941278: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-09-28 11:51:33.963480: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-09-28 11:51:33.966497: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-09-28 11:51:34.008830: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-09-28 11:51:34.011830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-09-28 11:51:34.012310: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-28 11:51:34.014784: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-09-28 11:51:34.016431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: Quadro RTX 6000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2022-09-28 11:51:34.016461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-09-28 11:51:34.016483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-09-28 11:51:34.016499: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-09-28 11:51:34.016514: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-09-28 11:51:34.016528: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-09-28 11:51:34.016543: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-09-28 11:51:34.016557: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-09-28 11:51:34.016572: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-09-28 11:51:34.019636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-09-28 11:51:34.019899: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-09-28 11:51:34.923380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-09-28 11:51:34.923424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-09-28 11:51:34.923435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-09-28 11:51:34.927442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22006 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 11:51:35.417182: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-09-28 11:51:35.417815: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2994530000 Hz\n",
      "2022-09-28 11:51:35.821836: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 - 3s - loss: 11.3828 - acc: 0.8336 - val_loss: 1.9430 - val_acc: 0.7559\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.5936 - acc: 0.8859 - val_loss: 0.2351 - val_acc: 0.9300\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.3810 - acc: 0.8990 - val_loss: 0.2096 - val_acc: 0.9298\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2463 - acc: 0.9162 - val_loss: 0.1933 - val_acc: 0.9357\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.2127 - acc: 0.9238 - val_loss: 0.1886 - val_acc: 0.9329\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.2052 - acc: 0.9259 - val_loss: 0.1815 - val_acc: 0.9341\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1853 - acc: 0.9319 - val_loss: 0.1701 - val_acc: 0.9368\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1820 - acc: 0.9335 - val_loss: 0.1650 - val_acc: 0.9406\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1829 - acc: 0.9327 - val_loss: 0.1658 - val_acc: 0.9386\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1720 - acc: 0.9369 - val_loss: 0.1685 - val_acc: 0.9393\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1817 - acc: 0.9333 - val_loss: 0.1649 - val_acc: 0.9419\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1648 - acc: 0.9393 - val_loss: 0.1687 - val_acc: 0.9365\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1726 - acc: 0.9371 - val_loss: 0.1619 - val_acc: 0.9405\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1639 - acc: 0.9396 - val_loss: 0.1562 - val_acc: 0.9431\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1658 - acc: 0.9391 - val_loss: 0.1594 - val_acc: 0.9419\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1611 - acc: 0.9403 - val_loss: 0.1541 - val_acc: 0.9442\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1613 - acc: 0.9405 - val_loss: 0.1590 - val_acc: 0.9431\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1632 - acc: 0.9392 - val_loss: 0.1651 - val_acc: 0.9404\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1639 - acc: 0.9393 - val_loss: 0.1666 - val_acc: 0.9380\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1542 - acc: 0.9426 - val_loss: 0.1755 - val_acc: 0.9382\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1573 - acc: 0.9418 - val_loss: 0.1692 - val_acc: 0.9405\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1472 - acc: 0.9453 - val_loss: 0.1509 - val_acc: 0.9450\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1494 - acc: 0.9442 - val_loss: 0.1494 - val_acc: 0.9461\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1480 - acc: 0.9448 - val_loss: 0.1665 - val_acc: 0.9396\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1473 - acc: 0.9450 - val_loss: 0.1476 - val_acc: 0.9466\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1462 - acc: 0.9456 - val_loss: 0.1458 - val_acc: 0.9475\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1488 - acc: 0.9447 - val_loss: 0.1519 - val_acc: 0.9449\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1510 - acc: 0.9435 - val_loss: 0.1577 - val_acc: 0.9426\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1489 - acc: 0.9448 - val_loss: 0.1463 - val_acc: 0.9465\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1500 - acc: 0.9443 - val_loss: 0.1451 - val_acc: 0.9478\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1456 - acc: 0.9460 - val_loss: 0.1502 - val_acc: 0.9446\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1486 - acc: 0.9451 - val_loss: 0.1515 - val_acc: 0.9450\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1483 - acc: 0.9450 - val_loss: 0.1484 - val_acc: 0.9469\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1473 - acc: 0.9453 - val_loss: 0.1556 - val_acc: 0.9414\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1462 - acc: 0.9459 - val_loss: 0.1582 - val_acc: 0.9430\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1415 - acc: 0.9476 - val_loss: 0.1472 - val_acc: 0.9473\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1407 - acc: 0.9473 - val_loss: 0.1422 - val_acc: 0.9474\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1399 - acc: 0.9480 - val_loss: 0.1441 - val_acc: 0.9479\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1402 - acc: 0.9479 - val_loss: 0.1436 - val_acc: 0.9477\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1391 - acc: 0.9481 - val_loss: 0.1476 - val_acc: 0.9465\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1410 - acc: 0.9473 - val_loss: 0.1431 - val_acc: 0.9478\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1407 - acc: 0.9476 - val_loss: 0.1411 - val_acc: 0.9488\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1389 - acc: 0.9485 - val_loss: 0.1469 - val_acc: 0.9480\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1409 - acc: 0.9473 - val_loss: 0.1448 - val_acc: 0.9473\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1397 - acc: 0.9483 - val_loss: 0.1605 - val_acc: 0.9421\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1390 - acc: 0.9481 - val_loss: 0.1419 - val_acc: 0.9497\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1386 - acc: 0.9487 - val_loss: 0.1423 - val_acc: 0.9492\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1332 - acc: 0.9501 - val_loss: 0.1424 - val_acc: 0.9478\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1317 - acc: 0.9508 - val_loss: 0.1371 - val_acc: 0.9506\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1339 - acc: 0.9500 - val_loss: 0.1359 - val_acc: 0.9501\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.1319 - acc: 0.9505 - val_loss: 0.1355 - val_acc: 0.9507\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.1308 - acc: 0.9510 - val_loss: 0.1377 - val_acc: 0.9495\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.1314 - acc: 0.9511 - val_loss: 0.1440 - val_acc: 0.9473\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.1297 - acc: 0.9517 - val_loss: 0.1387 - val_acc: 0.9496\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.1304 - acc: 0.9516 - val_loss: 0.1355 - val_acc: 0.9507\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.1271 - acc: 0.9525 - val_loss: 0.1322 - val_acc: 0.9518\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.1282 - acc: 0.9525 - val_loss: 0.1329 - val_acc: 0.9512\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.1287 - acc: 0.9519 - val_loss: 0.1315 - val_acc: 0.9518\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.1272 - acc: 0.9522 - val_loss: 0.1332 - val_acc: 0.9520\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.1275 - acc: 0.9522 - val_loss: 0.1322 - val_acc: 0.9517\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.1239 - acc: 0.9536 - val_loss: 0.1522 - val_acc: 0.9425\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.1240 - acc: 0.9534 - val_loss: 0.1268 - val_acc: 0.9535\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.1231 - acc: 0.9539 - val_loss: 0.1395 - val_acc: 0.9496\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.1204 - acc: 0.9549 - val_loss: 0.1270 - val_acc: 0.9531\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.1193 - acc: 0.9553 - val_loss: 0.1276 - val_acc: 0.9532\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.1167 - acc: 0.9562 - val_loss: 0.1279 - val_acc: 0.9541\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.1174 - acc: 0.9558 - val_loss: 0.1223 - val_acc: 0.9555\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.1143 - acc: 0.9570 - val_loss: 0.1282 - val_acc: 0.9529\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.1139 - acc: 0.9573 - val_loss: 0.1211 - val_acc: 0.9559\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.1138 - acc: 0.9576 - val_loss: 0.1200 - val_acc: 0.9553\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.1083 - acc: 0.9593 - val_loss: 0.1162 - val_acc: 0.9577\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.1070 - acc: 0.9596 - val_loss: 0.1179 - val_acc: 0.9576\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.1088 - acc: 0.9592 - val_loss: 0.1122 - val_acc: 0.9585\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.1078 - acc: 0.9596 - val_loss: 0.1125 - val_acc: 0.9585\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.1052 - acc: 0.9605 - val_loss: 0.1155 - val_acc: 0.9569\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.1009 - acc: 0.9617 - val_loss: 0.1144 - val_acc: 0.9582\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.1000 - acc: 0.9626 - val_loss: 0.1100 - val_acc: 0.9595\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.0996 - acc: 0.9624 - val_loss: 0.1174 - val_acc: 0.9571\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.0983 - acc: 0.9627 - val_loss: 0.1266 - val_acc: 0.9524\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.0982 - acc: 0.9631 - val_loss: 0.1184 - val_acc: 0.9556\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.0958 - acc: 0.9636 - val_loss: 0.1196 - val_acc: 0.9556\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.0961 - acc: 0.9639 - val_loss: 0.1055 - val_acc: 0.9613\n",
      "Epoch 83/200\n",
      "198/198 - 2s - loss: 0.0925 - acc: 0.9653 - val_loss: 0.1077 - val_acc: 0.9609\n",
      "Epoch 84/200\n",
      "198/198 - 2s - loss: 0.0932 - acc: 0.9648 - val_loss: 0.1099 - val_acc: 0.9600\n",
      "Epoch 85/200\n",
      "198/198 - 2s - loss: 0.0926 - acc: 0.9650 - val_loss: 0.1104 - val_acc: 0.9595\n",
      "Epoch 86/200\n",
      "198/198 - 2s - loss: 0.0897 - acc: 0.9660 - val_loss: 0.1047 - val_acc: 0.9620\n",
      "Epoch 87/200\n",
      "198/198 - 2s - loss: 0.0915 - acc: 0.9655 - val_loss: 0.1145 - val_acc: 0.9596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/200\n",
      "198/198 - 2s - loss: 0.0879 - acc: 0.9672 - val_loss: 0.1039 - val_acc: 0.9632\n",
      "Epoch 89/200\n",
      "198/198 - 2s - loss: 0.0881 - acc: 0.9664 - val_loss: 0.1029 - val_acc: 0.9625\n",
      "Epoch 90/200\n",
      "198/198 - 2s - loss: 0.0872 - acc: 0.9670 - val_loss: 0.1089 - val_acc: 0.9610\n",
      "Epoch 91/200\n",
      "198/198 - 2s - loss: 0.0882 - acc: 0.9668 - val_loss: 0.1052 - val_acc: 0.9617\n",
      "Epoch 92/200\n",
      "198/198 - 2s - loss: 0.0860 - acc: 0.9675 - val_loss: 0.1049 - val_acc: 0.9617\n",
      "Epoch 93/200\n",
      "198/198 - 2s - loss: 0.0834 - acc: 0.9686 - val_loss: 0.1084 - val_acc: 0.9624\n",
      "Epoch 94/200\n",
      "198/198 - 2s - loss: 0.0830 - acc: 0.9685 - val_loss: 0.1072 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 95/200\n",
      "198/198 - 2s - loss: 0.0769 - acc: 0.9708 - val_loss: 0.1092 - val_acc: 0.9613\n",
      "Epoch 96/200\n",
      "198/198 - 2s - loss: 0.0746 - acc: 0.9718 - val_loss: 0.1010 - val_acc: 0.9635\n",
      "Epoch 97/200\n",
      "198/198 - 2s - loss: 0.0738 - acc: 0.9723 - val_loss: 0.1042 - val_acc: 0.9637\n",
      "Epoch 98/200\n",
      "198/198 - 2s - loss: 0.0750 - acc: 0.9716 - val_loss: 0.1010 - val_acc: 0.9631\n",
      "Epoch 99/200\n",
      "198/198 - 2s - loss: 0.0733 - acc: 0.9722 - val_loss: 0.1024 - val_acc: 0.9641\n",
      "Epoch 100/200\n",
      "198/198 - 2s - loss: 0.0719 - acc: 0.9727 - val_loss: 0.1045 - val_acc: 0.9632\n",
      "Epoch 101/200\n",
      "198/198 - 2s - loss: 0.0714 - acc: 0.9728 - val_loss: 0.1092 - val_acc: 0.9612\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 102/200\n",
      "198/198 - 2s - loss: 0.0686 - acc: 0.9741 - val_loss: 0.1019 - val_acc: 0.9642\n",
      "Epoch 103/200\n",
      "198/198 - 2s - loss: 0.0656 - acc: 0.9751 - val_loss: 0.1045 - val_acc: 0.9633\n",
      "Epoch 104/200\n",
      "198/198 - 2s - loss: 0.0656 - acc: 0.9754 - val_loss: 0.1169 - val_acc: 0.9603\n",
      "Epoch 105/200\n",
      "198/198 - 2s - loss: 0.0654 - acc: 0.9752 - val_loss: 0.1024 - val_acc: 0.9644\n",
      "Epoch 106/200\n",
      "198/198 - 2s - loss: 0.0642 - acc: 0.9756 - val_loss: 0.1051 - val_acc: 0.9638\n",
      "\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 107/200\n",
      "198/198 - 2s - loss: 0.0613 - acc: 0.9769 - val_loss: 0.1041 - val_acc: 0.9649\n",
      "Epoch 108/200\n",
      "198/198 - 2s - loss: 0.0603 - acc: 0.9773 - val_loss: 0.1020 - val_acc: 0.9646\n",
      "Epoch 00108: early stopping\n"
     ]
    }
   ],
   "source": [
    "pfn_original, hist1, original_training_data = train_pfn(signal1, signal2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e472d473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PFN AUC: 0.9934976726430119\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/home/users/yifengh3/VAE/EMD_VAE/PFN/pfn_utils.py:147: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  plt.plot(pfn_tp, 1/pfn_fp, '-', color='black', label='PFN')\n",
      "/global/home/users/yifengh3/VAE/EMD_VAE/PFN/pfn_utils.py:148: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  plt.plot(pfn_tp, 1/pfn_tp, '-', color='red', label='random')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "analysis(pfn_original,original_training_data[1][2], original_training_data[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe8f2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_preds = pfn_original.predict(original_training_data[1][2], batch_size=10000)\n",
    "original_roc_info = roc_curve(original_training_data[1][-1][:,1], ori_preds[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d35f4",
   "metadata": {},
   "source": [
    "# Test of recon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5594be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_betas = raw_b_signals[\"beta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a706d425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.85714287  0.71428573  0.5714286   0.42857143  0.2857143\n",
      "  0.14285715  0.         -0.14285715 -0.2857143  -0.42857143 -0.5714286\n",
      " -0.71428573 -0.85714287 -1.         -1.1428572  -1.2857143  -1.4285715\n",
      " -1.5714285  -1.7142857  -1.8571428  -2.         -2.142857   -2.2857144\n",
      " -2.4285715  -2.5714285  -2.7142856  -2.857143   -3.         -3.142857\n",
      " -3.2857144  -3.4285715  -3.5714285  -3.7142856  -3.857143   -4.\n",
      " -4.142857   -4.285714   -4.428571   -4.571429   -4.714286   -4.857143  ]\n"
     ]
    }
   ],
   "source": [
    "# beta_idx = np.logical_and(log_betas<10, log_betas>-5)\n",
    "# new_betas = log_betas[beta_idx]\n",
    "# new_betas = new_betas\n",
    "# print(new_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a5c948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_1_recons = raw_b_signals[\"recon\"]\n",
    "signal_2_recons = raw_hv_signals[\"recon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929e7557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 13.7847 - acc: 0.6735 - val_loss: 0.8701 - val_acc: 0.7511\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.6487 - acc: 0.7101 - val_loss: 0.6225 - val_acc: 0.6713\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.5439 - acc: 0.7281 - val_loss: 0.5165 - val_acc: 0.7246\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.4888 - acc: 0.7429 - val_loss: 0.5494 - val_acc: 0.7074\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.4895 - acc: 0.7426 - val_loss: 0.4751 - val_acc: 0.7473\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.4980 - acc: 0.7378 - val_loss: 0.4809 - val_acc: 0.7519\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.4893 - acc: 0.7432 - val_loss: 0.4830 - val_acc: 0.7419\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 1.5065 - acc: 0.7086 - val_loss: 0.5022 - val_acc: 0.7299\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.4780 - acc: 0.7494 - val_loss: 0.4721 - val_acc: 0.7518\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.4747 - acc: 0.7515 - val_loss: 0.4717 - val_acc: 0.7529\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.4770 - acc: 0.7506 - val_loss: 0.5095 - val_acc: 0.7128\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.4753 - acc: 0.7520 - val_loss: 0.4864 - val_acc: 0.7458\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.4735 - acc: 0.7535 - val_loss: 0.4759 - val_acc: 0.7488\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.4743 - acc: 0.7525 - val_loss: 0.4748 - val_acc: 0.7455\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.4738 - acc: 0.7537 - val_loss: 0.4708 - val_acc: 0.7537\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.4732 - acc: 0.7523 - val_loss: 0.4740 - val_acc: 0.7532\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.4736 - acc: 0.7524 - val_loss: 0.4725 - val_acc: 0.7537\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.4733 - acc: 0.7531 - val_loss: 0.4716 - val_acc: 0.7524\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.4752 - acc: 0.7521 - val_loss: 0.4782 - val_acc: 0.7473\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.4731 - acc: 0.7533 - val_loss: 0.4773 - val_acc: 0.7416\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.4724 - acc: 0.7543 - val_loss: 0.4716 - val_acc: 0.7512\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.4722 - acc: 0.7543 - val_loss: 0.4712 - val_acc: 0.7536\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.4732 - acc: 0.7535 - val_loss: 0.4754 - val_acc: 0.7537\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.4735 - acc: 0.7533 - val_loss: 0.4801 - val_acc: 0.7475\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.4732 - acc: 0.7530 - val_loss: 0.4736 - val_acc: 0.7522\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 00025: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 10.0449 - acc: 0.6788 - val_loss: 0.5060 - val_acc: 0.7282\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.5986 - acc: 0.7185 - val_loss: 0.4742 - val_acc: 0.7536\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.5113 - acc: 0.7319 - val_loss: 0.4935 - val_acc: 0.7413\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.5100 - acc: 0.7311 - val_loss: 0.5252 - val_acc: 0.7205\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.4893 - acc: 0.7421 - val_loss: 0.4714 - val_acc: 0.7563\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.4823 - acc: 0.7469 - val_loss: 0.4846 - val_acc: 0.7429\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.4825 - acc: 0.7448 - val_loss: 0.4693 - val_acc: 0.7557\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.4815 - acc: 0.7468 - val_loss: 0.4736 - val_acc: 0.7498\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.4792 - acc: 0.7479 - val_loss: 0.4745 - val_acc: 0.7507\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.4809 - acc: 0.7469 - val_loss: 0.4736 - val_acc: 0.7558\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.4832 - acc: 0.7456 - val_loss: 0.4781 - val_acc: 0.7552\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.4777 - acc: 0.7493 - val_loss: 0.4724 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.4756 - acc: 0.7512 - val_loss: 0.4783 - val_acc: 0.7478\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.4762 - acc: 0.7509 - val_loss: 0.4698 - val_acc: 0.7549\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.4748 - acc: 0.7516 - val_loss: 0.4689 - val_acc: 0.7573\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.4758 - acc: 0.7519 - val_loss: 0.4686 - val_acc: 0.7573\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.4760 - acc: 0.7517 - val_loss: 0.4728 - val_acc: 0.7524\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.4759 - acc: 0.7507 - val_loss: 0.4711 - val_acc: 0.7550\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 1.3105 - acc: 0.7158 - val_loss: 0.5505 - val_acc: 0.7099\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.4787 - acc: 0.7492 - val_loss: 0.4698 - val_acc: 0.7554\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.4769 - acc: 0.7501 - val_loss: 0.4691 - val_acc: 0.7544\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.4743 - acc: 0.7519 - val_loss: 0.4689 - val_acc: 0.7572\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.4741 - acc: 0.7512 - val_loss: 0.4695 - val_acc: 0.7540\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.4755 - acc: 0.7509 - val_loss: 0.4740 - val_acc: 0.7495\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.4748 - acc: 0.7519 - val_loss: 0.4692 - val_acc: 0.7538\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.4738 - acc: 0.7528 - val_loss: 0.4723 - val_acc: 0.7515\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 00026: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 12.2588 - acc: 0.6780 - val_loss: 1.7864 - val_acc: 0.7505\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 1.1122 - acc: 0.6999 - val_loss: 0.4908 - val_acc: 0.7415\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.5282 - acc: 0.7276 - val_loss: 0.4764 - val_acc: 0.7538\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.4980 - acc: 0.7396 - val_loss: 0.4753 - val_acc: 0.7463\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.5077 - acc: 0.7329 - val_loss: 0.4717 - val_acc: 0.7537\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.4872 - acc: 0.7435 - val_loss: 0.4743 - val_acc: 0.7542\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.4838 - acc: 0.7460 - val_loss: 0.4711 - val_acc: 0.7549\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.4900 - acc: 0.7411 - val_loss: 0.5282 - val_acc: 0.7075\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.4854 - acc: 0.7445 - val_loss: 0.4777 - val_acc: 0.7480\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 1.1887 - acc: 0.7309 - val_loss: 13.5670 - val_acc: 0.5730\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.8092 - acc: 0.7337 - val_loss: 0.4737 - val_acc: 0.7509\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.4834 - acc: 0.7459 - val_loss: 0.4702 - val_acc: 0.7542\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.4757 - acc: 0.7510 - val_loss: 0.4787 - val_acc: 0.7479\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.4768 - acc: 0.7514 - val_loss: 0.4715 - val_acc: 0.7523\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.4794 - acc: 0.7482 - val_loss: 0.4894 - val_acc: 0.7407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.4774 - acc: 0.7502 - val_loss: 0.4861 - val_acc: 0.7362\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.4748 - acc: 0.7523 - val_loss: 0.4982 - val_acc: 0.7374\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.4740 - acc: 0.7529 - val_loss: 0.4720 - val_acc: 0.7513\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.4742 - acc: 0.7531 - val_loss: 0.4693 - val_acc: 0.7535\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.4732 - acc: 0.7534 - val_loss: 0.4704 - val_acc: 0.7538\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.4731 - acc: 0.7532 - val_loss: 0.4753 - val_acc: 0.7509\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.4741 - acc: 0.7528 - val_loss: 0.4694 - val_acc: 0.7550\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.4739 - acc: 0.7530 - val_loss: 0.4735 - val_acc: 0.7511\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.4742 - acc: 0.7531 - val_loss: 0.4711 - val_acc: 0.7532\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.4721 - acc: 0.7548 - val_loss: 0.4755 - val_acc: 0.7506\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.4726 - acc: 0.7538 - val_loss: 0.4757 - val_acc: 0.7486\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.4729 - acc: 0.7541 - val_loss: 0.4740 - val_acc: 0.7504\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.4724 - acc: 0.7537 - val_loss: 0.4778 - val_acc: 0.7490\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.4727 - acc: 0.7541 - val_loss: 0.4730 - val_acc: 0.7511\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 00029: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 9.2516 - acc: 0.6773 - val_loss: 1.2869 - val_acc: 0.7066\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.6618 - acc: 0.7190 - val_loss: 0.5105 - val_acc: 0.7259\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.5088 - acc: 0.7355 - val_loss: 0.4787 - val_acc: 0.7369\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.4911 - acc: 0.7394 - val_loss: 0.4711 - val_acc: 0.7548\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.5005 - acc: 0.7359 - val_loss: 0.4726 - val_acc: 0.7548\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.4796 - acc: 0.7470 - val_loss: 0.4836 - val_acc: 0.7545\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.4803 - acc: 0.7478 - val_loss: 0.4873 - val_acc: 0.7439\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.4802 - acc: 0.7471 - val_loss: 0.5036 - val_acc: 0.7205\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.4772 - acc: 0.7491 - val_loss: 0.4718 - val_acc: 0.7546\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.4735 - acc: 0.7520 - val_loss: 0.4715 - val_acc: 0.7540\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.4734 - acc: 0.7519 - val_loss: 0.4773 - val_acc: 0.7515\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.4763 - acc: 0.7502 - val_loss: 0.4786 - val_acc: 0.7498\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.4737 - acc: 0.7527 - val_loss: 0.4915 - val_acc: 0.7402\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.4743 - acc: 0.7517 - val_loss: 0.4722 - val_acc: 0.7558\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 00014: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 10.6842 - acc: 0.6784 - val_loss: 1.7685 - val_acc: 0.6013\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.7995 - acc: 0.7045 - val_loss: 0.4766 - val_acc: 0.7528\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.5052 - acc: 0.7350 - val_loss: 0.5571 - val_acc: 0.6875\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.4945 - acc: 0.7392 - val_loss: 0.4926 - val_acc: 0.7415\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.4887 - acc: 0.7420 - val_loss: 0.4994 - val_acc: 0.7229\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.4936 - acc: 0.7409 - val_loss: 0.5148 - val_acc: 0.7247\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.4870 - acc: 0.7431 - val_loss: 0.4822 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.4774 - acc: 0.7497 - val_loss: 0.4732 - val_acc: 0.7534\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.4760 - acc: 0.7504 - val_loss: 0.4763 - val_acc: 0.7534\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.4770 - acc: 0.7506 - val_loss: 0.4724 - val_acc: 0.7543\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.4766 - acc: 0.7511 - val_loss: 0.4755 - val_acc: 0.7542\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.4771 - acc: 0.7502 - val_loss: 0.4712 - val_acc: 0.7546\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.4761 - acc: 0.7505 - val_loss: 0.4707 - val_acc: 0.7542\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.4741 - acc: 0.7520 - val_loss: 0.4753 - val_acc: 0.7531\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.4741 - acc: 0.7519 - val_loss: 0.4762 - val_acc: 0.7500\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 2.1057 - acc: 0.7187 - val_loss: 0.4722 - val_acc: 0.7542\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.4795 - acc: 0.7486 - val_loss: 0.4778 - val_acc: 0.7517\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.4765 - acc: 0.7502 - val_loss: 0.4739 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.4739 - acc: 0.7528 - val_loss: 0.4735 - val_acc: 0.7539\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.4730 - acc: 0.7530 - val_loss: 0.4731 - val_acc: 0.7521\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.4739 - acc: 0.7530 - val_loss: 0.4732 - val_acc: 0.7534\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.4740 - acc: 0.7521 - val_loss: 0.4933 - val_acc: 0.7374\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.4731 - acc: 0.7536 - val_loss: 0.4711 - val_acc: 0.7540\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 00023: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 9.5262 - acc: 0.6769 - val_loss: 4.7789 - val_acc: 0.5551\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 1.0198 - acc: 0.6916 - val_loss: 0.5224 - val_acc: 0.7424\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.5242 - acc: 0.7299 - val_loss: 0.5759 - val_acc: 0.6882\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.5520 - acc: 0.7232 - val_loss: 0.5676 - val_acc: 0.6824\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.4890 - acc: 0.7432 - val_loss: 0.4694 - val_acc: 0.7566\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.4874 - acc: 0.7439 - val_loss: 0.5392 - val_acc: 0.6898\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.4899 - acc: 0.7417 - val_loss: 0.5587 - val_acc: 0.7179\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.4873 - acc: 0.7435 - val_loss: 0.5429 - val_acc: 0.6944\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.4904 - acc: 0.7433 - val_loss: 0.4747 - val_acc: 0.7554\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 1.9003 - acc: 0.7175 - val_loss: 0.4700 - val_acc: 0.7545\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.4744 - acc: 0.7528 - val_loss: 0.4746 - val_acc: 0.7445\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.4734 - acc: 0.7534 - val_loss: 0.4700 - val_acc: 0.7570\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.4749 - acc: 0.7516 - val_loss: 0.4742 - val_acc: 0.7505\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.4739 - acc: 0.7532 - val_loss: 0.4700 - val_acc: 0.7537\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.4737 - acc: 0.7517 - val_loss: 0.4692 - val_acc: 0.7568\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.4738 - acc: 0.7524 - val_loss: 0.4710 - val_acc: 0.7546\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.4737 - acc: 0.7532 - val_loss: 0.4710 - val_acc: 0.7560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.4731 - acc: 0.7533 - val_loss: 0.4766 - val_acc: 0.7487\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.4748 - acc: 0.7520 - val_loss: 0.4729 - val_acc: 0.7565\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.4734 - acc: 0.7541 - val_loss: 0.4694 - val_acc: 0.7567\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.4724 - acc: 0.7536 - val_loss: 0.4690 - val_acc: 0.7563\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.4721 - acc: 0.7541 - val_loss: 0.4802 - val_acc: 0.7484\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.4724 - acc: 0.7541 - val_loss: 0.4694 - val_acc: 0.7566\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.4723 - acc: 0.7546 - val_loss: 0.4715 - val_acc: 0.7553\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.4727 - acc: 0.7534 - val_loss: 0.4695 - val_acc: 0.7565\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.4730 - acc: 0.7532 - val_loss: 0.4694 - val_acc: 0.7563\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.4721 - acc: 0.7547 - val_loss: 0.4751 - val_acc: 0.7534\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.4718 - acc: 0.7548 - val_loss: 0.4688 - val_acc: 0.7568\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.4725 - acc: 0.7536 - val_loss: 0.4698 - val_acc: 0.7569\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.4722 - acc: 0.7541 - val_loss: 0.4714 - val_acc: 0.7533\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.4721 - acc: 0.7539 - val_loss: 0.4694 - val_acc: 0.7567\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.4724 - acc: 0.7542 - val_loss: 0.4709 - val_acc: 0.7570\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.4718 - acc: 0.7545 - val_loss: 0.4721 - val_acc: 0.7518\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.4718 - acc: 0.7540 - val_loss: 0.4692 - val_acc: 0.7568\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.4719 - acc: 0.7537 - val_loss: 0.4722 - val_acc: 0.7530\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.4721 - acc: 0.7541 - val_loss: 0.4694 - val_acc: 0.7566\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.4720 - acc: 0.7544 - val_loss: 0.4689 - val_acc: 0.7565\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.4720 - acc: 0.7535 - val_loss: 0.4709 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 00038: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 8.5197 - acc: 0.6816 - val_loss: 0.5298 - val_acc: 0.7453\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.5710 - acc: 0.7170 - val_loss: 0.5422 - val_acc: 0.7335\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.5033 - acc: 0.7360 - val_loss: 0.5044 - val_acc: 0.7311\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.5035 - acc: 0.7352 - val_loss: 0.4732 - val_acc: 0.7558\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.4908 - acc: 0.7404 - val_loss: 0.4784 - val_acc: 0.7508\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.4901 - acc: 0.7407 - val_loss: 0.4776 - val_acc: 0.7508\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.4820 - acc: 0.7462 - val_loss: 0.4925 - val_acc: 0.7490\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.4795 - acc: 0.7479 - val_loss: 0.4740 - val_acc: 0.7555\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.4824 - acc: 0.7474 - val_loss: 0.4739 - val_acc: 0.7554\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.4749 - acc: 0.7519 - val_loss: 0.4795 - val_acc: 0.7488\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.4744 - acc: 0.7519 - val_loss: 0.4717 - val_acc: 0.7553\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.4781 - acc: 0.7486 - val_loss: 0.4980 - val_acc: 0.7496\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.4750 - acc: 0.7528 - val_loss: 0.4729 - val_acc: 0.7549\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.4773 - acc: 0.7497 - val_loss: 0.4815 - val_acc: 0.7480\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.4749 - acc: 0.7520 - val_loss: 0.4706 - val_acc: 0.7552\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.4737 - acc: 0.7525 - val_loss: 0.4862 - val_acc: 0.7468\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.4737 - acc: 0.7521 - val_loss: 0.4716 - val_acc: 0.7555\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.4748 - acc: 0.7527 - val_loss: 0.4735 - val_acc: 0.7550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.4790 - acc: 0.7487 - val_loss: 0.4726 - val_acc: 0.7552\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.4810 - acc: 0.7474 - val_loss: 0.4705 - val_acc: 0.7544\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.4727 - acc: 0.7535 - val_loss: 0.4704 - val_acc: 0.7555\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.4729 - acc: 0.7532 - val_loss: 0.4838 - val_acc: 0.7460\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.4734 - acc: 0.7531 - val_loss: 0.4727 - val_acc: 0.7532\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.4734 - acc: 0.7524 - val_loss: 0.4727 - val_acc: 0.7533\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.4734 - acc: 0.7536 - val_loss: 0.4743 - val_acc: 0.7558\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.4742 - acc: 0.7531 - val_loss: 0.4813 - val_acc: 0.7453\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.4743 - acc: 0.7523 - val_loss: 0.4713 - val_acc: 0.7549\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.4719 - acc: 0.7546 - val_loss: 0.4757 - val_acc: 0.7493\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.4727 - acc: 0.7532 - val_loss: 0.4717 - val_acc: 0.7557\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.4726 - acc: 0.7539 - val_loss: 0.4709 - val_acc: 0.7548\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.4724 - acc: 0.7541 - val_loss: 0.4705 - val_acc: 0.7551\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 00031: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 10.0789 - acc: 0.6790 - val_loss: 0.9164 - val_acc: 0.6839\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.5571 - acc: 0.7300 - val_loss: 0.4715 - val_acc: 0.7595\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.4772 - acc: 0.7541 - val_loss: 0.5320 - val_acc: 0.7307\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.4941 - acc: 0.7460 - val_loss: 0.4649 - val_acc: 0.7643\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.7469 - acc: 0.7350 - val_loss: 1.5607 - val_acc: 0.7521\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.6526 - acc: 0.7338 - val_loss: 0.4595 - val_acc: 0.7669\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.4681 - acc: 0.7596 - val_loss: 0.5115 - val_acc: 0.7419\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.4681 - acc: 0.7597 - val_loss: 0.4658 - val_acc: 0.7550\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.4647 - acc: 0.7606 - val_loss: 0.4558 - val_acc: 0.7693\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.4636 - acc: 0.7625 - val_loss: 0.4725 - val_acc: 0.7575\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.4644 - acc: 0.7619 - val_loss: 0.4724 - val_acc: 0.7565\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.4624 - acc: 0.7633 - val_loss: 0.4581 - val_acc: 0.7623\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.4615 - acc: 0.7639 - val_loss: 0.4670 - val_acc: 0.7558\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.4849 - acc: 0.7478 - val_loss: 0.4613 - val_acc: 0.7605\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.4609 - acc: 0.7631 - val_loss: 0.4638 - val_acc: 0.7629\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.4610 - acc: 0.7639 - val_loss: 0.4547 - val_acc: 0.7689\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.4595 - acc: 0.7649 - val_loss: 0.4588 - val_acc: 0.7607\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.4585 - acc: 0.7657 - val_loss: 0.4554 - val_acc: 0.7687\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.4587 - acc: 0.7652 - val_loss: 0.4558 - val_acc: 0.7694\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.4593 - acc: 0.7645 - val_loss: 0.4541 - val_acc: 0.7691\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.4601 - acc: 0.7640 - val_loss: 0.4742 - val_acc: 0.7511\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.4606 - acc: 0.7638 - val_loss: 0.4554 - val_acc: 0.7697\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.4596 - acc: 0.7647 - val_loss: 0.4555 - val_acc: 0.7682\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.4585 - acc: 0.7658 - val_loss: 0.4536 - val_acc: 0.7689\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.4585 - acc: 0.7655 - val_loss: 0.4538 - val_acc: 0.7695\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.4594 - acc: 0.7643 - val_loss: 0.4538 - val_acc: 0.7689\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.4584 - acc: 0.7654 - val_loss: 0.4623 - val_acc: 0.7601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.4592 - acc: 0.7645 - val_loss: 0.4547 - val_acc: 0.7689\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.4615 - acc: 0.7634 - val_loss: 0.4664 - val_acc: 0.7543\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.4583 - acc: 0.7657 - val_loss: 0.4546 - val_acc: 0.7693\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.4570 - acc: 0.7662 - val_loss: 0.4560 - val_acc: 0.7625\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.4577 - acc: 0.7658 - val_loss: 0.4537 - val_acc: 0.7694\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.4580 - acc: 0.7659 - val_loss: 0.4545 - val_acc: 0.7696\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.4591 - acc: 0.7645 - val_loss: 0.4556 - val_acc: 0.7666\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 00034: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 11.0104 - acc: 0.6832 - val_loss: 0.6077 - val_acc: 0.7021\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.7702 - acc: 0.7107 - val_loss: 0.6696 - val_acc: 0.7477\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.5428 - acc: 0.7400 - val_loss: 0.4673 - val_acc: 0.7699\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.4683 - acc: 0.7656 - val_loss: 0.4444 - val_acc: 0.7797\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.4529 - acc: 0.7735 - val_loss: 0.4430 - val_acc: 0.7798\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.4750 - acc: 0.7591 - val_loss: 0.4790 - val_acc: 0.7500\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.4554 - acc: 0.7715 - val_loss: 0.4437 - val_acc: 0.7819\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.4518 - acc: 0.7735 - val_loss: 0.4414 - val_acc: 0.7800\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.4468 - acc: 0.7781 - val_loss: 0.4456 - val_acc: 0.7712\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.4515 - acc: 0.7742 - val_loss: 0.4518 - val_acc: 0.7781\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.4533 - acc: 0.7715 - val_loss: 0.4346 - val_acc: 0.7816\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.4461 - acc: 0.7769 - val_loss: 0.4621 - val_acc: 0.7663\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.4450 - acc: 0.7779 - val_loss: 0.4299 - val_acc: 0.7888\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.4512 - acc: 0.7748 - val_loss: 0.4277 - val_acc: 0.7871\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 1.3553 - acc: 0.7322 - val_loss: 0.4565 - val_acc: 0.7724\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.4545 - acc: 0.7691 - val_loss: 0.4468 - val_acc: 0.7764\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.4527 - acc: 0.7710 - val_loss: 0.4459 - val_acc: 0.7796\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.4504 - acc: 0.7742 - val_loss: 0.4457 - val_acc: 0.7780\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.4443 - acc: 0.7775 - val_loss: 0.4496 - val_acc: 0.7729\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.4352 - acc: 0.7848 - val_loss: 0.4295 - val_acc: 0.7871\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.4362 - acc: 0.7836 - val_loss: 0.4295 - val_acc: 0.7876\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.4309 - acc: 0.7879 - val_loss: 0.4437 - val_acc: 0.7850\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.4302 - acc: 0.7891 - val_loss: 0.4148 - val_acc: 0.8047\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.4443 - acc: 0.7788 - val_loss: 0.4385 - val_acc: 0.7804\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.4384 - acc: 0.7814 - val_loss: 0.4244 - val_acc: 0.7941\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.4322 - acc: 0.7868 - val_loss: 0.4227 - val_acc: 0.7903\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.4369 - acc: 0.7849 - val_loss: 0.4324 - val_acc: 0.7857\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.4409 - acc: 0.7800 - val_loss: 0.4311 - val_acc: 0.7867\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.4214 - acc: 0.7950 - val_loss: 0.4186 - val_acc: 0.7969\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.4193 - acc: 0.7954 - val_loss: 0.4148 - val_acc: 0.7990\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.4142 - acc: 0.7995 - val_loss: 0.4089 - val_acc: 0.8021\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.4120 - acc: 0.8009 - val_loss: 0.3943 - val_acc: 0.8154\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.4168 - acc: 0.7975 - val_loss: 0.4262 - val_acc: 0.7936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.4123 - acc: 0.7997 - val_loss: 0.4184 - val_acc: 0.7981\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.4132 - acc: 0.8003 - val_loss: 0.4040 - val_acc: 0.8010\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.4137 - acc: 0.8004 - val_loss: 0.3984 - val_acc: 0.8063\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.4085 - acc: 0.8023 - val_loss: 0.3965 - val_acc: 0.8090\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.3942 - acc: 0.8125 - val_loss: 0.3944 - val_acc: 0.8082\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.3937 - acc: 0.8121 - val_loss: 0.3830 - val_acc: 0.8183\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.3925 - acc: 0.8131 - val_loss: 0.3774 - val_acc: 0.8208\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.3920 - acc: 0.8122 - val_loss: 0.4219 - val_acc: 0.7849\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.3988 - acc: 0.8087 - val_loss: 0.3961 - val_acc: 0.8078\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.3862 - acc: 0.8173 - val_loss: 0.3857 - val_acc: 0.8194\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.3898 - acc: 0.8145 - val_loss: 0.4309 - val_acc: 0.7916\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.3939 - acc: 0.8123 - val_loss: 0.3899 - val_acc: 0.8103\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.3762 - acc: 0.8241 - val_loss: 0.3667 - val_acc: 0.8314\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.3737 - acc: 0.8243 - val_loss: 0.4092 - val_acc: 0.8014\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.3741 - acc: 0.8244 - val_loss: 0.3607 - val_acc: 0.8402\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.3717 - acc: 0.8257 - val_loss: 0.3735 - val_acc: 0.8241\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.3715 - acc: 0.8261 - val_loss: 0.3741 - val_acc: 0.8192\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.3675 - acc: 0.8284 - val_loss: 0.3563 - val_acc: 0.8329\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.3663 - acc: 0.8301 - val_loss: 0.3574 - val_acc: 0.8412\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.3692 - acc: 0.8276 - val_loss: 0.3543 - val_acc: 0.8394\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.3598 - acc: 0.8340 - val_loss: 0.3445 - val_acc: 0.8465\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.3588 - acc: 0.8351 - val_loss: 0.3557 - val_acc: 0.8376\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.3656 - acc: 0.8305 - val_loss: 0.3474 - val_acc: 0.8454\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.3533 - acc: 0.8391 - val_loss: 0.4240 - val_acc: 0.7886\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.3611 - acc: 0.8332 - val_loss: 0.3593 - val_acc: 0.8321\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.3554 - acc: 0.8361 - val_loss: 0.3365 - val_acc: 0.8461\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.3507 - acc: 0.8401 - val_loss: 0.3383 - val_acc: 0.8530\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.3539 - acc: 0.8371 - val_loss: 0.3469 - val_acc: 0.8482\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.3528 - acc: 0.8401 - val_loss: 0.4603 - val_acc: 0.7637\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.3519 - acc: 0.8385 - val_loss: 0.3836 - val_acc: 0.8239\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.3522 - acc: 0.8383 - val_loss: 0.4083 - val_acc: 0.7948\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.3393 - acc: 0.8481 - val_loss: 0.3238 - val_acc: 0.8612\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.3310 - acc: 0.8528 - val_loss: 0.3219 - val_acc: 0.8585\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.3323 - acc: 0.8524 - val_loss: 0.3194 - val_acc: 0.8676\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.3293 - acc: 0.8540 - val_loss: 0.3251 - val_acc: 0.8516\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.3283 - acc: 0.8538 - val_loss: 0.3117 - val_acc: 0.8691\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.3259 - acc: 0.8561 - val_loss: 0.3162 - val_acc: 0.8686\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.3299 - acc: 0.8528 - val_loss: 0.3171 - val_acc: 0.8577\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.3288 - acc: 0.8545 - val_loss: 0.3163 - val_acc: 0.8647\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.3253 - acc: 0.8557 - val_loss: 0.3209 - val_acc: 0.8503\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.3133 - acc: 0.8636 - val_loss: 0.3000 - val_acc: 0.8745\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.3290 - acc: 0.8517 - val_loss: 0.3120 - val_acc: 0.8598\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.3200 - acc: 0.8586 - val_loss: 0.3054 - val_acc: 0.8674\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.3184 - acc: 0.8597 - val_loss: 0.2993 - val_acc: 0.8702\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.3177 - acc: 0.8592 - val_loss: 0.3027 - val_acc: 0.8725\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.3132 - acc: 0.8633 - val_loss: 0.3594 - val_acc: 0.8278\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.3166 - acc: 0.8610 - val_loss: 0.3147 - val_acc: 0.8662\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.3135 - acc: 0.8620 - val_loss: 0.2933 - val_acc: 0.8753\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.3093 - acc: 0.8656 - val_loss: 0.2923 - val_acc: 0.8796\n",
      "Epoch 83/200\n",
      "198/198 - 2s - loss: 0.3151 - acc: 0.8623 - val_loss: 0.3164 - val_acc: 0.8633\n",
      "Epoch 84/200\n",
      "198/198 - 2s - loss: 0.3133 - acc: 0.8627 - val_loss: 0.2946 - val_acc: 0.8755\n",
      "Epoch 85/200\n",
      "198/198 - 2s - loss: 0.3086 - acc: 0.8652 - val_loss: 0.2996 - val_acc: 0.8753\n",
      "Epoch 86/200\n",
      "198/198 - 2s - loss: 0.3072 - acc: 0.8663 - val_loss: 0.2895 - val_acc: 0.8778\n",
      "Epoch 87/200\n",
      "198/198 - 2s - loss: 0.3073 - acc: 0.8661 - val_loss: 0.2913 - val_acc: 0.8778\n",
      "Epoch 88/200\n",
      "198/198 - 2s - loss: 0.3003 - acc: 0.8717 - val_loss: 0.2884 - val_acc: 0.8787\n",
      "Epoch 89/200\n",
      "198/198 - 2s - loss: 0.3106 - acc: 0.8656 - val_loss: 0.3004 - val_acc: 0.8671\n",
      "Epoch 90/200\n",
      "198/198 - 2s - loss: 0.3135 - acc: 0.8621 - val_loss: 0.2933 - val_acc: 0.8739\n",
      "Epoch 91/200\n",
      "198/198 - 2s - loss: 0.3156 - acc: 0.8610 - val_loss: 0.2913 - val_acc: 0.8738\n",
      "Epoch 92/200\n",
      "198/198 - 2s - loss: 0.3101 - acc: 0.8631 - val_loss: 0.2881 - val_acc: 0.8772\n",
      "Epoch 93/200\n",
      "198/198 - 2s - loss: 0.3116 - acc: 0.8638 - val_loss: 0.2999 - val_acc: 0.8728\n",
      "Epoch 94/200\n",
      "198/198 - 2s - loss: 0.3021 - acc: 0.8691 - val_loss: 0.3170 - val_acc: 0.8611\n",
      "Epoch 95/200\n",
      "198/198 - 2s - loss: 0.3052 - acc: 0.8684 - val_loss: 0.3573 - val_acc: 0.8364\n",
      "Epoch 96/200\n",
      "198/198 - 2s - loss: 0.3061 - acc: 0.8666 - val_loss: 0.3034 - val_acc: 0.8653\n",
      "Epoch 97/200\n",
      "198/198 - 2s - loss: 0.3128 - acc: 0.8618 - val_loss: 0.3200 - val_acc: 0.8665\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 98/200\n",
      "198/198 - 2s - loss: 0.2904 - acc: 0.8770 - val_loss: 0.2915 - val_acc: 0.8715\n",
      "Epoch 99/200\n",
      "198/198 - 2s - loss: 0.2897 - acc: 0.8782 - val_loss: 0.3144 - val_acc: 0.8541\n",
      "Epoch 100/200\n",
      "198/198 - 2s - loss: 0.2924 - acc: 0.8759 - val_loss: 0.2814 - val_acc: 0.8850\n",
      "Epoch 101/200\n",
      "198/198 - 2s - loss: 0.2980 - acc: 0.8718 - val_loss: 0.2838 - val_acc: 0.8801\n",
      "Epoch 102/200\n",
      "198/198 - 2s - loss: 0.2879 - acc: 0.8778 - val_loss: 0.2876 - val_acc: 0.8744\n",
      "Epoch 103/200\n",
      "198/198 - 2s - loss: 0.2891 - acc: 0.8780 - val_loss: 0.3011 - val_acc: 0.8720\n",
      "Epoch 104/200\n",
      "198/198 - 2s - loss: 0.2850 - acc: 0.8802 - val_loss: 0.2832 - val_acc: 0.8777\n",
      "Epoch 105/200\n",
      "198/198 - 2s - loss: 0.3020 - acc: 0.8702 - val_loss: 0.2793 - val_acc: 0.8835\n",
      "Epoch 106/200\n",
      "198/198 - 2s - loss: 0.2931 - acc: 0.8743 - val_loss: 0.2813 - val_acc: 0.8809\n",
      "Epoch 107/200\n",
      "198/198 - 2s - loss: 0.2942 - acc: 0.8725 - val_loss: 0.2884 - val_acc: 0.8823\n",
      "Epoch 108/200\n",
      "198/198 - 2s - loss: 0.2998 - acc: 0.8703 - val_loss: 0.3022 - val_acc: 0.8701\n",
      "Epoch 109/200\n",
      "198/198 - 2s - loss: 0.2908 - acc: 0.8759 - val_loss: 0.3402 - val_acc: 0.8477\n",
      "Epoch 110/200\n",
      "198/198 - 2s - loss: 0.2916 - acc: 0.8751 - val_loss: 0.2790 - val_acc: 0.8880\n",
      "Epoch 111/200\n",
      "198/198 - 2s - loss: 0.2871 - acc: 0.8783 - val_loss: 0.2774 - val_acc: 0.8857\n",
      "Epoch 112/200\n",
      "198/198 - 2s - loss: 0.2871 - acc: 0.8781 - val_loss: 0.2891 - val_acc: 0.8760\n",
      "Epoch 113/200\n",
      "198/198 - 2s - loss: 0.2883 - acc: 0.8774 - val_loss: 0.2754 - val_acc: 0.8869\n",
      "Epoch 114/200\n",
      "198/198 - 2s - loss: 0.2887 - acc: 0.8774 - val_loss: 0.3204 - val_acc: 0.8509\n",
      "Epoch 115/200\n",
      "198/198 - 2s - loss: 0.2906 - acc: 0.8750 - val_loss: 0.2802 - val_acc: 0.8801\n",
      "Epoch 116/200\n",
      "198/198 - 2s - loss: 0.2997 - acc: 0.8709 - val_loss: 0.2783 - val_acc: 0.8879\n",
      "Epoch 117/200\n",
      "198/198 - 2s - loss: 0.2911 - acc: 0.8757 - val_loss: 0.2771 - val_acc: 0.8854\n",
      "Epoch 118/200\n",
      "198/198 - 2s - loss: 0.2894 - acc: 0.8761 - val_loss: 0.2800 - val_acc: 0.8807\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 - 2s - loss: 0.2803 - acc: 0.8831 - val_loss: 0.2752 - val_acc: 0.8886\n",
      "Epoch 120/200\n",
      "198/198 - 2s - loss: 0.2818 - acc: 0.8818 - val_loss: 0.2834 - val_acc: 0.8821\n",
      "Epoch 121/200\n",
      "198/198 - 2s - loss: 0.2816 - acc: 0.8817 - val_loss: 0.2715 - val_acc: 0.8891\n",
      "Epoch 122/200\n",
      "198/198 - 2s - loss: 0.2798 - acc: 0.8829 - val_loss: 0.2783 - val_acc: 0.8843\n",
      "Epoch 123/200\n",
      "198/198 - 2s - loss: 0.2815 - acc: 0.8815 - val_loss: 0.2719 - val_acc: 0.8885\n",
      "Epoch 124/200\n",
      "198/198 - 2s - loss: 0.2789 - acc: 0.8831 - val_loss: 0.2711 - val_acc: 0.8883\n",
      "Epoch 125/200\n",
      "198/198 - 2s - loss: 0.2849 - acc: 0.8789 - val_loss: 0.2728 - val_acc: 0.8886\n",
      "Epoch 126/200\n",
      "198/198 - 2s - loss: 0.2784 - acc: 0.8835 - val_loss: 0.2804 - val_acc: 0.8834\n",
      "Epoch 127/200\n",
      "198/198 - 2s - loss: 0.2807 - acc: 0.8819 - val_loss: 0.2782 - val_acc: 0.8811\n",
      "Epoch 128/200\n",
      "198/198 - 2s - loss: 0.2891 - acc: 0.8754 - val_loss: 0.2840 - val_acc: 0.8755\n",
      "Epoch 129/200\n",
      "198/198 - 2s - loss: 0.2785 - acc: 0.8829 - val_loss: 0.2756 - val_acc: 0.8835\n",
      "\n",
      "Epoch 00129: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 130/200\n",
      "198/198 - 2s - loss: 0.2754 - acc: 0.8859 - val_loss: 0.2945 - val_acc: 0.8763\n",
      "Epoch 131/200\n",
      "198/198 - 2s - loss: 0.2756 - acc: 0.8858 - val_loss: 0.2832 - val_acc: 0.8843\n",
      "Epoch 132/200\n",
      "198/198 - 2s - loss: 0.2778 - acc: 0.8836 - val_loss: 0.2705 - val_acc: 0.8877\n",
      "Epoch 133/200\n",
      "198/198 - 2s - loss: 0.2768 - acc: 0.8841 - val_loss: 0.2699 - val_acc: 0.8890\n",
      "Epoch 134/200\n",
      "198/198 - 2s - loss: 0.2737 - acc: 0.8865 - val_loss: 0.2743 - val_acc: 0.8890\n",
      "Epoch 135/200\n",
      "198/198 - 2s - loss: 0.2761 - acc: 0.8850 - val_loss: 0.2701 - val_acc: 0.8887\n",
      "Epoch 136/200\n",
      "198/198 - 2s - loss: 0.2804 - acc: 0.8817 - val_loss: 0.2706 - val_acc: 0.8902\n",
      "Epoch 137/200\n",
      "198/198 - 2s - loss: 0.2744 - acc: 0.8864 - val_loss: 0.2701 - val_acc: 0.8893\n",
      "Epoch 138/200\n",
      "198/198 - 2s - loss: 0.2755 - acc: 0.8850 - val_loss: 0.2767 - val_acc: 0.8816\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 1.5848932274101303e-05.\n",
      "Epoch 139/200\n",
      "198/198 - 2s - loss: 0.2729 - acc: 0.8874 - val_loss: 0.2721 - val_acc: 0.8865\n",
      "Epoch 140/200\n",
      "198/198 - 2s - loss: 0.2720 - acc: 0.8875 - val_loss: 0.2692 - val_acc: 0.8902\n",
      "Epoch 141/200\n",
      "198/198 - 2s - loss: 0.2723 - acc: 0.8873 - val_loss: 0.2677 - val_acc: 0.8899\n",
      "Epoch 142/200\n",
      "198/198 - 2s - loss: 0.2731 - acc: 0.8870 - val_loss: 0.2685 - val_acc: 0.8898\n",
      "Epoch 143/200\n",
      "198/198 - 2s - loss: 0.2716 - acc: 0.8882 - val_loss: 0.2674 - val_acc: 0.8907\n",
      "Epoch 144/200\n",
      "198/198 - 2s - loss: 0.2715 - acc: 0.8879 - val_loss: 0.2710 - val_acc: 0.8877\n",
      "Epoch 145/200\n",
      "198/198 - 2s - loss: 0.2727 - acc: 0.8868 - val_loss: 0.2688 - val_acc: 0.8898\n",
      "Epoch 146/200\n",
      "198/198 - 2s - loss: 0.2717 - acc: 0.8873 - val_loss: 0.2682 - val_acc: 0.8910\n",
      "Epoch 147/200\n",
      "198/198 - 2s - loss: 0.2725 - acc: 0.8872 - val_loss: 0.2677 - val_acc: 0.8917\n",
      "Epoch 148/200\n",
      "198/198 - 2s - loss: 0.2710 - acc: 0.8882 - val_loss: 0.2674 - val_acc: 0.8915\n",
      "\n",
      "Epoch 00148: ReduceLROnPlateau reducing learning rate to 1.0000000608891671e-05.\n",
      "Epoch 149/200\n",
      "198/198 - 2s - loss: 0.2709 - acc: 0.8883 - val_loss: 0.2707 - val_acc: 0.8873\n",
      "Epoch 150/200\n",
      "198/198 - 2s - loss: 0.2711 - acc: 0.8882 - val_loss: 0.2668 - val_acc: 0.8910\n",
      "Epoch 151/200\n",
      "198/198 - 2s - loss: 0.2709 - acc: 0.8883 - val_loss: 0.2667 - val_acc: 0.8904\n",
      "Epoch 152/200\n",
      "198/198 - 2s - loss: 0.2713 - acc: 0.8877 - val_loss: 0.2697 - val_acc: 0.8896\n",
      "Epoch 153/200\n",
      "198/198 - 2s - loss: 0.2711 - acc: 0.8879 - val_loss: 0.2764 - val_acc: 0.8855\n",
      "Epoch 154/200\n",
      "198/198 - 2s - loss: 0.2704 - acc: 0.8881 - val_loss: 0.2682 - val_acc: 0.8896\n",
      "Epoch 155/200\n",
      "198/198 - 2s - loss: 0.2708 - acc: 0.8878 - val_loss: 0.2714 - val_acc: 0.8870\n",
      "\n",
      "Epoch 00155: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 156/200\n",
      "198/198 - 2s - loss: 0.2719 - acc: 0.8873 - val_loss: 0.2809 - val_acc: 0.8836\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m signal_1, signal_2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(signal_1_recons,signal_2_recons):\n\u001b[0;32m----> 3\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_pfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignal_2\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/VAE/EMD_VAE/PFN/pfn_utils.py:115\u001b[0m, in \u001b[0;36mtrain_pfn\u001b[0;34m(signal_1, signal_2)\u001b[0m\n\u001b[1;32m    110\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m    111\u001b[0m                                             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    113\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [reduce_lr,early_stop]\n\u001b[0;32m--> 115\u001b[0m hist1 \u001b[38;5;241m=\u001b[39m \u001b[43mpfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pfn, hist1, [(X,y), (X_train, X_val, X_test,\n\u001b[1;32m    124\u001b[0m  Y_train, Y_val, Y_test), class_weight]\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2_new/lib/python3.8/site-packages/energyflow/archs/archbase.py:370\u001b[0m, in \u001b[0;36mNNBase.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m'\u001b[39m, [])\u001b[38;5;241m.\u001b[39mextend(callbacks)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# do the fitting\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# handle saving at the end, if we weren't already saving throughout \u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_while_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2_new/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1095\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1096\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1097\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1098\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1099\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1100\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1102\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2_new/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[0;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2_new/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:855\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    852\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    853\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    857\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    858\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2_new/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2940\u001b[0m   (graph_function,\n\u001b[1;32m   2941\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2_new/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1914\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1917\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1920\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m     args,\n\u001b[1;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1923\u001b[0m     executing_eagerly)\n\u001b[1;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2_new/lib/python3.8/site-packages/tensorflow/python/eager/function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    564\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    568\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2_new/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for signal_1, signal_2 in zip(signal_1_recons,signal_2_recons):\n",
    "    result.append(train_pfn(signal_1, signal_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3697547",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_res = []\n",
    "aucs = []\n",
    "for i, (pfn, hist, data) in enumerate(result):\n",
    "    preds = pfn.predict(data[1][2], batch_size=10000)\n",
    "    pfn_fp, pfn_tp, threshs = roc_curve(data[1][-1][:,1], preds[:,1])\n",
    "    roc_res.append([pfn_fp, pfn_tp, threshs])\n",
    "    auc = roc_auc_score(data[1][-1][:,1], preds[:,1])\n",
    "    aucs.append(auc)\n",
    "    print('reconstruct PFN AUC(beta = {:.2f}):{}'.format(log_betas[i],auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.power(10,log_betas),aucs)\n",
    "plt.semilogx()\n",
    "plt.ylabel(\"AUC\" ,fontsize=40)\n",
    "plt.xlabel(r\"$\\beta$\" ,fontsize=40)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.savefig(os.path.join(\"auc_beta.pdf\"), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1504f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"auc.npz\", beta= log_betas, roc=roc_res, original_roc_info=original_roc_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
