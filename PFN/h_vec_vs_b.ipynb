{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef50705",
   "metadata": {},
   "source": [
    "# Import and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43264681",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a505b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:23:21.679827: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import os\n",
    "\n",
    "from pfn_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45667e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "train, val, test = 0.6, 0.3, 0.1\n",
    "Phi_sizes, F_sizes = (256, 256, 256), (256, 256, 256)\n",
    "num_epoch = 500\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505a2894",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_dir = \"/global/home/users/yifengh3/VAE/vec_data/recon_data\"\n",
    "raw_b_signals = np.load(os.path.join(data_base_dir, \"fixed_reconstructed_B_signal_vector.npz\")) \n",
    "raw_hv_signals = np.load(os.path.join(data_base_dir, \"reconstructed_hv_vector.npz\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3813124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'recon', 'beta']\n"
     ]
    }
   ],
   "source": [
    "print(list(raw_b_signals.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5aa5d4",
   "metadata": {},
   "source": [
    "# Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25276321",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal1 = raw_b_signals[\"data\"]\n",
    "signal2 = raw_hv_signals[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b559b182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:23:24.354328: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-09-28 10:23:24.355831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-09-28 10:23:24.388820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: Quadro RTX 6000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2022-09-28 10:23:24.388860: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-09-28 10:23:24.390203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-09-28 10:23:24.390232: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-09-28 10:23:24.391744: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-09-28 10:23:24.391945: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-09-28 10:23:24.393319: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-09-28 10:23:24.394137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-09-28 10:23:24.397315: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-09-28 10:23:24.399935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-09-28 10:23:24.400530: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-28 10:23:24.402450: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-09-28 10:23:24.403754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: Quadro RTX 6000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2022-09-28 10:23:24.403775: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-09-28 10:23:24.403790: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-09-28 10:23:24.403801: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-09-28 10:23:24.403812: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-09-28 10:23:24.403822: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-09-28 10:23:24.403832: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-09-28 10:23:24.403842: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-09-28 10:23:24.403852: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-09-28 10:23:24.406271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-09-28 10:23:24.406316: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-09-28 10:23:24.969869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-09-28 10:23:24.969915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-09-28 10:23:24.969926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-09-28 10:23:24.973894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22475 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:23:25.419299: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-09-28 10:23:25.419767: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2994530000 Hz\n",
      "2022-09-28 10:23:25.819269: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 - 3s - loss: 7.8301 - acc: 0.8342 - val_loss: 0.5090 - val_acc: 0.9057\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.4715 - acc: 0.8936 - val_loss: 0.2636 - val_acc: 0.9031\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.4565 - acc: 0.8906 - val_loss: 0.5088 - val_acc: 0.8692\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2423 - acc: 0.9195 - val_loss: 0.1877 - val_acc: 0.9327\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.2118 - acc: 0.9253 - val_loss: 0.1797 - val_acc: 0.9341\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1839 - acc: 0.9329 - val_loss: 0.2028 - val_acc: 0.9193\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1740 - acc: 0.9365 - val_loss: 0.2999 - val_acc: 0.9006\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.2039 - acc: 0.9274 - val_loss: 0.1866 - val_acc: 0.9326\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1683 - acc: 0.9380 - val_loss: 0.1623 - val_acc: 0.9399\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1669 - acc: 0.9391 - val_loss: 0.1635 - val_acc: 0.9404\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1945 - acc: 0.9301 - val_loss: 0.2254 - val_acc: 0.9246\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1776 - acc: 0.9358 - val_loss: 0.1905 - val_acc: 0.9323\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1644 - acc: 0.9395 - val_loss: 0.2036 - val_acc: 0.9229\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1623 - acc: 0.9399 - val_loss: 0.1726 - val_acc: 0.9362\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1542 - acc: 0.9436 - val_loss: 0.1559 - val_acc: 0.9422\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1546 - acc: 0.9435 - val_loss: 0.1610 - val_acc: 0.9420\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1552 - acc: 0.9432 - val_loss: 0.1653 - val_acc: 0.9399\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1537 - acc: 0.9436 - val_loss: 0.1564 - val_acc: 0.9421\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1527 - acc: 0.9439 - val_loss: 0.1603 - val_acc: 0.9425\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1542 - acc: 0.9433 - val_loss: 0.1515 - val_acc: 0.9436\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1510 - acc: 0.9445 - val_loss: 0.1518 - val_acc: 0.9437\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1518 - acc: 0.9438 - val_loss: 0.1538 - val_acc: 0.9430\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1561 - acc: 0.9427 - val_loss: 0.1616 - val_acc: 0.9421\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1534 - acc: 0.9437 - val_loss: 0.2594 - val_acc: 0.9107\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1578 - acc: 0.9419 - val_loss: 0.1624 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1466 - acc: 0.9460 - val_loss: 0.1544 - val_acc: 0.9432\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1489 - acc: 0.9445 - val_loss: 0.1530 - val_acc: 0.9438\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1462 - acc: 0.9459 - val_loss: 0.1535 - val_acc: 0.9431\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1474 - acc: 0.9460 - val_loss: 0.1753 - val_acc: 0.9355\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1454 - acc: 0.9464 - val_loss: 0.1480 - val_acc: 0.9442\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1469 - acc: 0.9460 - val_loss: 0.1627 - val_acc: 0.9400\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1499 - acc: 0.9447 - val_loss: 0.1558 - val_acc: 0.9431\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1482 - acc: 0.9455 - val_loss: 0.1516 - val_acc: 0.9441\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1462 - acc: 0.9459 - val_loss: 0.1549 - val_acc: 0.9424\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1461 - acc: 0.9463 - val_loss: 0.1483 - val_acc: 0.9446\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1412 - acc: 0.9479 - val_loss: 0.1467 - val_acc: 0.9461\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1417 - acc: 0.9479 - val_loss: 0.1450 - val_acc: 0.9457\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1410 - acc: 0.9480 - val_loss: 0.1451 - val_acc: 0.9459\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1405 - acc: 0.9485 - val_loss: 0.1468 - val_acc: 0.9451\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1396 - acc: 0.9486 - val_loss: 0.1485 - val_acc: 0.9448\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1427 - acc: 0.9472 - val_loss: 0.1559 - val_acc: 0.9426\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1409 - acc: 0.9480 - val_loss: 0.1457 - val_acc: 0.9459\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1358 - acc: 0.9496 - val_loss: 0.1440 - val_acc: 0.9459\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1360 - acc: 0.9498 - val_loss: 0.1497 - val_acc: 0.9447\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1344 - acc: 0.9503 - val_loss: 0.1431 - val_acc: 0.9470\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1366 - acc: 0.9494 - val_loss: 0.1663 - val_acc: 0.9395\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1351 - acc: 0.9501 - val_loss: 0.1414 - val_acc: 0.9475\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1338 - acc: 0.9504 - val_loss: 0.1405 - val_acc: 0.9478\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1353 - acc: 0.9500 - val_loss: 0.1463 - val_acc: 0.9456\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1348 - acc: 0.9499 - val_loss: 0.1410 - val_acc: 0.9473\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.1360 - acc: 0.9499 - val_loss: 0.1490 - val_acc: 0.9452\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.1334 - acc: 0.9500 - val_loss: 0.1433 - val_acc: 0.9471\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.1348 - acc: 0.9504 - val_loss: 0.1393 - val_acc: 0.9475\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.1327 - acc: 0.9506 - val_loss: 0.1391 - val_acc: 0.9480\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.1338 - acc: 0.9503 - val_loss: 0.1410 - val_acc: 0.9471\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.1313 - acc: 0.9513 - val_loss: 0.1470 - val_acc: 0.9461\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.1314 - acc: 0.9513 - val_loss: 0.1389 - val_acc: 0.9481\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.1314 - acc: 0.9511 - val_loss: 0.1410 - val_acc: 0.9473\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.1287 - acc: 0.9522 - val_loss: 0.1398 - val_acc: 0.9478\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.1299 - acc: 0.9520 - val_loss: 0.1370 - val_acc: 0.9490\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.1308 - acc: 0.9512 - val_loss: 0.1394 - val_acc: 0.9478\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.1280 - acc: 0.9525 - val_loss: 0.1522 - val_acc: 0.9424\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.1280 - acc: 0.9522 - val_loss: 0.1377 - val_acc: 0.9487\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.1268 - acc: 0.9529 - val_loss: 0.1355 - val_acc: 0.9497\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.1259 - acc: 0.9533 - val_loss: 0.1368 - val_acc: 0.9499\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.1267 - acc: 0.9526 - val_loss: 0.1359 - val_acc: 0.9493\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.1232 - acc: 0.9541 - val_loss: 0.1400 - val_acc: 0.9474\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.1227 - acc: 0.9547 - val_loss: 0.1310 - val_acc: 0.9509\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.1230 - acc: 0.9542 - val_loss: 0.1276 - val_acc: 0.9521\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.1214 - acc: 0.9548 - val_loss: 0.1412 - val_acc: 0.9479\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.1202 - acc: 0.9550 - val_loss: 0.1326 - val_acc: 0.9510\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.1191 - acc: 0.9555 - val_loss: 0.1276 - val_acc: 0.9521\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.1176 - acc: 0.9561 - val_loss: 0.1254 - val_acc: 0.9533\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.1162 - acc: 0.9568 - val_loss: 0.1265 - val_acc: 0.9530\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.1159 - acc: 0.9569 - val_loss: 0.1287 - val_acc: 0.9517\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.1144 - acc: 0.9573 - val_loss: 0.1244 - val_acc: 0.9537\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.1123 - acc: 0.9585 - val_loss: 0.1283 - val_acc: 0.9540\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.1108 - acc: 0.9581 - val_loss: 0.1188 - val_acc: 0.9555\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.1093 - acc: 0.9590 - val_loss: 0.1295 - val_acc: 0.9523\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.1101 - acc: 0.9590 - val_loss: 0.1176 - val_acc: 0.9562\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.1063 - acc: 0.9602 - val_loss: 0.1288 - val_acc: 0.9526\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.1084 - acc: 0.9593 - val_loss: 0.1272 - val_acc: 0.9531\n",
      "Epoch 83/200\n",
      "198/198 - 2s - loss: 0.1060 - acc: 0.9602 - val_loss: 0.1154 - val_acc: 0.9572\n",
      "Epoch 84/200\n",
      "198/198 - 2s - loss: 0.1041 - acc: 0.9609 - val_loss: 0.1152 - val_acc: 0.9574\n",
      "Epoch 85/200\n",
      "198/198 - 2s - loss: 0.1027 - acc: 0.9617 - val_loss: 0.1304 - val_acc: 0.9522\n",
      "Epoch 86/200\n",
      "198/198 - 2s - loss: 0.1034 - acc: 0.9609 - val_loss: 0.1147 - val_acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/200\n",
      "198/198 - 2s - loss: 0.1005 - acc: 0.9622 - val_loss: 0.1287 - val_acc: 0.9536\n",
      "Epoch 88/200\n",
      "198/198 - 2s - loss: 0.0997 - acc: 0.9624 - val_loss: 0.1301 - val_acc: 0.9540\n",
      "Epoch 89/200\n",
      "198/198 - 2s - loss: 0.0996 - acc: 0.9624 - val_loss: 0.1116 - val_acc: 0.9588\n",
      "Epoch 90/200\n",
      "198/198 - 2s - loss: 0.0970 - acc: 0.9635 - val_loss: 0.1102 - val_acc: 0.9591\n",
      "Epoch 91/200\n",
      "198/198 - 2s - loss: 0.0977 - acc: 0.9633 - val_loss: 0.1121 - val_acc: 0.9582\n",
      "Epoch 92/200\n",
      "198/198 - 2s - loss: 0.1003 - acc: 0.9621 - val_loss: 0.1157 - val_acc: 0.9568\n",
      "Epoch 93/200\n",
      "198/198 - 2s - loss: 0.0952 - acc: 0.9641 - val_loss: 0.1134 - val_acc: 0.9586\n",
      "Epoch 94/200\n",
      "198/198 - 2s - loss: 0.0949 - acc: 0.9642 - val_loss: 0.1120 - val_acc: 0.9592\n",
      "Epoch 95/200\n",
      "198/198 - 2s - loss: 0.0954 - acc: 0.9644 - val_loss: 0.1103 - val_acc: 0.9592\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 96/200\n",
      "198/198 - 2s - loss: 0.0890 - acc: 0.9666 - val_loss: 0.1081 - val_acc: 0.9599\n",
      "Epoch 97/200\n",
      "198/198 - 2s - loss: 0.0879 - acc: 0.9669 - val_loss: 0.1121 - val_acc: 0.9594\n",
      "Epoch 98/200\n",
      "198/198 - 2s - loss: 0.0896 - acc: 0.9665 - val_loss: 0.1203 - val_acc: 0.9563\n",
      "Epoch 99/200\n",
      "198/198 - 2s - loss: 0.0873 - acc: 0.9673 - val_loss: 0.1124 - val_acc: 0.9596\n",
      "Epoch 100/200\n",
      "198/198 - 2s - loss: 0.0876 - acc: 0.9669 - val_loss: 0.1126 - val_acc: 0.9594\n",
      "Epoch 101/200\n",
      "198/198 - 2s - loss: 0.0854 - acc: 0.9680 - val_loss: 0.1091 - val_acc: 0.9604\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 102/200\n",
      "198/198 - 2s - loss: 0.0839 - acc: 0.9687 - val_loss: 0.1074 - val_acc: 0.9608\n",
      "Epoch 103/200\n",
      "198/198 - 2s - loss: 0.0823 - acc: 0.9689 - val_loss: 0.1083 - val_acc: 0.9608\n",
      "Epoch 104/200\n",
      "198/198 - 2s - loss: 0.0812 - acc: 0.9691 - val_loss: 0.1209 - val_acc: 0.9565\n",
      "Epoch 105/200\n",
      "198/198 - 2s - loss: 0.0828 - acc: 0.9690 - val_loss: 0.1114 - val_acc: 0.9600\n",
      "Epoch 106/200\n",
      "198/198 - 2s - loss: 0.0813 - acc: 0.9692 - val_loss: 0.1071 - val_acc: 0.9614\n",
      "Epoch 107/200\n",
      "198/198 - 2s - loss: 0.0818 - acc: 0.9690 - val_loss: 0.1105 - val_acc: 0.9602\n",
      "Epoch 108/200\n",
      "198/198 - 2s - loss: 0.0814 - acc: 0.9691 - val_loss: 0.1284 - val_acc: 0.9546\n",
      "Epoch 109/200\n",
      "198/198 - 2s - loss: 0.0803 - acc: 0.9695 - val_loss: 0.1149 - val_acc: 0.9590\n",
      "Epoch 110/200\n",
      "198/198 - 2s - loss: 0.0789 - acc: 0.9702 - val_loss: 0.1088 - val_acc: 0.9612\n",
      "Epoch 111/200\n",
      "198/198 - 2s - loss: 0.0795 - acc: 0.9699 - val_loss: 0.1092 - val_acc: 0.9607\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 112/200\n",
      "198/198 - 2s - loss: 0.0765 - acc: 0.9711 - val_loss: 0.1091 - val_acc: 0.9611\n",
      "Epoch 113/200\n",
      "198/198 - 2s - loss: 0.0765 - acc: 0.9712 - val_loss: 0.1115 - val_acc: 0.9608\n",
      "Epoch 114/200\n",
      "198/198 - 2s - loss: 0.0766 - acc: 0.9708 - val_loss: 0.1089 - val_acc: 0.9611\n",
      "Epoch 115/200\n",
      "198/198 - 2s - loss: 0.0758 - acc: 0.9714 - val_loss: 0.1133 - val_acc: 0.9611\n",
      "Epoch 116/200\n",
      "198/198 - 2s - loss: 0.0757 - acc: 0.9715 - val_loss: 0.1095 - val_acc: 0.9609\n",
      "\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 00116: early stopping\n"
     ]
    }
   ],
   "source": [
    "pfn_original, hist1, original_training_data = train_pfn(signal1, signal2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e472d473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PFN AUC: 0.9936949956579688\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/home/users/yifengh3/VAE/EMD_VAE/PFN/pfn_utils.py:147: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  plt.plot(pfn_tp, 1/pfn_fp, '-', color='black', label='PFN')\n",
      "/global/home/users/yifengh3/VAE/EMD_VAE/PFN/pfn_utils.py:148: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  plt.plot(pfn_tp, 1/pfn_tp, '-', color='red', label='random')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAuA0lEQVR4nO3deXwV1d3H8c8JYcnCnrDIKgQ1IIsQFgEBBWkpoD7iglJUXNBqn5aqBXncQHGrS9UqICJFqUBV3BAKCrRKEESQ1YKyxghRCIEkGEkgOc8fk5sFkhBM7p25N9/36zWv5M6dO/PLiPnmnDlzxlhrERER8ZowtwsQEREpiQJKREQ8SQElIiKepIASERFPUkCJiIgnhbtdAEBMTIxt3bq122WIiIgL1q9fn2qtjT15vScCqnXr1qxbt87tMkRExAXGmKSS1quLT0REPEkBJSIinqSAEhERT1JAiYiIJymgRETEkxRQIiLiSQooERHxJFcDyhgz3BgzIz093c0yRETEg1wNKGvtQmvt2Lp167pZhoiIeJC6+CrBhAkTqFWr1hkvXbt2dbt0ERHP8sRUR8GuX79+GGPO6DOrVq0iMTERa+0Zf1ZEpCpQQFWCoUOHMnTo0DP6zJQpU0hMTCQvL49q1ar5qTIRkeClLj6XhIU5pz43N9flSkREvEkB5RJfQOXl5blciYiINymgXKKAEhEpmwLKJQooEZGyKaBc4hsYoYASESmZAsolGiQhIlI2BZRL1MUnIlI2vwWUMebfxpi+/tp/sFNAiYiUzS8BZYwZDPzkj32HCgWUiEjZyjWThDGmCTAF6Gyt7V5k/SDgSuAAYK21k40zb08CsM4P9YYMDZIQESlbeac66gt8AHTxrTDGRALTgQ7W2mxjzAJjzECgHvAecG3llhpaNEhCRKRs5eris9a+A2SetPpCIMlam53/ehUwFGgN9MdpRV1ujIktaZ/GmLHGmHXGmHUHDx78JbUHNXXxiYiUrSLXoBpRPLQygEbW2meBJUAekAuU+DRCa+0Ma22CtTYhNrbEDAtpCigRkbJVJKAOALWLvK6Tvw5r7V5r7WXW2vustTml7aAqP1HXF1BDhgzhsccec7kaERHvqUhArQZaGWNq5r/uAyw6kx1U5SfqDhgwgGuvvZa0tDTef/99t8sREfGccgWUMaY/MBpoaox5wBgTYa3NAn4HvGiMmQJsttYu92OtIaVly5bMnz+fhIQEt0sREfGkco3is9Z+CnxawvpPgE9+6cGNMcOB4XFxcb90FyIiEqJcneqoKnfxiYhI2TQXn4iIeJKrAVWVR/GJiEjZ1MUnIiKepC4+ERHxpPLOxSd+lJqaypw5cyq8n27dutG+fftKqEhExH2uBpSGmUNsbCx79+7lhhtuqPC+evbsyZo1ayqhKhER9xlrrds1kJCQYNetq5pP5zh+/Djfffddhfdzxx13cPDgQTZu3FjxokREAsgYs95ae8qsBeric1n16tVp27ZthfcTFRVFVZwVXkRClwZJiIiIJ+k+KBER8STdByUiIp6kLr4QsmnTJlJTU90uQ0SkUiigQkTz5s0BWL16tcuViIhUDgVUiBgzZozbJYiIVCoNkhAREU/SIAkREfEk3agbYp5++mm2b9/uag0DBgyge/furtYgIsFPARUiGjduTEREBCtXrmTlypWu1tKvXz8+/fRTV2sQkeCngAoRzZs358iRIxw/ftzVOoYPH052drarNYhIaFBAhZAaNWpQo0YNV2uoVq2aq8cXkdChYeZS6fbv3+92CSISAjTMXCpVRkYGe/fuRf9NRaSiNMxcKtUVV1wBoIASkQpTF59UqpYtWwJooISIVJgGSUil8g3SmDBhAvXq1QPghhtuYMCAAe4VJSJBSQEllapjx46ce+65rFu3DoCUlBR++uknBZSInDEFlFSq8847r9hMFh06dCAvL8/FikQkWCmgxK/CwsLYunUrDz/8cLH1l156KX379nWpKhEJBgoo8avzzz+f+fPn88gjjxRb/5///EfTIYlImTSKT/xq3rx5WGuLLQMHDiQ3N9ft0kTE43SjrgSctZZVq1ZhrXW7FBHxMN2oKwFnjAHgp59+crkSEfEyXYOSgPv1r3/N8uXLWbRoERERESVu06RJE3r06BHgykTESxRQEnC+G3hHjhxZ6jbGGA4fPoxa1yJVlwJKAu7mm2+mR48enDhxosT333rrLZ566in27dtHTk4OAFFRUURGRgayTBFxmQJKAi4sLIxOnTqV+v769esB5yZfn6ioKPbt26cWlUgVooASz7nmmmsAClpPn3/+OXPnzuX7778nMjKS6tWru1meiASIAko8p27dutx2220Fr2vXrs3cuXM5//zzadu2LTt37nSxOhEJFN2oK553xRVX8NJLLzFw4ED27NnDxo0b2bJli+b4EwlxCijxvDp16nDXXXdxySWXkJeXxwUXXECnTp2YPn2626WJiB8ZL9zNn5CQYH2PZxApzdGjR/n3v//NsWPHuOaaa2jTpg3ffPMN4eHqqRYJZsaY9dbahJPX6/9sCRrR0dEMHz68oGtv9+7dbNiwge7du7tcmYj4gwJKgk5YWBiLFi1i6NCh9OjRg7i4uDK3v+mmm7j//vsDVJ2IVBZ18UlQOnz4MBMmTDjtfH7Lli3jwIEDHDlyRPdQiXhUQLv4jDGdge5AJBBjrX3IH8eRqqt+/frMmDHjtNtdffXVvPPOO0ybNo377rsvAJWJSGUpdwvKGNMEmAJ0ttZ2L7J+EHAlcACw1trJ+evbAPcC71lrPylr32pBib9kZ2dTq1YtAD3eQ8SjKqMF1Rf4AOhSZKeRwHSgg7U22xizwBgz0Fq73Fq72xgzHngdOCWgjDFjgbEALVu2PKMfRqS8atasSYsWLUhOTi5282+9evV4/PHHNSuFiIeVO6Cste8YYwactPpCIMlam53/ehUw1BgTbq1daq09aoypXcr+ZgAzwGlBnXHlIuX017/+lXHjxrF48WIAfv75Zw4fPkyjRo3485//7HJ1IlKait6o2wjILPI6I39drDHm/4wx9wGzK3gMkQoZMWIEycnJ7Nu3j3379hUE1fjx49m9e7fL1YlIaSoaUAeAoi2kOsABa+0/rLWPW2uftNbOLe3DeuS7uKFXr148/fTTACxZsoTc3NyCRdMniXhHRQNqNdDKGFMz/3UfYFF5P6xHvotbfvvb3wJw1113ER4eXrDExMSQlpbmcnUiAmdwDcoY0x8YDTQ1xjwAPGutzTLG/A540RhzENhsrV3up1pFKk2TJk2YNWsW33//fcG6rVu38tZbb9GiRQtuvPFGpk6d6mKFIuLqjbrGmOHA8Li4uNt27NjhWh0iAKmpqfzlL38p6P5r1qzZGX0+NzeX3/72twWfF5HyKW2YuWaSEDnJ8uXLmTdv3hl/7rXXXgOgadOmxdbn5eXx6KOPFhvmLiKFFFAifrZs2TLeeuutU9a/+uqrgDPZ7clycnKoX78+X3zxBY0aNaJatWrUqFHD77WKeIknA0pdfFIVvP3226xZs6bE9xYvXsz27duLrVuwYAHt27cveN2sWTNq1y7xdkKRkODJgPJRC0qqqqysLF577TWysrJYv349b7/9donbvf/++9SuXZuLL74YY0yAqxTxLwWUiMfl5eWxZMkSMjIyCtbdd999JCUlFby+/PLL6datG+BM43T77bdrlnYJep4MKHXxiZQtJyeHr7/+mm+++YbrrrvulPdr1KjBN998Q+vWrQNfnEgl8WRA+agFJXJ6eXl5BTOyHzt2jO7du7Nt2zbAmfwWIDY2lvXr1+ualQQVPfJdJMiFhRVO/BIVFcXWrVt54oknOHDgAADbt2/n448/ZtKkSbRs2ZKxY8cSERHhVrkiFaYWlEiISExM5OKLL+bEiRMF63r27Im1lnbt2vHUU08BYIyhadOmGmwhnqEuPpEq4MSJE6Snp3P77bdz9OhRAJYuXXrKdmeddRZ33303gwYNonPnzoEuU6QYTwaUBkmI+F9KSgqLFi0quH41duzYYu+/9dZbxMXFUbNmTeLj49WykoDzZED5qAUlEji5ublkZWXxyCOP8MwzzxR778MPP2T48OEuVSZVlQJKRE6xcuVK0tLSSE9P58YbbyxYP23aNO644w4XK5OqRAElImV65ZVX2LhxI9OnTwegbdu2ANSpU4fFixfTpEkTN8uTEKZh5iJSpttvvx2AQYMG8e6772KMISUlhRUrVtCqVSuWLFlS7PpUw4YN6dixo1vlShWgQRIiUqrU1FRiY2NLfT8+Pp6mTZvSsGFDJk+eTHx8fACrk1ChLj4R+UWys7NZu3Ytubm5Bet2797Nyy+/TEREBKtWrSpY3759e2JiYujWrRu/+c1vTtlXt27dqF+/fkDqluChgBIRv8jMzGTevHm8+uqrREREsHLlyjK3X7VqFb179w5QdRIMFFAiEhD79+9n9+7dp6x/7LHHWLJkCeA84yoyMpL58+fTtWvXQJcoHqOAEhHXvfDCC2zcuJHk5GSWL19esP73v//9Kdv26dOHkSNHBrI8cYkCSkQ8w1rLtGnTeOWVV9i1axc1a9Ys9n5aWhoA3bt3Z8SIEZx99tkAtGzZkl69egW8XvEvTwaURvGJSEnmz5/Pgw8+yM6dO0t8f/To0bzxxhsBrkr8xZMB5aMWlIiUZP/+/Rw5cgSAvXv38vbbbzN79mwAevXqRWJiItWqVXOvQKkUCigRCQnbt28vuN9q5MiRjBkzhkGDBhV7XpYEFwWUiISMLVu20KlTp2LrjDFkZWVRq1Ytl6qSX6q0gNKfHCISdDp27Mj333/P2rVrufvuuwFn4EVERATGGDZu3OhugVIpFFAiEpSaNWtG9+7defbZZzl+/DgTJ04sePjiBRdcwNy5c12uUCpKASUiQS88PJzHH3+cDRs2MH78eABGjRpV6ihACQ4KKBEJGcYYnnrqKa677joA2rVrx7hx49i2bZvLlckvoYASkZAzd+5cRowYATizV7Rv355hw4bRtWtXNm3aRE5OjssVSnloFJ+IhLR77rmHFStWnDJw4qyzzqJNmzanndxW/M+Tw8w1k4SIBEpeXh4LFy5k0aJF5ObmMmvWrIL3tm3bRvPmzYmOjnaxwqrLkwHloxaUiARaUlISrVu3LrZu4cKFDBs2zJ2CqjDdByUiUkSrVq04ceIEc+bM4d577wVg+PDhHD582OXKxEctKBERnBGAPj179uSzzz6jRo0aLlZUdagFJSJSBmst1157LQBffPEFNWvW5PXXXycvL8/lyqouBZSISL758+dz7NgxmjVrBsBNN91EtWrVuPDCC8nIyHC5uqpHASUiUkTNmjX5/vvv2bNnD+3atQNgzZo11K1bl7CwsIK5/8T/FFAiIiVo3bo13377LdZannvuOTp06IC1lr/+9a8YY4iPj6ddu3Z89tlnbpcasjRIQkSknDZu3Mif//xnoqOjef/994u9N27cOC677DL69eunhyieId0HJSJSifLy8li8eDE33XQThw4dKvbeRx99xNChQ12qLPhoFJ+ISCUKCwtj2LBhpKamkpqayuLFiwveGzZsGMYY9u/f72KFwU8BJSJSQQ0bNmTIkCFYa9m6dWvB+mbNmnHnnXfihZ6qYKSAEhGpRB06dCA3N5eePXsCMG3aNMLCwvRsql/ALwFljLnMGDPeGHO/MeZqfxxDRMSrwsLCWLNmDUePHi1Y165dO4wxPPnkky5WFlzKHVDGmCbGmJnGmC9PWj/IGDPVGDPJGPNw/ur11tq/AC8B11ZivSIiQSMqKoqcnBymT59OXFwcABMnTiQmJoY//vGPLlfnfWfSguoLfAAUTFhljIkEpgN/stZOAjoZYwZaa/flb/I/wDOVVKuISNCpXr06t99+Ozt27GDFihX069ePQ4cO8eKLLzJp0iS3y/O0cgeUtfYdIPOk1RcCSdba7PzXq4ChAMaYocBuYB8lMMaMNcasM8asO3jw4BkXLiISbC6++GI+/fRT3n77bQAmT56MMYZp06a5XJk3VfQaVCOKh1YG0MgYcwXwAHA9UGKHq7V2hrU2wVqbEBsbW8EyRESCx1VXXcU777xD8+bNAbjzzju56667XK7KeyoaUAeA2kVe1wEOWGvft9ZeaK29w1o7qoLHEBEJOSNGjCA5OZnly5cDMHXqVOrVq8fPP//scmXeUdGAWg20MsbUzH/dB1hU3g8bY4YbY2akp6dXsAwRkeB0ySWX8J///AeA9PR0IiMj+fLLL8v+UBVxJqP4+gOjgabGmAeMMRHW2izgd8CLxpgpwGZr7fLy7tNau9BaO7Zu3bpnXLiISKjo378/6enpxMfHA9CjRw/GjBnD119/7XJl7nJ1Lj5jzHBgeFxc3G07duxwrQ4REa+YO3cuDz30ELt27QJg1qxZjBkzxuWq/MuTc/GpBSUiUtz111/Pzp07ee211wC4+eab6dixI3v27HG5ssDTVEciIh508803880339CgQQO2bt1KmzZtiI6O5r333nO7tIBxNaA0SEJEpHTnnHMOhw4d4uOPP6Z58+b89NNPXHnllbRq1Ypjx465XZ7fqYtPRMTjLr30UpKTk9m0aRMNGzbku+++IyIigoceeoiMjAy3y/MbdfGJiASJTp06cfDgQUaOHAnAo48+St26dfnb3/4Wko/0UECJiAQRYwzz5s3j6NGjPPOMM9XpH/7wB1q1asWRI0fcLa6S6RqUiEgQioqK4p577uHo0aOMHj2a5ORkBgwYEFItKV2DEhEJYlFRUbzxxhtcf/31bNq0ibCwsIIh6sFOXXwiIiHgH//4B7fddhsAt956Ky+//LLLFVWcAkpEJAQYY5gxYwYzZswA4Pe//z2DBw8mNzfX5cp+OV2DEhEJIbfddhs//vgj55xzDp988gljx451u6RfTNegRERCTKNGjdi+fTvt27dn1qxZ9O7dOygHT6iLT0QkBBlj+PzzzwFYvXo17du3d7miM6eAEhEJUXXr1iU7OxuA7du307JlSzIzM0/zKe9QQImIhLAaNWoUhFRycjI9e/YMmu4+DZIQEQlxNWrUIC8vj1tvvZVt27YFzcAJDZIQEakCfMPQr776ambOnMmjjz7qdkmnFe52ASIiEhjGGN58801SUlJ46KGHSElJYerUqW6XVarQuQZ15AgcP+52FSIinla9enWWLl1K48aNmTZtGuPGjXO7pFKFRkDt2QNnnw1//7vblYiIeF5kZCR79+4F4IUXXmDXrl3uFlSK0Aio1q3hvPNgyhTIH60iIiKlq1WrFhs3bgTgxhtvdLeYUoRGQBkDjz4Kycnw6qtuVyMiEhQ6d+7MpZdeyqpVq3jwwQfdLucUxs3x8MaY4cDwuLi423bs2FGxnVkLAwbAjh2waxdERFRGiSIiIe3HH3+kVatWZGdnk5iYSJ8+fQJegzFmvbU24eT1oTPM3NeKSkkBD49KERHxksaNG7Nz504ABg8ezOHDh12uqFBodPH59OsHgwc7QXXggNvViIgEhebNmzN37lyysrK466673C6nQGgFFMALL0BWFtx3n9uViIgEjeuuu46+ffuycOFCMjIy3C4HCMWAOu88uPtuZ8h5/ky+IiJyevfddx9Hjx5l4sSJbpcChGJAATzwALRsCTff7LSmRETktIYOHcqoUaOYOnUq8+fPd7ucEA2o6GinBfXNN+CRvwRERILBtGnTABg7diw5OTmu1hKaAQVwySXwhz/Aiy/C8uVuVyMiEhRq167NY489RmZmJvfcc4+rtbh6H5RPQkKCXbduXeXvOCsLunaFjAz46ito0qTyjyEiEmKstfTp04fVq1eTmZlJdHS0X4/nyfug/C4yEt56y5lI9pprNJmsiEg5GGMKhpu7OcNE6D+wsFMnZ/qjlSth/Hj/HUdEJISMGjWKgQMH8vzzz5OSkuJKDaEzk0RZRo1yrkc9/zy88op/jyUiEiJGjx4NwIIFC1w5fmh38RX17LMwdCjceSd8+KHb1YiIeN6oUaMA+OCDD1w5ftUJqPBw+Oc/oVs3GDlSN/GKiJxGeHg448aNY9myZezevTvgx686AQUQFQUffQTNmsGQIbBmjdsViYh42tChQwFYsmRJwI9dtQIKoFEjWLECYmPhV79SSImIlGHgwIE0btyYOXPmBPzYVS+gAFq0gP/8xwmpwYMhMdHtikREPMkYw8iRI1mzZg0///xzQI9dNQMKoHlzJ6SaNoVBg+Ddd92uSETEky688EIAPvnkk4Aet+oGFDghtWqVM9vEVVfBSy+5XZGIiOcMGTIEgHcD/Id81Q4ogJgYWLYMLrsM/vd/nUUzToiIFKhTpw7nn38+r7/+Onl5eQE7rgIKnCmRFixwniP10kvORLM//OB2VSIinnHllVcCge3mU0D5VKvm3Mw7dy6sX+/cL7VqldtViYh4gm9uvjfffDNgx/RLQBljwo0x9xtjZvhj/3513XXO0PNataB/f5g8GU6ccLsqERFXNWrUiPr167N3796AHdNfLagoYIkf9+9fnTrBhg1OWE2aBP36wZ49blclIuKqIUOGsH79+oBdhyp3gBhjmhhjZhpjvjxp/SBjzFRjzCRjzMMA1tp04FAl1xpYderAnDnw5pvw9ddOaE2dCgG8QCgi4iXdunUjKyuL5OTkgBzvTFo4fYEPAONbYYyJBKYDf7LWTgI6GWMGVmqFbrv+eti0CXr1grvuclpT27a5XZWISMC1b98egK1btwbkeOUOKGvtO0DmSasvBJKstdn5r1cBQ8uzP2PMWGPMOmPMuoMHD5a3DHe0bg0ffwyzZ8N//wtdujjXpgJ8V7WIiJt8N+wGal6+il4jakTx0MoAGhljDHAtcK4xpmtJH7TWzrDWJlhrE2JjYytYRgAYAzfe6LSerrzSuTYVHw/vvAPWul2diIjf1a1bl/DwcNauXRuQ41U0oA4AtYu8rgMcsI6nrLUXWWu/Ku3DAXmibmVr3BjmzYN//xvq1oWrr4YBA2DjRrcrExHxu/j4eL76qtRf65WqogG1GmhljKmZ/7oPsKi8Hw7YE3X9YcAA+OormD7d6fbr2tV5cu/OnW5XJiLiNwkJCZwI0K03ZzKKrz8wGmhqjHnAGBNhrc0Cfge8aIyZAmy21i73U63eU60a3H477NgBEybA++/DeefB2LEQoFEuIiKB5Lskk5aW5vdjnckgiU+ttbdYa5tZa6dYa3/OX/+JtfZ2a+0D1trJZ3LwoOziK0m9evDEE7Brl/NI+ddfh7g4Z16/pCS3qxMRqTQ9e/YEYN26dX4/lqs30gZ1F19JmjSBF190WlQ33OB0/8XFOYMrvv7a7epERCqsRYsWABw65P9bXYNzpgeva9kSXn0Vdu+G3//eGel3/vlwxRXOwxE16k9EglSTJk0AAjLlkasBFTJdfKVp0QL++lf47jt4+GFYuRIuusgZUDFrlu6jEpGg07hxYwB2BmBAmLr4AqFhQ+e+qe++g1decSafveUWJ8Duu0/XqUQkaNSoUQOAHwLwSCJ18QVSVJQzwm/zZuc+qv794emnoU0b+M1vnK7AnBy3qxQRKVPjxo2pVauW34+jgHKDMc59VAsWOLOk/9//wZYtzk2/zZrBn/4EAZrrSkTkTLVq1YrU1FS/H0fXoNzWsiU8+ijs3Qv/+hdcfDG8/DJ07Ag9ejijAvV0XxHxkGbNmrF69Wq/H0fXoLyiWjX49a/hrbdg/354/nmnu++Pf3RaVZde6gysOHLE7UpFpIoLDw+nevXqfj+Ouvi8KCbGCaaNG51plO6/3+kKvOUWZy7AK65w5gPMyHC7UhGpguLi4sjKyvL7cRRQXhcfD4884tz8u3at80yqL790nlMVEwNDhsCMGfDjj25XKiJVRG5uLgA/+/lWGV2DChbGQPfu8NxzznD1VaucVtaOHc58gE2bQt++8OyzzpRLIiJ+4rtZ98CBA349jq5BBaNq1aB3b2eI+o4dzrD1SZPgp5/g3nud6ZXOPdcZDfjxx3DsmNsVi0gI8f3O9vfDZtXFF+yMcUb8PfQQbNjgTK/0wgvOvVXTp8OvfuXcKDxsGEyd6lzLEhGpgPPOOw+AlJQUvx4n3K97l8A7+2z4wx+cJSsLPv0UFi92hrAvyn9U17nnwsCBcMklzrD2Bg3crVlEgkqD/N8ZR/w8qlgBFcoiI51BFEOGOK937HDCaulSeOMNp0VlDHTpUhhYF10E0dGuli0i3hYZGQnAvn37/HocDZKoStq1cwZWLF4MaWnOQIvJk51H17/4ojPdUv36zmCLBx90rl9pKLuInCQ6/49YX1D5i6stKGvtQmBhQkLCbW7WUSVVr+4MtOjd2wmjrCz4/HNYvtxZHn8c8vIgLAw6d3ZC66KLnK9Nm7pdvYi4yDdhbI6f5w5VF584IiNh0CBnAcjMhC++cB4RkpgIr70Gf/ub816bNk5Q9e3rBNx55zkjC0WkSvAFVHZ2tl+Po4CSktWuXTywjh93ZrZITHSWJUuc61i+bRMSoGdPZ/7Anj3hrLNcK11E/Ms3zZG/nwmlgJLyqV7duVG4e3fn/iprnUEXX3xRuDz7rBNk4MwfWDSwunVzgkxEgp4xBoCGDRv69TgKKPlljIFzznGW0aOddceOOa0sX2CtXQvvvlu4/bnnOk8TvuCCwq/167v2I4jIL1evXj2O+/4g9RMFlFSeWrWgVy9n8UlNdeYOXLsWvvrKuaY1d27h+61bO2FVNLjyp1EREe+qXr16aA+SMMYMB4bHxcW5WYb4k29CW9+9WAAHDzqzXmzY4ITWhg2FLS1wAqpzZ2eGjE6dnK/x8VCzZuDrF5EShYeH8+233/r3GH7d+2lomHkVFRsLgwc7i09GBmza5ATWV1858wv++9/OM7HAGSV4zjlOWBVdWrd2hsKLSEClpqbSqFEjvx5DXXziDXXqOPdZXXRR4boTJ5yBGFu2FC7r1jkPdfSJjoYOHZywat/eaWnFx0OLFgouET9q166drkFJFRYeXhg411xTuD4zE77+unhwvfcezJxZuE1kpHN/lu/zvu/j4iD/Hg4R+eWqV6+ugBI5Re3apw7GAOfa1rZthcv27c6gjDffLNwmPBzati0MLt/Srp0z5ZOIlEt4eDgnTpzw7zH8uneRQIqNdZZ+/YqvP3oUvvmmeHBt2wYffeR0Ixb9fLt2py5xcbqHS+Qk1atXZ5efH46qgCrD2rVrGT9+PDk5OQwePJiDBw8SFhbGRRddxPjx4+nduzfnnHMOANu3b2fkyJFkZGTw0EMPcdVVV/HMM88AkJiYyAMPPECXLl145JFHqFOnjps/VtUTHe3cKNytW/H1x487Tx/evt251uVbli2D118vvm2TJqWHl58nzBTxov3799OsWTO/HiMoAmrcuHFs3LixUvfZpUsXnn/++TK36dGjBwMGDODo0aNMmjQJgP79+zNkyBBat27N9ddfz7BhwwD473//C0D79u356KOPWLhwIT179uTqq6+mb9++DBgwgJtuuknh5CXVqzvXpvIfvlbMTz854VU0uHyPK/nhh+LbnnWW8xyuNm2cxff92Wc772mwhoSgdu3a8fPPP/v1GEERUF5x4sQJUlNTiYmJKbZ+2bJlHD16lCuuuAJwpqB///33GThwIB06dKB9+/YuVCsVEhXl3IPVqdOp72Vmws6dhaG1a5fzJONPP4V//MOZBsqnZk1nKHzR0Cr6Vde9JEiFhYVhi/5b94OguFH3dC0df/v888+ZNGkShw4d4v7776dHjx4AvPrqqyxbtozk5GRG+6b7yRcfH8+0adMYMWIEX3zxhRtli7/Uru3MenHBBae+l5MDSUmwZ48TWr6vu3c70z8dPlx8+/r1CwOrVSto2bL41/r1nWmiRDwmLCyMvLw8vx5DN+qWQ+/evQu6+Iq67bbbGDZsGGlpaeTm5p7y/uWXX86GDRu48cYb6VTSX+ISemrUKLw+VZIjR0oOr82bnUEbx44V3z4q6tTQKvq1WTNnZKJIgBljQjugQkWDBg1Kfe/hhx/mf/7nf5g1axZjxowJYFXiSfXqld76staZuzApCb77rvCr7/v1652h9EWFhTkhVVJwNW/uLDExaoVJpQv5Lj6vW7duHZ999hk5OTksWLCAESNGALBgwQKSkpL45z//SaNGjQq6/ADefPNNNm/ezPTp07njjjswxjBnzhx6nXzPjsjJjCkcKp+QUPI2WVmQnFxyiK1e7cyycfK9KTVqFAbWyV993zdtqpaYnJFAdPEZfydgeSQkJNh169a5XYZI8MvNdUYZ7tvnLN9/X/i16PcndyWGhTlD6U8OLt9XX4hFR7vzc4nnXHbZZSQnJ7Nhw4YK78sYs95ae8pfZfqTSSSUVKtWGCilsRbS0koOru+/h2+/hRUrID391M/Wru0EVUnLWWcVfl+3rroVQ5y6+ESk8hkDDRs6S1mDd44eLd4KS0kpXPbvd57zlZLidDuerFat04dY06ZODbpPLChpkISIuCc62nkK8rnnlr6Ntc59YUXDq2iIpaQ4E/suW1Zyiyw83OlabNQIGjd2lqLfF30dE+O0EMUT1IISEW8zxnlUSp06ZQcZOC2tH34oOcwOHIAff3Rmpv/xR2caqpKOFRNTcniV9L0ecOlXIX8flIhUIZGRhdNBlcVap7X144/O4guvk1+vWeN8/emnkvdTt27J4RUT44yULPo1JkajGM+QuvhEpOoxxrlfrF6907fKwAkoX2iVFmZff+08oTktrfT91Kt3anCV9TU6ukoPBFEXXwjJzMxk3Lhx5ObmMnv2bLfLEQkdUVHOVFFnn336bY8fh0OHnBueU1NL/7p3r/P05oMHS+5uBKcL0df6Ol2gNWwIDRqEVLejMYadO3f69RjBEVDjxkElz2ZOly4QwDn+ateuzejRoxVOIm6qXt0ZlNGkSfm29w0COV2gHTzohNrBgyUPBvGJiioMq6JfS/u+QQNnPkYPdj+mpKTQtGlTvx7Dez+1h/z9739n4sSJ3HHHHezatYulS5fSu3dvevfuzZYtW7jnnnvo0qULTz/9NJMnT2by5MmsX7+e9PR0PvzwQ6pVq8bSpUuZOnUqvXr14nCRiUIzMzO59957adOmDUlJSfzqV7/i8ssvZ8KECcybN48777yTlStX0qVLF+rVq8eXX35JdHQ0s2bNcvGMiFQxRQeBtG1bvs/k5DittKIBlpbmrDt0qPD7tDRnDkbf92Vdz6lXr/QAKy3g/Hwv2tlnn82ePXv8tn/ww0wSxphIYBLwHfCjtfbt033GyzNJDBgwgPHjx/Ob3/yGxMREIiMj6dq1K1999RVPPPEEb7/t/HitW7dm6dKlnHvuuQwdOpRHHnmECy64gLPOOouNGzfSpEkTZs6cSWJiIrNnz2bixIk0bNiQe++9l+zsbNq2bcuWLVuoX78+ERERHDx4kMjISGJjY1m7di1t27alc+fOrFixgoYNG7p8VkSkUuXlQUbGqQF2cqid/P6RI6Xvs1o1p/VV0uJrmZW2lOP62pgxY1ixYgVJSUkV/vErNJOEMaYJMAXobK3tXmT9IOBK4ABgrbWT819/aa192xjzPnDagPK6+Ph4wHlA17PPPsu//vUvMjIyOHjSxJ2+p+vGxsaSmZlJamoqWVlZNMnvTmjTpg2JiYkAbN68mVtuuQWAmjVrUr9+fXbu3En37t1p3Lgx0flTytSuXZu2+X+51a9fn8zMTAWUSKgJCyscGFLelho48y4ePlx2qB0+XLjNrl2Fr8tqsYWHlx1gDRrQd8cOrEceWNgX+ADo4luR31KaDnSw1mYbYxYYYwYCLYDV+ZtFlLZDY8xYYCxAy5Ytz7zyADL5f0k8+eST1K9fn/vvv59vv/32lOc8mZP+4oiJiSEiIqKgr3b37t0F73Xu3Jldu3YBcOzYMQ4fPky70h7RICJSkvDwwgmGz4Tv2povuHyhVdKSluZ0U+7Y4bw+cgTy8rgFGOznG6fLFVDW2neMMQNOWn0hkGStzc5/vQoYCnwF+M5WqfFqrZ0BzACni6/8JQfOJ598QlJSEi+99BITJkxgxIgRTJw4kezsbHJyckhKSmL58uWkpaWRnp7OrFmz6NKlC5s3b2bOnDn06dOH2bNnc+utt9K9e3d++OEHNm/eTGJiIhMnTuTuu+9mypQpfPfdd7z88svUq1ePmTNnkp6eznvvvQdAeno6s2fPplWrViQlJTFz5kymTJni8pkRkaBW9Npaq1Zn9tm8PMjM5Idt2zhx8gM4K1m5r0HlB9Qzvn5CY8x1wLXW2ivyX98KDMBpFU0iRK5BiYiIf/ljNvMDQO0ir+sAB6y1WcD4chZVrke+i4hI1VORaYRXA62MMb47z/oAi85kB9bahdbasXXr1q1AGSIiEorKFVDGmP7AaKCpMeYBY0xEfkvpd8CLxpgpwGZr7XI/1ioiIlVIeQdJfAp8WsL6T4BPfunB1cUnIiKlcfVJYeriExGR0uhRliIi4kmuBpQxZrgxZkZ6WZMriohIlaQuPhER8SR18YmIiCdV+mzmv6gIYw4CpU2JGwOkBrCcYKPzUzadn7Lp/JRN56dslXV+WllrT5lQ0BMBVRZjzLqSpsAQh85P2XR+yqbzUzadn7L5+/yoi09ERDxJASUiIp4UDAE1w+0CPE7np2w6P2XT+Smbzk/Z/Hp+PH8NSkREqqZgaEGJiEgVpIASERFPqsgDCyuVMWYQcCXOgxCttXbySe/XAp4B9gHtgCettd8GvFCXlOP8TACaAClAAvCQtXZ7wAt1yenOT5HtRgH/AGpba48GsERXlePfjwH+N/9la6CetfbmgBbponKcn7Nxfv98CXQB5lprPwx0nW4wxjQBpgCdrbXdS3g/DHgcyMT5t/OatXZNpRzcWuv6AkQCO4Ga+a8XAANP2uY+YHz+9x2BlW7X7bHz8yiF1xSvBRa6XbeXzk/++njgMcAC0W7X7aXzg/O8txuKvO7kdt0eOz/TgD/lf38BsMPtugN4fq4ChgPrSnl/JDA1//sGwLdAtco4tle6+C4Ekqy12fmvVwFDT9pmKM5TfLHWbgE6G2PqBK5EV532/FhrH7T5/0Jwum6rTOuAcpwfY0wkMB4osWUV4srz/9cooIEx5g/GmMfRv5+Tz8+PgG+mg1hgfYBqc5219h2c1lFpiv5uTgOOAR0q49he6eJrRPETkJG/rjzbZPi3NE8oz/kBwBhTA7gRuCsAdXlFec7PY8Aj1tocpzerSinP+WkF1LHWPmKMOQdYYoyJt9bmBqpIF5Xn/DwHvGeMeQ7ogdNjIY5y/346U14JqANA7SKv6+SvO9NtQlW5fvb8cJoG3G+t3RWg2rygzPNjjGkB1AeuLRJOdxtjFltr1wWsSveU599PBvAFgLX22/zeiRbA3kAU6LLynJ/ZwExr7TxjTCywwxjTJr/FUNX57XezV7r4VgOtjDE181/3ARYZYxoU6cZbhNMUxxjTEdhkra0KrScox/nJ78J6BXjOWrveGDPCpVrdUOb5sdYmW2tvstY+aa19Mn+b56pIOEH5/v9aDrQByF9XDfgh4JW6ozznpwXOACSAw0Ae3vn9GXDGmKj8oIbiv5sbALWAryvlOIWXLdxljLkU52LcQeC4tXayMeYvQJq19kljTATOKJoUIA543FatUXynOz/vAucD+/M/EmVLGHETqk53fvK3iQVux+meeRR4xVq7z62aA6kc/37qAn/BeapAW2CBtXaxexUHVjnOT19gHPAVcDaw3lo73bWCA8gY0x+4Afg1Tg/Ns8DNQEdr7R35o/ieALKAlsCrtpJG8XkmoERERIqqsk1UERHxNgWUiIh4kgJKREQ8SQElIiKepIASERFPUkCJiIgnKaBERMST/h+PROgQvormOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "analysis(pfn_original,original_training_data[1][2], original_training_data[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe8f2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_preds = pfn_original.predict(original_training_data[1][2], batch_size=10000)\n",
    "original_roc_info = roc_curve(original_training_data[1][-1][:,1], ori_preds[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d35f4",
   "metadata": {},
   "source": [
    "# Test of recon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5594be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_betas = raw_b_signals[\"beta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a706d425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14285715 -0.2857143  -0.42857143 -0.5714286  -0.71428573 -0.85714287\n",
      " -1.         -1.1428572  -1.2857143  -1.4285715  -1.5714285  -1.7142857\n",
      " -1.8571428  -2.         -2.142857   -2.2857144  -2.4285715  -2.5714285\n",
      " -2.7142856  -2.857143   -3.         -3.142857   -3.2857144  -3.4285715\n",
      " -3.5714285  -3.7142856  -3.857143   -4.         -4.142857   -4.285714\n",
      " -4.428571   -4.571429   -4.714286   -4.857143  ]\n"
     ]
    }
   ],
   "source": [
    "beta_idx = np.logical_and(log_betas<0, log_betas>-5)\n",
    "new_betas = log_betas[beta_idx]\n",
    "new_betas = new_betas\n",
    "print(new_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a5c948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_1_recons = raw_b_signals[\"recon\"][beta_idx]\n",
    "signal_2_recons = raw_hv_signals[\"recon\"][beta_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e7557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 15.6798 - acc: 0.6802 - val_loss: 0.9253 - val_acc: 0.6794\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.5656 - acc: 0.7370 - val_loss: 0.5101 - val_acc: 0.7414\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.5249 - acc: 0.7379 - val_loss: 0.4623 - val_acc: 0.7688\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.4796 - acc: 0.7571 - val_loss: 0.4681 - val_acc: 0.7740\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.4766 - acc: 0.7586 - val_loss: 0.4684 - val_acc: 0.7624\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.4577 - acc: 0.7680 - val_loss: 0.4623 - val_acc: 0.7617\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.4571 - acc: 0.7690 - val_loss: 0.4369 - val_acc: 0.7775\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.4540 - acc: 0.7719 - val_loss: 0.4378 - val_acc: 0.7789\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.4548 - acc: 0.7714 - val_loss: 0.4432 - val_acc: 0.7779\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.4520 - acc: 0.7731 - val_loss: 0.4950 - val_acc: 0.7471\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.4593 - acc: 0.7691 - val_loss: 0.4529 - val_acc: 0.7735\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.4468 - acc: 0.7768 - val_loss: 0.4605 - val_acc: 0.7744\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.4397 - acc: 0.7801 - val_loss: 0.4334 - val_acc: 0.7826\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.4413 - acc: 0.7802 - val_loss: 0.4328 - val_acc: 0.7846\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.4453 - acc: 0.7775 - val_loss: 0.4401 - val_acc: 0.7797\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.4361 - acc: 0.7836 - val_loss: 0.4343 - val_acc: 0.7830\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.4384 - acc: 0.7825 - val_loss: 0.4258 - val_acc: 0.7906\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.4408 - acc: 0.7818 - val_loss: 0.4428 - val_acc: 0.7806\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.4406 - acc: 0.7805 - val_loss: 0.4251 - val_acc: 0.8008\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.4443 - acc: 0.7789 - val_loss: 0.4475 - val_acc: 0.7741\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.4417 - acc: 0.7805 - val_loss: 0.4212 - val_acc: 0.7948\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.4388 - acc: 0.7827 - val_loss: 0.4741 - val_acc: 0.7697\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.4387 - acc: 0.7817 - val_loss: 0.5389 - val_acc: 0.7247\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.4440 - acc: 0.7800 - val_loss: 0.5203 - val_acc: 0.6986\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.4390 - acc: 0.7837 - val_loss: 0.4199 - val_acc: 0.7948\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.4395 - acc: 0.7834 - val_loss: 0.4567 - val_acc: 0.7647\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.4308 - acc: 0.7881 - val_loss: 0.4338 - val_acc: 0.7901\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.4403 - acc: 0.7844 - val_loss: 0.4232 - val_acc: 0.7977\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.4329 - acc: 0.7881 - val_loss: 0.4105 - val_acc: 0.8016\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.4348 - acc: 0.7866 - val_loss: 0.4585 - val_acc: 0.7670\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.4380 - acc: 0.7846 - val_loss: 0.4336 - val_acc: 0.7772\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.4302 - acc: 0.7895 - val_loss: 0.4309 - val_acc: 0.7952\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.4285 - acc: 0.7901 - val_loss: 0.4562 - val_acc: 0.7773\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.4320 - acc: 0.7894 - val_loss: 0.4180 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.4185 - acc: 0.7981 - val_loss: 0.4456 - val_acc: 0.7794\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.4168 - acc: 0.7980 - val_loss: 0.4092 - val_acc: 0.8147\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.4184 - acc: 0.7975 - val_loss: 0.4248 - val_acc: 0.7905\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.4172 - acc: 0.7986 - val_loss: 0.4257 - val_acc: 0.8014\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.4052 - acc: 0.8064 - val_loss: 0.4069 - val_acc: 0.8050\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.4182 - acc: 0.7979 - val_loss: 0.4033 - val_acc: 0.8041\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.4165 - acc: 0.7994 - val_loss: 0.4013 - val_acc: 0.8120\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.4157 - acc: 0.7999 - val_loss: 0.4792 - val_acc: 0.7757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.4338 - acc: 0.7911 - val_loss: 0.4312 - val_acc: 0.7780\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.4228 - acc: 0.7959 - val_loss: 0.4328 - val_acc: 0.7916\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.4145 - acc: 0.8010 - val_loss: 0.4062 - val_acc: 0.8082\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.4120 - acc: 0.8022 - val_loss: 0.4020 - val_acc: 0.8041\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.3894 - acc: 0.8167 - val_loss: 0.3737 - val_acc: 0.8234\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.3863 - acc: 0.8189 - val_loss: 0.4036 - val_acc: 0.8064\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.3933 - acc: 0.8139 - val_loss: 0.3844 - val_acc: 0.8175\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.3850 - acc: 0.8188 - val_loss: 0.3666 - val_acc: 0.8324\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.3833 - acc: 0.8200 - val_loss: 0.3631 - val_acc: 0.8357\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.3848 - acc: 0.8192 - val_loss: 0.3791 - val_acc: 0.8232\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.3822 - acc: 0.8206 - val_loss: 0.3784 - val_acc: 0.8348\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.3739 - acc: 0.8268 - val_loss: 0.3552 - val_acc: 0.8411\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.3753 - acc: 0.8247 - val_loss: 0.3639 - val_acc: 0.8349\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.3752 - acc: 0.8239 - val_loss: 0.4032 - val_acc: 0.8060\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.3855 - acc: 0.8202 - val_loss: 0.3679 - val_acc: 0.8270\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.3761 - acc: 0.8255 - val_loss: 0.3600 - val_acc: 0.8366\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.3748 - acc: 0.8260 - val_loss: 0.3875 - val_acc: 0.8184\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.3635 - acc: 0.8323 - val_loss: 0.3494 - val_acc: 0.8426\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.3527 - acc: 0.8391 - val_loss: 0.3431 - val_acc: 0.8493\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.3524 - acc: 0.8377 - val_loss: 0.3383 - val_acc: 0.8455\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.3543 - acc: 0.8382 - val_loss: 0.3373 - val_acc: 0.8506\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.3478 - acc: 0.8424 - val_loss: 0.3453 - val_acc: 0.8519\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.3463 - acc: 0.8429 - val_loss: 0.3866 - val_acc: 0.8164\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.3479 - acc: 0.8415 - val_loss: 0.3358 - val_acc: 0.8450\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.3469 - acc: 0.8423 - val_loss: 0.3561 - val_acc: 0.8349\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.3387 - acc: 0.8475 - val_loss: 0.3198 - val_acc: 0.8611\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.3401 - acc: 0.8468 - val_loss: 0.3323 - val_acc: 0.8463\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.3339 - acc: 0.8496 - val_loss: 0.3148 - val_acc: 0.8641\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.3327 - acc: 0.8507 - val_loss: 0.3591 - val_acc: 0.8326\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.3346 - acc: 0.8495 - val_loss: 0.3298 - val_acc: 0.8561\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.3343 - acc: 0.8496 - val_loss: 0.3172 - val_acc: 0.8623\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.3261 - acc: 0.8558 - val_loss: 0.3502 - val_acc: 0.8389\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.3317 - acc: 0.8514 - val_loss: 0.3046 - val_acc: 0.8677\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.3334 - acc: 0.8503 - val_loss: 0.3091 - val_acc: 0.8667\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.3306 - acc: 0.8531 - val_loss: 0.3038 - val_acc: 0.8668\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.3218 - acc: 0.8579 - val_loss: 0.3067 - val_acc: 0.8643\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.3305 - acc: 0.8515 - val_loss: 0.3202 - val_acc: 0.8640\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.3257 - acc: 0.8556 - val_loss: 0.3151 - val_acc: 0.8645\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.3187 - acc: 0.8590 - val_loss: 0.3037 - val_acc: 0.8720\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.3189 - acc: 0.8592 - val_loss: 0.3677 - val_acc: 0.8307\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 83/200\n",
      "198/198 - 2s - loss: 0.3114 - acc: 0.8638 - val_loss: 0.3194 - val_acc: 0.8547\n",
      "Epoch 84/200\n",
      "198/198 - 2s - loss: 0.2998 - acc: 0.8713 - val_loss: 0.3109 - val_acc: 0.8633\n",
      "Epoch 85/200\n",
      "198/198 - 2s - loss: 0.3067 - acc: 0.8660 - val_loss: 0.3626 - val_acc: 0.8271\n",
      "Epoch 86/200\n",
      "198/198 - 2s - loss: 0.3071 - acc: 0.8665 - val_loss: 0.2943 - val_acc: 0.8757\n",
      "Epoch 87/200\n",
      "198/198 - 2s - loss: 0.3055 - acc: 0.8673 - val_loss: 0.2906 - val_acc: 0.8792\n",
      "Epoch 88/200\n",
      "198/198 - 2s - loss: 0.3023 - acc: 0.8693 - val_loss: 0.3302 - val_acc: 0.8489\n",
      "Epoch 89/200\n",
      "198/198 - 2s - loss: 0.2998 - acc: 0.8702 - val_loss: 0.2930 - val_acc: 0.8771\n",
      "Epoch 90/200\n",
      "198/198 - 2s - loss: 0.3099 - acc: 0.8645 - val_loss: 0.2903 - val_acc: 0.8773\n",
      "Epoch 91/200\n",
      "198/198 - 2s - loss: 0.3040 - acc: 0.8680 - val_loss: 0.2878 - val_acc: 0.8773\n",
      "Epoch 92/200\n",
      "198/198 - 2s - loss: 0.2993 - acc: 0.8707 - val_loss: 0.2867 - val_acc: 0.8775\n",
      "Epoch 93/200\n",
      "198/198 - 2s - loss: 0.2916 - acc: 0.8763 - val_loss: 0.3252 - val_acc: 0.8511\n",
      "Epoch 94/200\n",
      "198/198 - 2s - loss: 0.2968 - acc: 0.8724 - val_loss: 0.2895 - val_acc: 0.8779\n",
      "Epoch 95/200\n",
      "198/198 - 2s - loss: 0.3030 - acc: 0.8678 - val_loss: 0.2984 - val_acc: 0.8714\n",
      "Epoch 96/200\n",
      "198/198 - 2s - loss: 0.2910 - acc: 0.8761 - val_loss: 0.2840 - val_acc: 0.8821\n",
      "Epoch 97/200\n",
      "198/198 - 2s - loss: 0.2981 - acc: 0.8710 - val_loss: 0.2847 - val_acc: 0.8803\n",
      "Epoch 98/200\n",
      "198/198 - 2s - loss: 0.2992 - acc: 0.8708 - val_loss: 0.3327 - val_acc: 0.8524\n",
      "Epoch 99/200\n",
      "198/198 - 2s - loss: 0.2969 - acc: 0.8724 - val_loss: 0.3230 - val_acc: 0.8604\n",
      "Epoch 100/200\n",
      "198/198 - 2s - loss: 0.2897 - acc: 0.8771 - val_loss: 0.3086 - val_acc: 0.8603\n",
      "Epoch 101/200\n",
      "198/198 - 2s - loss: 0.2930 - acc: 0.8741 - val_loss: 0.4119 - val_acc: 0.7961\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 102/200\n",
      "198/198 - 2s - loss: 0.2855 - acc: 0.8794 - val_loss: 0.2783 - val_acc: 0.8852\n",
      "Epoch 103/200\n",
      "198/198 - 2s - loss: 0.2831 - acc: 0.8803 - val_loss: 0.2868 - val_acc: 0.8795\n",
      "Epoch 104/200\n",
      "198/198 - 2s - loss: 0.2836 - acc: 0.8802 - val_loss: 0.2819 - val_acc: 0.8828\n",
      "Epoch 105/200\n",
      "198/198 - 2s - loss: 0.2866 - acc: 0.8783 - val_loss: 0.2829 - val_acc: 0.8815\n",
      "Epoch 106/200\n",
      "198/198 - 2s - loss: 0.2849 - acc: 0.8792 - val_loss: 0.2806 - val_acc: 0.8855\n",
      "Epoch 107/200\n",
      "198/198 - 2s - loss: 0.2882 - acc: 0.8771 - val_loss: 0.2857 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 108/200\n",
      "198/198 - 2s - loss: 0.2782 - acc: 0.8839 - val_loss: 0.2753 - val_acc: 0.8872\n",
      "Epoch 109/200\n",
      "198/198 - 2s - loss: 0.2776 - acc: 0.8842 - val_loss: 0.2792 - val_acc: 0.8820\n",
      "Epoch 110/200\n",
      "198/198 - 2s - loss: 0.2784 - acc: 0.8834 - val_loss: 0.2829 - val_acc: 0.8795\n",
      "Epoch 111/200\n",
      "198/198 - 2s - loss: 0.2823 - acc: 0.8807 - val_loss: 0.2775 - val_acc: 0.8869\n",
      "Epoch 112/200\n",
      "198/198 - 2s - loss: 0.2834 - acc: 0.8791 - val_loss: 0.2747 - val_acc: 0.8874\n",
      "Epoch 113/200\n",
      "198/198 - 2s - loss: 0.2768 - acc: 0.8840 - val_loss: 0.2756 - val_acc: 0.8860\n",
      "Epoch 114/200\n",
      "198/198 - 2s - loss: 0.2771 - acc: 0.8842 - val_loss: 0.2790 - val_acc: 0.8846\n",
      "Epoch 115/200\n",
      "198/198 - 2s - loss: 0.2764 - acc: 0.8840 - val_loss: 0.2762 - val_acc: 0.8854\n",
      "Epoch 116/200\n",
      "198/198 - 2s - loss: 0.2793 - acc: 0.8825 - val_loss: 0.2929 - val_acc: 0.8728\n",
      "Epoch 117/200\n",
      "198/198 - 2s - loss: 0.2798 - acc: 0.8820 - val_loss: 0.2804 - val_acc: 0.8831\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 118/200\n",
      "198/198 - 2s - loss: 0.2740 - acc: 0.8858 - val_loss: 0.2733 - val_acc: 0.8865\n",
      "Epoch 119/200\n",
      "198/198 - 2s - loss: 0.2732 - acc: 0.8860 - val_loss: 0.2824 - val_acc: 0.8799\n",
      "Epoch 120/200\n",
      "198/198 - 2s - loss: 0.2743 - acc: 0.8858 - val_loss: 0.2876 - val_acc: 0.8766\n",
      "Epoch 121/200\n",
      "198/198 - 2s - loss: 0.2754 - acc: 0.8852 - val_loss: 0.2838 - val_acc: 0.8835\n",
      "Epoch 122/200\n",
      "198/198 - 2s - loss: 0.2750 - acc: 0.8854 - val_loss: 0.2723 - val_acc: 0.8872\n",
      "Epoch 123/200\n",
      "198/198 - 2s - loss: 0.2730 - acc: 0.8861 - val_loss: 0.2732 - val_acc: 0.8881\n",
      "Epoch 124/200\n",
      "198/198 - 2s - loss: 0.2738 - acc: 0.8856 - val_loss: 0.2719 - val_acc: 0.8873\n",
      "Epoch 125/200\n",
      "198/198 - 2s - loss: 0.2764 - acc: 0.8838 - val_loss: 0.2966 - val_acc: 0.8707\n",
      "Epoch 126/200\n",
      "198/198 - 2s - loss: 0.2726 - acc: 0.8862 - val_loss: 0.2848 - val_acc: 0.8798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/200\n",
      "198/198 - 2s - loss: 0.2728 - acc: 0.8865 - val_loss: 0.2754 - val_acc: 0.8853\n",
      "Epoch 128/200\n",
      "198/198 - 2s - loss: 0.2753 - acc: 0.8851 - val_loss: 0.2798 - val_acc: 0.8826\n",
      "Epoch 129/200\n",
      "198/198 - 2s - loss: 0.2745 - acc: 0.8853 - val_loss: 0.2716 - val_acc: 0.8872\n",
      "Epoch 130/200\n",
      "198/198 - 2s - loss: 0.2740 - acc: 0.8860 - val_loss: 0.2824 - val_acc: 0.8798\n",
      "Epoch 131/200\n",
      "198/198 - 2s - loss: 0.2733 - acc: 0.8858 - val_loss: 0.2764 - val_acc: 0.8841\n",
      "Epoch 132/200\n",
      "198/198 - 2s - loss: 0.2728 - acc: 0.8863 - val_loss: 0.2735 - val_acc: 0.8865\n",
      "Epoch 133/200\n",
      "198/198 - 2s - loss: 0.2761 - acc: 0.8841 - val_loss: 0.2708 - val_acc: 0.8881\n",
      "Epoch 134/200\n",
      "198/198 - 2s - loss: 0.2724 - acc: 0.8862 - val_loss: 0.2774 - val_acc: 0.8860\n",
      "Epoch 135/200\n",
      "198/198 - 2s - loss: 0.2716 - acc: 0.8875 - val_loss: 0.2904 - val_acc: 0.8778\n",
      "Epoch 136/200\n",
      "198/198 - 2s - loss: 0.2748 - acc: 0.8849 - val_loss: 0.2733 - val_acc: 0.8869\n",
      "Epoch 137/200\n",
      "198/198 - 2s - loss: 0.2737 - acc: 0.8858 - val_loss: 0.2699 - val_acc: 0.8890\n",
      "Epoch 138/200\n",
      "198/198 - 2s - loss: 0.2723 - acc: 0.8864 - val_loss: 0.2750 - val_acc: 0.8849\n",
      "Epoch 139/200\n",
      "198/198 - 2s - loss: 0.2729 - acc: 0.8858 - val_loss: 0.2711 - val_acc: 0.8884\n",
      "Epoch 140/200\n",
      "198/198 - 2s - loss: 0.2715 - acc: 0.8875 - val_loss: 0.2760 - val_acc: 0.8861\n",
      "Epoch 141/200\n",
      "198/198 - 2s - loss: 0.2783 - acc: 0.8825 - val_loss: 0.2730 - val_acc: 0.8871\n",
      "Epoch 142/200\n",
      "198/198 - 2s - loss: 0.2718 - acc: 0.8863 - val_loss: 0.2714 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00142: ReduceLROnPlateau reducing learning rate to 1.5848932274101303e-05.\n",
      "Epoch 143/200\n",
      "198/198 - 2s - loss: 0.2704 - acc: 0.8872 - val_loss: 0.2738 - val_acc: 0.8861\n",
      "Epoch 144/200\n",
      "198/198 - 2s - loss: 0.2716 - acc: 0.8867 - val_loss: 0.2706 - val_acc: 0.8886\n",
      "Epoch 145/200\n",
      "198/198 - 2s - loss: 0.2699 - acc: 0.8881 - val_loss: 0.2708 - val_acc: 0.8879\n",
      "Epoch 146/200\n",
      "198/198 - 2s - loss: 0.2734 - acc: 0.8858 - val_loss: 0.2737 - val_acc: 0.8861\n",
      "Epoch 147/200\n",
      "198/198 - 2s - loss: 0.2706 - acc: 0.8873 - val_loss: 0.2691 - val_acc: 0.8889\n",
      "Epoch 148/200\n",
      "198/198 - 2s - loss: 0.2705 - acc: 0.8878 - val_loss: 0.2857 - val_acc: 0.8788\n",
      "Epoch 149/200\n",
      "198/198 - 2s - loss: 0.2696 - acc: 0.8882 - val_loss: 0.2781 - val_acc: 0.8843\n",
      "Epoch 150/200\n",
      "198/198 - 2s - loss: 0.2685 - acc: 0.8890 - val_loss: 0.2742 - val_acc: 0.8855\n",
      "Epoch 151/200\n",
      "198/198 - 2s - loss: 0.2688 - acc: 0.8884 - val_loss: 0.2725 - val_acc: 0.8867\n",
      "Epoch 152/200\n",
      "198/198 - 2s - loss: 0.2704 - acc: 0.8875 - val_loss: 0.2722 - val_acc: 0.8888\n",
      "\n",
      "Epoch 00152: ReduceLROnPlateau reducing learning rate to 1.0000000608891671e-05.\n",
      "Epoch 153/200\n",
      "198/198 - 2s - loss: 0.2684 - acc: 0.8886 - val_loss: 0.2694 - val_acc: 0.8880\n",
      "Epoch 154/200\n",
      "198/198 - 2s - loss: 0.2683 - acc: 0.8882 - val_loss: 0.2687 - val_acc: 0.8889\n",
      "Epoch 155/200\n",
      "198/198 - 2s - loss: 0.2680 - acc: 0.8889 - val_loss: 0.2706 - val_acc: 0.8885\n",
      "Epoch 156/200\n",
      "198/198 - 2s - loss: 0.2680 - acc: 0.8889 - val_loss: 0.2951 - val_acc: 0.8724\n",
      "Epoch 157/200\n",
      "198/198 - 2s - loss: 0.2694 - acc: 0.8876 - val_loss: 0.2689 - val_acc: 0.8890\n",
      "Epoch 158/200\n",
      "198/198 - 2s - loss: 0.2680 - acc: 0.8892 - val_loss: 0.2691 - val_acc: 0.8882\n",
      "Epoch 159/200\n",
      "198/198 - 2s - loss: 0.2690 - acc: 0.8879 - val_loss: 0.2802 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 160/200\n",
      "198/198 - 2s - loss: 0.2685 - acc: 0.8884 - val_loss: 0.2685 - val_acc: 0.8895\n",
      "Epoch 161/200\n",
      "198/198 - 2s - loss: 0.2687 - acc: 0.8881 - val_loss: 0.2726 - val_acc: 0.8868\n",
      "Epoch 162/200\n",
      "198/198 - 2s - loss: 0.2683 - acc: 0.8888 - val_loss: 0.2684 - val_acc: 0.8891\n",
      "Epoch 163/200\n",
      "198/198 - 2s - loss: 0.2673 - acc: 0.8893 - val_loss: 0.2701 - val_acc: 0.8890\n",
      "Epoch 164/200\n",
      "198/198 - 2s - loss: 0.2683 - acc: 0.8887 - val_loss: 0.2691 - val_acc: 0.8896\n",
      "Epoch 165/200\n",
      "198/198 - 2s - loss: 0.2680 - acc: 0.8888 - val_loss: 0.2686 - val_acc: 0.8894\n",
      "Epoch 166/200\n",
      "198/198 - 2s - loss: 0.2678 - acc: 0.8896 - val_loss: 0.2689 - val_acc: 0.8888\n",
      "Epoch 167/200\n",
      "198/198 - 2s - loss: 0.2681 - acc: 0.8886 - val_loss: 0.2685 - val_acc: 0.8897\n",
      "Epoch 168/200\n",
      "198/198 - 2s - loss: 0.2674 - acc: 0.8890 - val_loss: 0.2694 - val_acc: 0.8884\n",
      "Epoch 169/200\n",
      "198/198 - 2s - loss: 0.2673 - acc: 0.8893 - val_loss: 0.2682 - val_acc: 0.8892\n",
      "Epoch 170/200\n",
      "198/198 - 2s - loss: 0.2679 - acc: 0.8884 - val_loss: 0.2754 - val_acc: 0.8844\n",
      "Epoch 171/200\n",
      "198/198 - 2s - loss: 0.2695 - acc: 0.8879 - val_loss: 0.2685 - val_acc: 0.8896\n",
      "Epoch 172/200\n",
      "198/198 - 2s - loss: 0.2671 - acc: 0.8895 - val_loss: 0.2681 - val_acc: 0.8901\n",
      "Epoch 173/200\n",
      "198/198 - 2s - loss: 0.2680 - acc: 0.8884 - val_loss: 0.2853 - val_acc: 0.8797\n",
      "Epoch 174/200\n",
      "198/198 - 2s - loss: 0.2671 - acc: 0.8892 - val_loss: 0.2808 - val_acc: 0.8824\n",
      "Epoch 175/200\n",
      "198/198 - 2s - loss: 0.2677 - acc: 0.8889 - val_loss: 0.2690 - val_acc: 0.8889\n",
      "Epoch 176/200\n",
      "198/198 - 2s - loss: 0.2678 - acc: 0.8887 - val_loss: 0.2712 - val_acc: 0.8880\n",
      "Epoch 177/200\n",
      "198/198 - 2s - loss: 0.2669 - acc: 0.8894 - val_loss: 0.2699 - val_acc: 0.8881\n",
      "Epoch 178/200\n",
      "198/198 - 2s - loss: 0.2670 - acc: 0.8894 - val_loss: 0.2678 - val_acc: 0.8897\n",
      "Epoch 179/200\n",
      "198/198 - 2s - loss: 0.2673 - acc: 0.8893 - val_loss: 0.2725 - val_acc: 0.8874\n",
      "Epoch 180/200\n",
      "198/198 - 2s - loss: 0.2677 - acc: 0.8887 - val_loss: 0.2810 - val_acc: 0.8815\n",
      "Epoch 181/200\n",
      "198/198 - 2s - loss: 0.2679 - acc: 0.8884 - val_loss: 0.2762 - val_acc: 0.8854\n",
      "Epoch 182/200\n",
      "198/198 - 2s - loss: 0.2672 - acc: 0.8894 - val_loss: 0.2774 - val_acc: 0.8834\n",
      "Epoch 183/200\n",
      "198/198 - 2s - loss: 0.2665 - acc: 0.8899 - val_loss: 0.2682 - val_acc: 0.8895\n",
      "Epoch 184/200\n",
      "198/198 - 2s - loss: 0.2676 - acc: 0.8887 - val_loss: 0.2734 - val_acc: 0.8857\n",
      "Epoch 185/200\n",
      "198/198 - 2s - loss: 0.2678 - acc: 0.8886 - val_loss: 0.2695 - val_acc: 0.8877\n",
      "Epoch 186/200\n",
      "198/198 - 2s - loss: 0.2684 - acc: 0.8885 - val_loss: 0.2679 - val_acc: 0.8894\n",
      "Epoch 187/200\n",
      "198/198 - 2s - loss: 0.2677 - acc: 0.8884 - val_loss: 0.2677 - val_acc: 0.8895\n",
      "Epoch 188/200\n",
      "198/198 - 2s - loss: 0.2665 - acc: 0.8897 - val_loss: 0.2722 - val_acc: 0.8867\n",
      "Epoch 189/200\n",
      "198/198 - 2s - loss: 0.2668 - acc: 0.8897 - val_loss: 0.2680 - val_acc: 0.8902\n",
      "Epoch 190/200\n",
      "198/198 - 2s - loss: 0.2676 - acc: 0.8892 - val_loss: 0.2683 - val_acc: 0.8897\n",
      "Epoch 191/200\n",
      "198/198 - 2s - loss: 0.2676 - acc: 0.8891 - val_loss: 0.2686 - val_acc: 0.8888\n",
      "Epoch 192/200\n",
      "198/198 - 2s - loss: 0.2671 - acc: 0.8894 - val_loss: 0.2700 - val_acc: 0.8896\n",
      "Epoch 193/200\n",
      "198/198 - 2s - loss: 0.2674 - acc: 0.8891 - val_loss: 0.2681 - val_acc: 0.8896\n",
      "Epoch 194/200\n",
      "198/198 - 2s - loss: 0.2678 - acc: 0.8885 - val_loss: 0.2681 - val_acc: 0.8895\n",
      "Epoch 195/200\n",
      "198/198 - 2s - loss: 0.2668 - acc: 0.8894 - val_loss: 0.2716 - val_acc: 0.8876\n",
      "Epoch 196/200\n",
      "198/198 - 2s - loss: 0.2673 - acc: 0.8892 - val_loss: 0.2693 - val_acc: 0.8890\n",
      "Epoch 197/200\n",
      "198/198 - 2s - loss: 0.2662 - acc: 0.8894 - val_loss: 0.2684 - val_acc: 0.8897\n",
      "Epoch 00197: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 9.3666 - acc: 0.6822 - val_loss: 0.6214 - val_acc: 0.7490\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.5210 - acc: 0.7579 - val_loss: 0.3838 - val_acc: 0.8473\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.4375 - acc: 0.8014 - val_loss: 1.2048 - val_acc: 0.5565\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.3993 - acc: 0.8192 - val_loss: 0.3514 - val_acc: 0.8345\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.3754 - acc: 0.8311 - val_loss: 0.4368 - val_acc: 0.7784\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.3731 - acc: 0.8350 - val_loss: 0.4542 - val_acc: 0.7862\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.3591 - acc: 0.8411 - val_loss: 0.3612 - val_acc: 0.8408\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.3357 - acc: 0.8532 - val_loss: 0.3514 - val_acc: 0.8538\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.3229 - acc: 0.8581 - val_loss: 0.2813 - val_acc: 0.8888\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.3171 - acc: 0.8618 - val_loss: 0.2856 - val_acc: 0.8847\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.3205 - acc: 0.8612 - val_loss: 0.2887 - val_acc: 0.8847\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.3145 - acc: 0.8627 - val_loss: 0.3372 - val_acc: 0.8653\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.3199 - acc: 0.8596 - val_loss: 0.2960 - val_acc: 0.8827\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.7039 - acc: 0.7958 - val_loss: 0.3738 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.3059 - acc: 0.8702 - val_loss: 0.2935 - val_acc: 0.8747\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.2942 - acc: 0.8755 - val_loss: 0.3033 - val_acc: 0.8650\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.2913 - acc: 0.8765 - val_loss: 0.2748 - val_acc: 0.8839\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.2929 - acc: 0.8754 - val_loss: 0.3214 - val_acc: 0.8650\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.2970 - acc: 0.8731 - val_loss: 0.2951 - val_acc: 0.8745\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.2929 - acc: 0.8757 - val_loss: 0.2818 - val_acc: 0.8854\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.2872 - acc: 0.8798 - val_loss: 0.2808 - val_acc: 0.8812\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.2833 - acc: 0.8811 - val_loss: 0.2786 - val_acc: 0.8822\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.2729 - acc: 0.8876 - val_loss: 0.2763 - val_acc: 0.8836\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.2666 - acc: 0.8912 - val_loss: 0.2597 - val_acc: 0.8950\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.2704 - acc: 0.8890 - val_loss: 0.2630 - val_acc: 0.8927\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.2702 - acc: 0.8886 - val_loss: 0.3026 - val_acc: 0.8759\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.2739 - acc: 0.8860 - val_loss: 0.2713 - val_acc: 0.8948\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.2716 - acc: 0.8882 - val_loss: 0.2793 - val_acc: 0.8812\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.2681 - acc: 0.8897 - val_loss: 0.2633 - val_acc: 0.8932\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.2602 - acc: 0.8950 - val_loss: 0.2545 - val_acc: 0.8994\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.2619 - acc: 0.8929 - val_loss: 0.2636 - val_acc: 0.8922\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.2604 - acc: 0.8945 - val_loss: 0.2585 - val_acc: 0.8962\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.2595 - acc: 0.8947 - val_loss: 0.2546 - val_acc: 0.8973\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.2644 - acc: 0.8918 - val_loss: 0.2798 - val_acc: 0.8840\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.2580 - acc: 0.8953 - val_loss: 0.3026 - val_acc: 0.8707\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.2584 - acc: 0.8951 - val_loss: 0.2592 - val_acc: 0.8958\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.2585 - acc: 0.8948 - val_loss: 0.2916 - val_acc: 0.8748\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.2562 - acc: 0.8960 - val_loss: 0.2579 - val_acc: 0.8959\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.2552 - acc: 0.8965 - val_loss: 0.2518 - val_acc: 0.8985\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.2552 - acc: 0.8963 - val_loss: 0.2543 - val_acc: 0.8972\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.2537 - acc: 0.8971 - val_loss: 0.2565 - val_acc: 0.8954\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.2537 - acc: 0.8972 - val_loss: 0.2703 - val_acc: 0.8888\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.2545 - acc: 0.8969 - val_loss: 0.2472 - val_acc: 0.9009\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.2572 - acc: 0.8950 - val_loss: 0.2461 - val_acc: 0.9017\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.2512 - acc: 0.8983 - val_loss: 0.3000 - val_acc: 0.8705\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.2597 - acc: 0.8934 - val_loss: 0.2673 - val_acc: 0.8875\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.2516 - acc: 0.8973 - val_loss: 0.2587 - val_acc: 0.8965\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.2518 - acc: 0.8979 - val_loss: 0.2810 - val_acc: 0.8878\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.2533 - acc: 0.8972 - val_loss: 0.2481 - val_acc: 0.8999\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.2470 - acc: 0.9003 - val_loss: 0.2609 - val_acc: 0.8917\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.2477 - acc: 0.8996 - val_loss: 0.2427 - val_acc: 0.9019\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.2491 - acc: 0.8993 - val_loss: 0.2450 - val_acc: 0.9012\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.2446 - acc: 0.9009 - val_loss: 0.2411 - val_acc: 0.9042\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.2466 - acc: 0.9003 - val_loss: 0.2483 - val_acc: 0.8990\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.2465 - acc: 0.9004 - val_loss: 0.2532 - val_acc: 0.8969\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.2479 - acc: 0.8997 - val_loss: 0.2468 - val_acc: 0.9003\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.2467 - acc: 0.8999 - val_loss: 0.2468 - val_acc: 0.9007\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.2462 - acc: 0.8998 - val_loss: 0.2524 - val_acc: 0.8963\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.2436 - acc: 0.9014 - val_loss: 0.2455 - val_acc: 0.9012\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.2415 - acc: 0.9032 - val_loss: 0.2415 - val_acc: 0.9041\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.2407 - acc: 0.9034 - val_loss: 0.2405 - val_acc: 0.9033\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.2418 - acc: 0.9024 - val_loss: 0.2393 - val_acc: 0.9049\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.2410 - acc: 0.9032 - val_loss: 0.2399 - val_acc: 0.9041\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.2415 - acc: 0.9030 - val_loss: 0.2429 - val_acc: 0.9023\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.2415 - acc: 0.9032 - val_loss: 0.2399 - val_acc: 0.9034\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.2416 - acc: 0.9032 - val_loss: 0.2649 - val_acc: 0.8916\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.2397 - acc: 0.9038 - val_loss: 0.2764 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.2402 - acc: 0.9040 - val_loss: 0.2362 - val_acc: 0.9060\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.2374 - acc: 0.9048 - val_loss: 0.2375 - val_acc: 0.9045\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.2380 - acc: 0.9048 - val_loss: 0.2365 - val_acc: 0.9058\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.2365 - acc: 0.9050 - val_loss: 0.2364 - val_acc: 0.9049\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.2367 - acc: 0.9047 - val_loss: 0.2351 - val_acc: 0.9063\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.2382 - acc: 0.9046 - val_loss: 0.2394 - val_acc: 0.9038\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.2375 - acc: 0.9047 - val_loss: 0.2353 - val_acc: 0.9062\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.2364 - acc: 0.9056 - val_loss: 0.2336 - val_acc: 0.9061\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.2370 - acc: 0.9054 - val_loss: 0.2372 - val_acc: 0.9048\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.2376 - acc: 0.9049 - val_loss: 0.2341 - val_acc: 0.9064\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.2352 - acc: 0.9060 - val_loss: 0.2368 - val_acc: 0.9057\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.2360 - acc: 0.9054 - val_loss: 0.2341 - val_acc: 0.9063\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.2340 - acc: 0.9067 - val_loss: 0.2396 - val_acc: 0.9053\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.2334 - acc: 0.9069 - val_loss: 0.2338 - val_acc: 0.9067\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.2343 - acc: 0.9062 - val_loss: 0.2323 - val_acc: 0.9075\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 - 2s - loss: 0.2331 - acc: 0.9068 - val_loss: 0.2365 - val_acc: 0.9055\n",
      "Epoch 84/200\n",
      "198/198 - 2s - loss: 0.2334 - acc: 0.9066 - val_loss: 0.2325 - val_acc: 0.9069\n",
      "Epoch 85/200\n",
      "198/198 - 2s - loss: 0.2331 - acc: 0.9068 - val_loss: 0.2323 - val_acc: 0.9070\n",
      "Epoch 86/200\n",
      "198/198 - 2s - loss: 0.2326 - acc: 0.9075 - val_loss: 0.2316 - val_acc: 0.9073\n",
      "Epoch 87/200\n",
      "198/198 - 2s - loss: 0.2333 - acc: 0.9067 - val_loss: 0.2340 - val_acc: 0.9063\n",
      "Epoch 88/200\n",
      "198/198 - 2s - loss: 0.2329 - acc: 0.9069 - val_loss: 0.2315 - val_acc: 0.9073\n",
      "Epoch 89/200\n",
      "198/198 - 2s - loss: 0.2334 - acc: 0.9069 - val_loss: 0.2311 - val_acc: 0.9080\n",
      "Epoch 90/200\n",
      "198/198 - 2s - loss: 0.2323 - acc: 0.9074 - val_loss: 0.2310 - val_acc: 0.9081\n",
      "Epoch 91/200\n",
      "198/198 - 2s - loss: 0.2314 - acc: 0.9076 - val_loss: 0.2322 - val_acc: 0.9073\n",
      "Epoch 92/200\n",
      "198/198 - 2s - loss: 0.2315 - acc: 0.9079 - val_loss: 0.2314 - val_acc: 0.9083\n",
      "Epoch 93/200\n",
      "198/198 - 2s - loss: 0.2314 - acc: 0.9079 - val_loss: 0.2318 - val_acc: 0.9071\n",
      "Epoch 94/200\n",
      "198/198 - 2s - loss: 0.2310 - acc: 0.9082 - val_loss: 0.2335 - val_acc: 0.9066\n",
      "Epoch 95/200\n",
      "198/198 - 2s - loss: 0.2319 - acc: 0.9077 - val_loss: 0.2290 - val_acc: 0.9089\n",
      "Epoch 96/200\n",
      "198/198 - 2s - loss: 0.2311 - acc: 0.9084 - val_loss: 0.2291 - val_acc: 0.9093\n",
      "Epoch 97/200\n",
      "198/198 - 2s - loss: 0.2299 - acc: 0.9083 - val_loss: 0.2294 - val_acc: 0.9085\n",
      "Epoch 98/200\n",
      "198/198 - 2s - loss: 0.2305 - acc: 0.9085 - val_loss: 0.2336 - val_acc: 0.9065\n",
      "Epoch 99/200\n",
      "198/198 - 2s - loss: 0.2305 - acc: 0.9077 - val_loss: 0.2326 - val_acc: 0.9074\n",
      "Epoch 100/200\n",
      "198/198 - 2s - loss: 0.2297 - acc: 0.9088 - val_loss: 0.2319 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 1.5848932274101303e-05.\n",
      "Epoch 101/200\n",
      "198/198 - 2s - loss: 0.2287 - acc: 0.9090 - val_loss: 0.2321 - val_acc: 0.9081\n",
      "Epoch 102/200\n",
      "198/198 - 2s - loss: 0.2288 - acc: 0.9092 - val_loss: 0.2295 - val_acc: 0.9087\n",
      "Epoch 103/200\n",
      "198/198 - 2s - loss: 0.2282 - acc: 0.9090 - val_loss: 0.2289 - val_acc: 0.9088\n",
      "Epoch 104/200\n",
      "198/198 - 2s - loss: 0.2283 - acc: 0.9092 - val_loss: 0.2291 - val_acc: 0.9082\n",
      "Epoch 105/200\n",
      "198/198 - 2s - loss: 0.2280 - acc: 0.9093 - val_loss: 0.2290 - val_acc: 0.9088\n",
      "Epoch 106/200\n",
      "198/198 - 2s - loss: 0.2294 - acc: 0.9085 - val_loss: 0.2300 - val_acc: 0.9080\n",
      "Epoch 107/200\n",
      "198/198 - 2s - loss: 0.2275 - acc: 0.9097 - val_loss: 0.2287 - val_acc: 0.9086\n",
      "Epoch 108/200\n",
      "198/198 - 2s - loss: 0.2276 - acc: 0.9095 - val_loss: 0.2275 - val_acc: 0.9093\n",
      "Epoch 109/200\n",
      "198/198 - 2s - loss: 0.2269 - acc: 0.9100 - val_loss: 0.2283 - val_acc: 0.9093\n",
      "Epoch 110/200\n",
      "198/198 - 2s - loss: 0.2277 - acc: 0.9098 - val_loss: 0.2266 - val_acc: 0.9096\n",
      "Epoch 111/200\n",
      "198/198 - 2s - loss: 0.2264 - acc: 0.9100 - val_loss: 0.2310 - val_acc: 0.9084\n",
      "Epoch 112/200\n",
      "198/198 - 2s - loss: 0.2274 - acc: 0.9097 - val_loss: 0.2317 - val_acc: 0.9067\n",
      "Epoch 113/200\n",
      "198/198 - 2s - loss: 0.2275 - acc: 0.9099 - val_loss: 0.2274 - val_acc: 0.9098\n",
      "Epoch 114/200\n",
      "198/198 - 2s - loss: 0.2270 - acc: 0.9096 - val_loss: 0.2284 - val_acc: 0.9089\n",
      "Epoch 115/200\n",
      "198/198 - 2s - loss: 0.2265 - acc: 0.9102 - val_loss: 0.2275 - val_acc: 0.9098\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 1.0000000608891671e-05.\n",
      "Epoch 116/200\n",
      "198/198 - 2s - loss: 0.2261 - acc: 0.9098 - val_loss: 0.2261 - val_acc: 0.9095\n",
      "Epoch 117/200\n",
      "198/198 - 2s - loss: 0.2251 - acc: 0.9107 - val_loss: 0.2250 - val_acc: 0.9108\n",
      "Epoch 118/200\n",
      "198/198 - 2s - loss: 0.2253 - acc: 0.9105 - val_loss: 0.2255 - val_acc: 0.9109\n",
      "Epoch 119/200\n",
      "198/198 - 2s - loss: 0.2252 - acc: 0.9105 - val_loss: 0.2256 - val_acc: 0.9107\n",
      "Epoch 120/200\n",
      "198/198 - 2s - loss: 0.2251 - acc: 0.9110 - val_loss: 0.2255 - val_acc: 0.9105\n",
      "Epoch 121/200\n",
      "198/198 - 2s - loss: 0.2255 - acc: 0.9105 - val_loss: 0.2265 - val_acc: 0.9100\n",
      "Epoch 122/200\n",
      "198/198 - 2s - loss: 0.2251 - acc: 0.9106 - val_loss: 0.2247 - val_acc: 0.9108\n",
      "Epoch 123/200\n",
      "198/198 - 2s - loss: 0.2252 - acc: 0.9105 - val_loss: 0.2257 - val_acc: 0.9101\n",
      "Epoch 124/200\n",
      "198/198 - 2s - loss: 0.2248 - acc: 0.9109 - val_loss: 0.2240 - val_acc: 0.9108\n",
      "Epoch 125/200\n",
      "198/198 - 2s - loss: 0.2255 - acc: 0.9105 - val_loss: 0.2279 - val_acc: 0.9090\n",
      "Epoch 126/200\n",
      "198/198 - 2s - loss: 0.2246 - acc: 0.9108 - val_loss: 0.2242 - val_acc: 0.9109\n",
      "Epoch 127/200\n",
      "198/198 - 2s - loss: 0.2238 - acc: 0.9114 - val_loss: 0.2242 - val_acc: 0.9110\n",
      "Epoch 128/200\n",
      "198/198 - 2s - loss: 0.2245 - acc: 0.9114 - val_loss: 0.2246 - val_acc: 0.9111\n",
      "Epoch 129/200\n",
      "198/198 - 2s - loss: 0.2240 - acc: 0.9111 - val_loss: 0.2237 - val_acc: 0.9113\n",
      "Epoch 130/200\n",
      "198/198 - 2s - loss: 0.2242 - acc: 0.9112 - val_loss: 0.2304 - val_acc: 0.9085\n",
      "Epoch 131/200\n",
      "198/198 - 2s - loss: 0.2239 - acc: 0.9114 - val_loss: 0.2291 - val_acc: 0.9091\n",
      "Epoch 132/200\n",
      "198/198 - 2s - loss: 0.2239 - acc: 0.9111 - val_loss: 0.2251 - val_acc: 0.9107\n",
      "Epoch 133/200\n",
      "198/198 - 2s - loss: 0.2245 - acc: 0.9110 - val_loss: 0.2295 - val_acc: 0.9084\n",
      "Epoch 134/200\n",
      "198/198 - 2s - loss: 0.2240 - acc: 0.9111 - val_loss: 0.2336 - val_acc: 0.9070\n",
      "\n",
      "Epoch 00134: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 135/200\n",
      "198/198 - 2s - loss: 0.2232 - acc: 0.9117 - val_loss: 0.2236 - val_acc: 0.9115\n",
      "Epoch 136/200\n",
      "198/198 - 2s - loss: 0.2240 - acc: 0.9113 - val_loss: 0.2242 - val_acc: 0.9107\n",
      "Epoch 137/200\n",
      "198/198 - 2s - loss: 0.2228 - acc: 0.9117 - val_loss: 0.2287 - val_acc: 0.9090\n",
      "Epoch 138/200\n",
      "198/198 - 2s - loss: 0.2233 - acc: 0.9116 - val_loss: 0.2226 - val_acc: 0.9120\n",
      "Epoch 139/200\n",
      "198/198 - 2s - loss: 0.2232 - acc: 0.9114 - val_loss: 0.2228 - val_acc: 0.9116\n",
      "Epoch 140/200\n",
      "198/198 - 2s - loss: 0.2229 - acc: 0.9120 - val_loss: 0.2231 - val_acc: 0.9112\n",
      "Epoch 141/200\n",
      "198/198 - 2s - loss: 0.2231 - acc: 0.9118 - val_loss: 0.2259 - val_acc: 0.9103\n",
      "Epoch 142/200\n",
      "198/198 - 2s - loss: 0.2227 - acc: 0.9116 - val_loss: 0.2224 - val_acc: 0.9119\n",
      "Epoch 143/200\n",
      "198/198 - 2s - loss: 0.2226 - acc: 0.9120 - val_loss: 0.2228 - val_acc: 0.9117\n",
      "Epoch 144/200\n",
      "198/198 - 2s - loss: 0.2232 - acc: 0.9118 - val_loss: 0.2223 - val_acc: 0.9121\n",
      "Epoch 145/200\n",
      "198/198 - 2s - loss: 0.2230 - acc: 0.9120 - val_loss: 0.2230 - val_acc: 0.9113\n",
      "Epoch 146/200\n",
      "198/198 - 2s - loss: 0.2224 - acc: 0.9120 - val_loss: 0.2231 - val_acc: 0.9112\n",
      "Epoch 147/200\n",
      "198/198 - 2s - loss: 0.2224 - acc: 0.9122 - val_loss: 0.2218 - val_acc: 0.9127\n",
      "Epoch 148/200\n",
      "198/198 - 2s - loss: 0.2216 - acc: 0.9126 - val_loss: 0.2221 - val_acc: 0.9119\n",
      "Epoch 149/200\n",
      "198/198 - 2s - loss: 0.2217 - acc: 0.9125 - val_loss: 0.2230 - val_acc: 0.9112\n",
      "Epoch 150/200\n",
      "198/198 - 2s - loss: 0.2224 - acc: 0.9117 - val_loss: 0.2273 - val_acc: 0.9095\n",
      "Epoch 151/200\n",
      "198/198 - 2s - loss: 0.2223 - acc: 0.9117 - val_loss: 0.2219 - val_acc: 0.9122\n",
      "Epoch 152/200\n",
      "198/198 - 2s - loss: 0.2224 - acc: 0.9122 - val_loss: 0.2217 - val_acc: 0.9122\n",
      "Epoch 153/200\n",
      "198/198 - 2s - loss: 0.2217 - acc: 0.9125 - val_loss: 0.2235 - val_acc: 0.9116\n",
      "Epoch 154/200\n",
      "198/198 - 2s - loss: 0.2215 - acc: 0.9125 - val_loss: 0.2272 - val_acc: 0.9093\n",
      "Epoch 155/200\n",
      "198/198 - 2s - loss: 0.2211 - acc: 0.9129 - val_loss: 0.2217 - val_acc: 0.9120\n",
      "Epoch 156/200\n",
      "198/198 - 2s - loss: 0.2213 - acc: 0.9125 - val_loss: 0.2209 - val_acc: 0.9127\n",
      "Epoch 157/200\n",
      "198/198 - 2s - loss: 0.2210 - acc: 0.9128 - val_loss: 0.2234 - val_acc: 0.9111\n",
      "Epoch 158/200\n",
      "198/198 - 2s - loss: 0.2218 - acc: 0.9124 - val_loss: 0.2215 - val_acc: 0.9123\n",
      "Epoch 159/200\n",
      "198/198 - 2s - loss: 0.2214 - acc: 0.9129 - val_loss: 0.2215 - val_acc: 0.9122\n",
      "Epoch 160/200\n",
      "198/198 - 2s - loss: 0.2225 - acc: 0.9119 - val_loss: 0.2220 - val_acc: 0.9128\n",
      "Epoch 161/200\n",
      "198/198 - 2s - loss: 0.2210 - acc: 0.9128 - val_loss: 0.2204 - val_acc: 0.9132\n",
      "Epoch 162/200\n",
      "198/198 - 2s - loss: 0.2215 - acc: 0.9126 - val_loss: 0.2204 - val_acc: 0.9130\n",
      "Epoch 163/200\n",
      "198/198 - 2s - loss: 0.2206 - acc: 0.9134 - val_loss: 0.2207 - val_acc: 0.9129\n",
      "Epoch 164/200\n",
      "198/198 - 2s - loss: 0.2212 - acc: 0.9126 - val_loss: 0.2210 - val_acc: 0.9129\n",
      "Epoch 165/200\n",
      "198/198 - 2s - loss: 0.2208 - acc: 0.9133 - val_loss: 0.2204 - val_acc: 0.9133\n",
      "Epoch 166/200\n",
      "198/198 - 2s - loss: 0.2204 - acc: 0.9130 - val_loss: 0.2250 - val_acc: 0.9107\n",
      "Epoch 167/200\n",
      "198/198 - 2s - loss: 0.2200 - acc: 0.9137 - val_loss: 0.2232 - val_acc: 0.9113\n",
      "Epoch 168/200\n",
      "198/198 - 2s - loss: 0.2205 - acc: 0.9129 - val_loss: 0.2201 - val_acc: 0.9131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "198/198 - 2s - loss: 0.2209 - acc: 0.9130 - val_loss: 0.2201 - val_acc: 0.9129\n",
      "Epoch 170/200\n",
      "198/198 - 2s - loss: 0.2197 - acc: 0.9134 - val_loss: 0.2201 - val_acc: 0.9133\n",
      "Epoch 171/200\n",
      "198/198 - 2s - loss: 0.2199 - acc: 0.9134 - val_loss: 0.2204 - val_acc: 0.9125\n",
      "Epoch 172/200\n",
      "198/198 - 2s - loss: 0.2203 - acc: 0.9131 - val_loss: 0.2230 - val_acc: 0.9115\n",
      "Epoch 173/200\n",
      "198/198 - 2s - loss: 0.2203 - acc: 0.9134 - val_loss: 0.2251 - val_acc: 0.9104\n",
      "Epoch 174/200\n",
      "198/198 - 2s - loss: 0.2201 - acc: 0.9135 - val_loss: 0.2226 - val_acc: 0.9114\n",
      "Epoch 175/200\n",
      "198/198 - 2s - loss: 0.2197 - acc: 0.9135 - val_loss: 0.2203 - val_acc: 0.9124\n",
      "Epoch 176/200\n",
      "198/198 - 2s - loss: 0.2196 - acc: 0.9137 - val_loss: 0.2191 - val_acc: 0.9137\n",
      "Epoch 177/200\n",
      "198/198 - 2s - loss: 0.2196 - acc: 0.9136 - val_loss: 0.2188 - val_acc: 0.9138\n",
      "Epoch 178/200\n",
      "198/198 - 2s - loss: 0.2205 - acc: 0.9132 - val_loss: 0.2210 - val_acc: 0.9124\n",
      "Epoch 179/200\n",
      "198/198 - 2s - loss: 0.2189 - acc: 0.9136 - val_loss: 0.2204 - val_acc: 0.9130\n",
      "Epoch 180/200\n",
      "198/198 - 2s - loss: 0.2190 - acc: 0.9136 - val_loss: 0.2199 - val_acc: 0.9132\n",
      "Epoch 181/200\n",
      "198/198 - 2s - loss: 0.2191 - acc: 0.9135 - val_loss: 0.2213 - val_acc: 0.9121\n",
      "Epoch 182/200\n",
      "198/198 - 2s - loss: 0.2208 - acc: 0.9129 - val_loss: 0.2225 - val_acc: 0.9129\n",
      "Epoch 183/200\n",
      "198/198 - 2s - loss: 0.2190 - acc: 0.9140 - val_loss: 0.2196 - val_acc: 0.9130\n",
      "Epoch 184/200\n",
      "198/198 - 2s - loss: 0.2187 - acc: 0.9143 - val_loss: 0.2246 - val_acc: 0.9112\n",
      "Epoch 185/200\n",
      "198/198 - 2s - loss: 0.2185 - acc: 0.9142 - val_loss: 0.2194 - val_acc: 0.9132\n",
      "Epoch 186/200\n",
      "198/198 - 2s - loss: 0.2180 - acc: 0.9145 - val_loss: 0.2192 - val_acc: 0.9133\n",
      "Epoch 187/200\n",
      "198/198 - 2s - loss: 0.2181 - acc: 0.9145 - val_loss: 0.2214 - val_acc: 0.9119\n",
      "Epoch 00187: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 8.7211 - acc: 0.7450 - val_loss: 0.5455 - val_acc: 0.8449\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.5990 - acc: 0.8263 - val_loss: 0.4655 - val_acc: 0.8267\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.3594 - acc: 0.8647 - val_loss: 0.2646 - val_acc: 0.8983\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.3053 - acc: 0.8790 - val_loss: 0.3926 - val_acc: 0.8493\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.3129 - acc: 0.8762 - val_loss: 0.3731 - val_acc: 0.8599\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.2871 - acc: 0.8847 - val_loss: 0.3831 - val_acc: 0.8381\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.4209 - acc: 0.8625 - val_loss: 0.5349 - val_acc: 0.8316\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.3245 - acc: 0.8752 - val_loss: 0.2727 - val_acc: 0.8909\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.2524 - acc: 0.8992 - val_loss: 0.2390 - val_acc: 0.9048\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.2462 - acc: 0.9028 - val_loss: 0.2350 - val_acc: 0.9080\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.2458 - acc: 0.9023 - val_loss: 0.2495 - val_acc: 0.9042\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.2446 - acc: 0.9032 - val_loss: 0.2390 - val_acc: 0.9079\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.2472 - acc: 0.9014 - val_loss: 0.2602 - val_acc: 0.8963\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.2423 - acc: 0.9037 - val_loss: 0.2987 - val_acc: 0.8742\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.2408 - acc: 0.9044 - val_loss: 0.2308 - val_acc: 0.9082\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.2446 - acc: 0.9023 - val_loss: 0.2381 - val_acc: 0.9033\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.2416 - acc: 0.9042 - val_loss: 0.2885 - val_acc: 0.8796\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.2442 - acc: 0.9026 - val_loss: 0.2594 - val_acc: 0.8956\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.2412 - acc: 0.9041 - val_loss: 0.2317 - val_acc: 0.9095\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.2413 - acc: 0.9040 - val_loss: 0.2730 - val_acc: 0.8845\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.2340 - acc: 0.9073 - val_loss: 0.2380 - val_acc: 0.9034\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.2277 - acc: 0.9105 - val_loss: 0.2218 - val_acc: 0.9144\n",
      "Epoch 23/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 - 2s - loss: 0.2331 - acc: 0.9084 - val_loss: 0.2210 - val_acc: 0.9147\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.2337 - acc: 0.9080 - val_loss: 0.2228 - val_acc: 0.9139\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.2290 - acc: 0.9102 - val_loss: 0.2212 - val_acc: 0.9123\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.2317 - acc: 0.9089 - val_loss: 0.2167 - val_acc: 0.9158\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.2337 - acc: 0.9078 - val_loss: 0.2358 - val_acc: 0.9086\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.2312 - acc: 0.9090 - val_loss: 0.2164 - val_acc: 0.9174\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.2304 - acc: 0.9096 - val_loss: 0.2235 - val_acc: 0.9158\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.2287 - acc: 0.9104 - val_loss: 0.2299 - val_acc: 0.9089\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.2262 - acc: 0.9117 - val_loss: 0.2302 - val_acc: 0.9092\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.2286 - acc: 0.9097 - val_loss: 0.2240 - val_acc: 0.9135\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.2427 - acc: 0.9038 - val_loss: 0.2492 - val_acc: 0.8996\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.2208 - acc: 0.9139 - val_loss: 0.2182 - val_acc: 0.9141\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.2216 - acc: 0.9131 - val_loss: 0.2420 - val_acc: 0.9014\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.2203 - acc: 0.9140 - val_loss: 0.2710 - val_acc: 0.8888\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.2287 - acc: 0.9096 - val_loss: 0.2137 - val_acc: 0.9171\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.2180 - acc: 0.9152 - val_loss: 0.2131 - val_acc: 0.9176\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.2195 - acc: 0.9147 - val_loss: 0.2248 - val_acc: 0.9127\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.2183 - acc: 0.9151 - val_loss: 0.2121 - val_acc: 0.9177\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.2179 - acc: 0.9151 - val_loss: 0.2540 - val_acc: 0.8975\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.2182 - acc: 0.9151 - val_loss: 0.2058 - val_acc: 0.9214\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.2234 - acc: 0.9126 - val_loss: 0.2136 - val_acc: 0.9185\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.2191 - acc: 0.9152 - val_loss: 0.2128 - val_acc: 0.9178\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.2228 - acc: 0.9126 - val_loss: 0.2094 - val_acc: 0.9197\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.2210 - acc: 0.9139 - val_loss: 0.2114 - val_acc: 0.9184\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.2200 - acc: 0.9141 - val_loss: 0.2132 - val_acc: 0.9176\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.2112 - acc: 0.9186 - val_loss: 0.2305 - val_acc: 0.9073\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.2131 - acc: 0.9174 - val_loss: 0.2098 - val_acc: 0.9185\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.2104 - acc: 0.9190 - val_loss: 0.2143 - val_acc: 0.9183\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.2129 - acc: 0.9175 - val_loss: 0.2122 - val_acc: 0.9175\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.2129 - acc: 0.9176 - val_loss: 0.2045 - val_acc: 0.9211\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.2120 - acc: 0.9180 - val_loss: 0.2066 - val_acc: 0.9215\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.2103 - acc: 0.9188 - val_loss: 0.2031 - val_acc: 0.9217\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.2100 - acc: 0.9189 - val_loss: 0.2020 - val_acc: 0.9226\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.2096 - acc: 0.9195 - val_loss: 0.2297 - val_acc: 0.9125\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.2153 - acc: 0.9169 - val_loss: 0.2021 - val_acc: 0.9231\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.2089 - acc: 0.9200 - val_loss: 0.2040 - val_acc: 0.9229\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.2100 - acc: 0.9191 - val_loss: 0.2301 - val_acc: 0.9091\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.2096 - acc: 0.9187 - val_loss: 0.2043 - val_acc: 0.9213\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.2067 - acc: 0.9204 - val_loss: 0.2194 - val_acc: 0.9131\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.2069 - acc: 0.9201 - val_loss: 0.2045 - val_acc: 0.9206\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.2037 - acc: 0.9215 - val_loss: 0.2007 - val_acc: 0.9239\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.2019 - acc: 0.9229 - val_loss: 0.1989 - val_acc: 0.9255\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.2034 - acc: 0.9221 - val_loss: 0.1985 - val_acc: 0.9245\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.2015 - acc: 0.9229 - val_loss: 0.2119 - val_acc: 0.9198\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.2035 - acc: 0.9220 - val_loss: 0.2029 - val_acc: 0.9222\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.2027 - acc: 0.9222 - val_loss: 0.2057 - val_acc: 0.9204\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.2076 - acc: 0.9199 - val_loss: 0.1990 - val_acc: 0.9245\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.2036 - acc: 0.9218 - val_loss: 0.1959 - val_acc: 0.9258\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.2011 - acc: 0.9231 - val_loss: 0.1963 - val_acc: 0.9253\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.2030 - acc: 0.9220 - val_loss: 0.1999 - val_acc: 0.9238\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.2041 - acc: 0.9215 - val_loss: 0.2021 - val_acc: 0.9224\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.2011 - acc: 0.9231 - val_loss: 0.1959 - val_acc: 0.9253\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.2045 - acc: 0.9214 - val_loss: 0.2011 - val_acc: 0.9234\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.1991 - acc: 0.9236 - val_loss: 0.2052 - val_acc: 0.9220\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.1980 - acc: 0.9244 - val_loss: 0.1971 - val_acc: 0.9247\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.1967 - acc: 0.9248 - val_loss: 0.2032 - val_acc: 0.9214\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.1976 - acc: 0.9245 - val_loss: 0.2272 - val_acc: 0.9121\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.1980 - acc: 0.9245 - val_loss: 0.1964 - val_acc: 0.9245\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 00080: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 11.9319 - acc: 0.7692 - val_loss: 0.4988 - val_acc: 0.8772\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.6330 - acc: 0.8411 - val_loss: 0.4869 - val_acc: 0.8769\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.4539 - acc: 0.8593 - val_loss: 0.2722 - val_acc: 0.8903\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2916 - acc: 0.8877 - val_loss: 0.4444 - val_acc: 0.8430\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.2796 - acc: 0.8947 - val_loss: 0.3473 - val_acc: 0.8553\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.2702 - acc: 0.8954 - val_loss: 0.2412 - val_acc: 0.8995\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.2333 - acc: 0.9095 - val_loss: 0.2503 - val_acc: 0.9046\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.2512 - acc: 0.9035 - val_loss: 0.2357 - val_acc: 0.9037\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.2332 - acc: 0.9091 - val_loss: 0.2107 - val_acc: 0.9228\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.2275 - acc: 0.9126 - val_loss: 0.2019 - val_acc: 0.9244\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.2309 - acc: 0.9113 - val_loss: 0.2541 - val_acc: 0.8998\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.2236 - acc: 0.9141 - val_loss: 0.2387 - val_acc: 0.9141\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.2227 - acc: 0.9145 - val_loss: 0.2159 - val_acc: 0.9192\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.2221 - acc: 0.9147 - val_loss: 0.2036 - val_acc: 0.9226\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.2302 - acc: 0.9112 - val_loss: 0.2049 - val_acc: 0.9228\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.2074 - acc: 0.9207 - val_loss: 0.1990 - val_acc: 0.9244\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.2121 - acc: 0.9187 - val_loss: 0.1994 - val_acc: 0.9263\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.2095 - acc: 0.9199 - val_loss: 0.2028 - val_acc: 0.9230\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.2107 - acc: 0.9199 - val_loss: 0.1995 - val_acc: 0.9260\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.2145 - acc: 0.9182 - val_loss: 0.1979 - val_acc: 0.9257\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.2083 - acc: 0.9209 - val_loss: 0.2137 - val_acc: 0.9196\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.2123 - acc: 0.9188 - val_loss: 0.2082 - val_acc: 0.9228\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.2068 - acc: 0.9210 - val_loss: 0.2001 - val_acc: 0.9252\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.2120 - acc: 0.9193 - val_loss: 0.1997 - val_acc: 0.9258\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.2116 - acc: 0.9191 - val_loss: 0.2000 - val_acc: 0.9241\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1982 - acc: 0.9251 - val_loss: 0.1992 - val_acc: 0.9248\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1994 - acc: 0.9246 - val_loss: 0.2100 - val_acc: 0.9200\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.2012 - acc: 0.9230 - val_loss: 0.1964 - val_acc: 0.9248\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.2023 - acc: 0.9232 - val_loss: 0.1904 - val_acc: 0.9288\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1998 - acc: 0.9242 - val_loss: 0.1986 - val_acc: 0.9264\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.2050 - acc: 0.9225 - val_loss: 0.2146 - val_acc: 0.9162\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.2007 - acc: 0.9237 - val_loss: 0.1883 - val_acc: 0.9296\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.2003 - acc: 0.9239 - val_loss: 0.1918 - val_acc: 0.9279\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.2015 - acc: 0.9236 - val_loss: 0.2015 - val_acc: 0.9253\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.2017 - acc: 0.9232 - val_loss: 0.2137 - val_acc: 0.9181\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.2013 - acc: 0.9236 - val_loss: 0.2339 - val_acc: 0.9115\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.2028 - acc: 0.9228 - val_loss: 0.1906 - val_acc: 0.9290\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1944 - acc: 0.9261 - val_loss: 0.2292 - val_acc: 0.9107\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1946 - acc: 0.9262 - val_loss: 0.1926 - val_acc: 0.9288\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1921 - acc: 0.9271 - val_loss: 0.1889 - val_acc: 0.9294\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1948 - acc: 0.9258 - val_loss: 0.1955 - val_acc: 0.9258\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1958 - acc: 0.9255 - val_loss: 0.1854 - val_acc: 0.9304\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1932 - acc: 0.9264 - val_loss: 0.1895 - val_acc: 0.9304\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1934 - acc: 0.9270 - val_loss: 0.1967 - val_acc: 0.9263\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1947 - acc: 0.9258 - val_loss: 0.1840 - val_acc: 0.9312\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1933 - acc: 0.9264 - val_loss: 0.1865 - val_acc: 0.9300\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1966 - acc: 0.9254 - val_loss: 0.2083 - val_acc: 0.9205\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1971 - acc: 0.9251 - val_loss: 0.1850 - val_acc: 0.9305\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1952 - acc: 0.9263 - val_loss: 0.1864 - val_acc: 0.9301\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1933 - acc: 0.9267 - val_loss: 0.1882 - val_acc: 0.9296\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.1893 - acc: 0.9278 - val_loss: 0.1892 - val_acc: 0.9298\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.1884 - acc: 0.9284 - val_loss: 0.1912 - val_acc: 0.9270\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.1884 - acc: 0.9284 - val_loss: 0.1916 - val_acc: 0.9276\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.1900 - acc: 0.9276 - val_loss: 0.1855 - val_acc: 0.9297\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.1886 - acc: 0.9282 - val_loss: 0.1833 - val_acc: 0.9314\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.1876 - acc: 0.9291 - val_loss: 0.1840 - val_acc: 0.9315\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.1871 - acc: 0.9292 - val_loss: 0.1862 - val_acc: 0.9304\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.1888 - acc: 0.9281 - val_loss: 0.1847 - val_acc: 0.9316\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.1886 - acc: 0.9287 - val_loss: 0.1962 - val_acc: 0.9260\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.1889 - acc: 0.9290 - val_loss: 0.1844 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.1838 - acc: 0.9301 - val_loss: 0.1837 - val_acc: 0.9307\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.1830 - acc: 0.9302 - val_loss: 0.1933 - val_acc: 0.9289\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.1846 - acc: 0.9299 - val_loss: 0.1868 - val_acc: 0.9295\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.1839 - acc: 0.9303 - val_loss: 0.1832 - val_acc: 0.9312\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.1844 - acc: 0.9300 - val_loss: 0.1934 - val_acc: 0.9264\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.1845 - acc: 0.9296 - val_loss: 0.1788 - val_acc: 0.9337\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.1829 - acc: 0.9305 - val_loss: 0.1874 - val_acc: 0.9298\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.1870 - acc: 0.9288 - val_loss: 0.1824 - val_acc: 0.9321\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.1845 - acc: 0.9297 - val_loss: 0.1890 - val_acc: 0.9289\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.1847 - acc: 0.9298 - val_loss: 0.1814 - val_acc: 0.9314\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.1844 - acc: 0.9298 - val_loss: 0.1824 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.1801 - acc: 0.9316 - val_loss: 0.1790 - val_acc: 0.9330\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.1805 - acc: 0.9314 - val_loss: 0.1805 - val_acc: 0.9316\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.1807 - acc: 0.9312 - val_loss: 0.1821 - val_acc: 0.9320\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.1807 - acc: 0.9310 - val_loss: 0.1804 - val_acc: 0.9319\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.1811 - acc: 0.9311 - val_loss: 0.1836 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 00076: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 7.0872 - acc: 0.8164 - val_loss: 1.2641 - val_acc: 0.8201\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.7932 - acc: 0.8603 - val_loss: 0.7607 - val_acc: 0.8028\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.3833 - acc: 0.8862 - val_loss: 0.2585 - val_acc: 0.9136\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2511 - acc: 0.9077 - val_loss: 0.2030 - val_acc: 0.9261\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.2332 - acc: 0.9136 - val_loss: 0.2173 - val_acc: 0.9234\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.2573 - acc: 0.9072 - val_loss: 0.2237 - val_acc: 0.9143\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.2249 - acc: 0.9158 - val_loss: 0.2080 - val_acc: 0.9243\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.2157 - acc: 0.9191 - val_loss: 0.2178 - val_acc: 0.9146\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.2071 - acc: 0.9218 - val_loss: 0.2242 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1971 - acc: 0.9266 - val_loss: 0.1911 - val_acc: 0.9291\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.2050 - acc: 0.9235 - val_loss: 0.2038 - val_acc: 0.9245\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.2021 - acc: 0.9238 - val_loss: 0.1907 - val_acc: 0.9286\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1990 - acc: 0.9250 - val_loss: 0.2149 - val_acc: 0.9205\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.2032 - acc: 0.9235 - val_loss: 0.2060 - val_acc: 0.9220\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1990 - acc: 0.9253 - val_loss: 0.2037 - val_acc: 0.9225\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.2075 - acc: 0.9218 - val_loss: 0.2006 - val_acc: 0.9244\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1980 - acc: 0.9260 - val_loss: 0.2039 - val_acc: 0.9222\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1942 - acc: 0.9274 - val_loss: 0.1899 - val_acc: 0.9292\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1962 - acc: 0.9265 - val_loss: 0.1967 - val_acc: 0.9259\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1914 - acc: 0.9284 - val_loss: 0.1950 - val_acc: 0.9275\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1922 - acc: 0.9285 - val_loss: 0.1938 - val_acc: 0.9276\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1936 - acc: 0.9272 - val_loss: 0.1892 - val_acc: 0.9307\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1967 - acc: 0.9262 - val_loss: 0.1874 - val_acc: 0.9309\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1939 - acc: 0.9273 - val_loss: 0.1885 - val_acc: 0.9299\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1939 - acc: 0.9278 - val_loss: 0.1921 - val_acc: 0.9282\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1995 - acc: 0.9250 - val_loss: 0.1966 - val_acc: 0.9261\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.2025 - acc: 0.9240 - val_loss: 0.1925 - val_acc: 0.9285\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1943 - acc: 0.9274 - val_loss: 0.2042 - val_acc: 0.9210\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1908 - acc: 0.9284 - val_loss: 0.1960 - val_acc: 0.9257\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1883 - acc: 0.9300 - val_loss: 0.1870 - val_acc: 0.9302\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1919 - acc: 0.9283 - val_loss: 0.1868 - val_acc: 0.9304\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1896 - acc: 0.9290 - val_loss: 0.1851 - val_acc: 0.9306\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1924 - acc: 0.9284 - val_loss: 0.1914 - val_acc: 0.9291\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1909 - acc: 0.9285 - val_loss: 0.1865 - val_acc: 0.9311\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1921 - acc: 0.9279 - val_loss: 0.1890 - val_acc: 0.9289\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1950 - acc: 0.9271 - val_loss: 0.1907 - val_acc: 0.9310\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1923 - acc: 0.9279 - val_loss: 0.1873 - val_acc: 0.9305\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1857 - acc: 0.9310 - val_loss: 0.1877 - val_acc: 0.9300\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1861 - acc: 0.9305 - val_loss: 0.1863 - val_acc: 0.9307\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1853 - acc: 0.9308 - val_loss: 0.1815 - val_acc: 0.9327\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1841 - acc: 0.9312 - val_loss: 0.1853 - val_acc: 0.9323\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1861 - acc: 0.9307 - val_loss: 0.1905 - val_acc: 0.9290\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1849 - acc: 0.9311 - val_loss: 0.1831 - val_acc: 0.9328\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1864 - acc: 0.9306 - val_loss: 0.1857 - val_acc: 0.9309\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1875 - acc: 0.9302 - val_loss: 0.2014 - val_acc: 0.9242\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1819 - acc: 0.9317 - val_loss: 0.1873 - val_acc: 0.9299\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1824 - acc: 0.9320 - val_loss: 0.1986 - val_acc: 0.9267\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1830 - acc: 0.9313 - val_loss: 0.1920 - val_acc: 0.9282\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1848 - acc: 0.9303 - val_loss: 0.1839 - val_acc: 0.9312\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1823 - acc: 0.9320 - val_loss: 0.1881 - val_acc: 0.9302\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 00050: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 5.1988 - acc: 0.8435 - val_loss: 0.9921 - val_acc: 0.8422\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.6049 - acc: 0.8791 - val_loss: 0.5049 - val_acc: 0.8730\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.4553 - acc: 0.8910 - val_loss: 0.2277 - val_acc: 0.9182\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2307 - acc: 0.9167 - val_loss: 0.2145 - val_acc: 0.9249\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.2249 - acc: 0.9183 - val_loss: 0.1980 - val_acc: 0.9267\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.2122 - acc: 0.9220 - val_loss: 0.2106 - val_acc: 0.9233\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1951 - acc: 0.9286 - val_loss: 0.2263 - val_acc: 0.9196\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.2007 - acc: 0.9260 - val_loss: 0.1845 - val_acc: 0.9327\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1960 - acc: 0.9280 - val_loss: 0.2081 - val_acc: 0.9238\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1888 - acc: 0.9309 - val_loss: 0.1951 - val_acc: 0.9283\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1871 - acc: 0.9312 - val_loss: 0.2199 - val_acc: 0.9181\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1945 - acc: 0.9290 - val_loss: 0.1909 - val_acc: 0.9292\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1921 - acc: 0.9294 - val_loss: 0.2011 - val_acc: 0.9255\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1803 - acc: 0.9338 - val_loss: 0.1811 - val_acc: 0.9328\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1825 - acc: 0.9333 - val_loss: 0.3090 - val_acc: 0.8843\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1831 - acc: 0.9325 - val_loss: 0.1797 - val_acc: 0.9335\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1802 - acc: 0.9339 - val_loss: 0.1803 - val_acc: 0.9328\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1828 - acc: 0.9326 - val_loss: 0.1894 - val_acc: 0.9297\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1818 - acc: 0.9332 - val_loss: 0.1908 - val_acc: 0.9296\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1794 - acc: 0.9341 - val_loss: 0.1832 - val_acc: 0.9327\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1830 - acc: 0.9332 - val_loss: 0.2055 - val_acc: 0.9247\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1775 - acc: 0.9346 - val_loss: 0.1774 - val_acc: 0.9342\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1778 - acc: 0.9345 - val_loss: 0.1786 - val_acc: 0.9340\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1778 - acc: 0.9344 - val_loss: 0.1805 - val_acc: 0.9330\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1792 - acc: 0.9339 - val_loss: 0.1893 - val_acc: 0.9311\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1795 - acc: 0.9340 - val_loss: 0.1834 - val_acc: 0.9326\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1782 - acc: 0.9346 - val_loss: 0.1773 - val_acc: 0.9343\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1779 - acc: 0.9343 - val_loss: 0.1823 - val_acc: 0.9331\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1817 - acc: 0.9330 - val_loss: 0.2100 - val_acc: 0.9221\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1805 - acc: 0.9336 - val_loss: 0.1790 - val_acc: 0.9340\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1810 - acc: 0.9332 - val_loss: 0.1792 - val_acc: 0.9341\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1812 - acc: 0.9335 - val_loss: 0.1877 - val_acc: 0.9307\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1775 - acc: 0.9343 - val_loss: 0.1937 - val_acc: 0.9283\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1758 - acc: 0.9351 - val_loss: 0.1969 - val_acc: 0.9262\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1785 - acc: 0.9338 - val_loss: 0.1779 - val_acc: 0.9339\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1780 - acc: 0.9340 - val_loss: 0.1819 - val_acc: 0.9320\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1768 - acc: 0.9347 - val_loss: 0.1797 - val_acc: 0.9338\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 00037: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 12.6517 - acc: 0.8349 - val_loss: 0.6903 - val_acc: 0.9225\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.6660 - acc: 0.8908 - val_loss: 1.7129 - val_acc: 0.6890\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.6022 - acc: 0.8891 - val_loss: 0.3011 - val_acc: 0.8939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2628 - acc: 0.9145 - val_loss: 0.1873 - val_acc: 0.9332\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.2295 - acc: 0.9200 - val_loss: 0.1782 - val_acc: 0.9352\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1944 - acc: 0.9297 - val_loss: 0.1803 - val_acc: 0.9336\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.2107 - acc: 0.9248 - val_loss: 0.1917 - val_acc: 0.9313\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.2015 - acc: 0.9272 - val_loss: 0.2238 - val_acc: 0.9184\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1899 - acc: 0.9314 - val_loss: 0.1858 - val_acc: 0.9337\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.2011 - acc: 0.9281 - val_loss: 0.1739 - val_acc: 0.9356\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1852 - acc: 0.9323 - val_loss: 0.1791 - val_acc: 0.9339\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1826 - acc: 0.9332 - val_loss: 0.2069 - val_acc: 0.9251\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1815 - acc: 0.9338 - val_loss: 0.2213 - val_acc: 0.9176\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1844 - acc: 0.9327 - val_loss: 0.1751 - val_acc: 0.9358\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1801 - acc: 0.9344 - val_loss: 0.1710 - val_acc: 0.9377\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1755 - acc: 0.9361 - val_loss: 0.1750 - val_acc: 0.9348\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1769 - acc: 0.9354 - val_loss: 0.1870 - val_acc: 0.9299\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1812 - acc: 0.9337 - val_loss: 0.1862 - val_acc: 0.9334\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1793 - acc: 0.9343 - val_loss: 0.1756 - val_acc: 0.9352\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1779 - acc: 0.9347 - val_loss: 0.2836 - val_acc: 0.8994\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1742 - acc: 0.9365 - val_loss: 0.1673 - val_acc: 0.9389\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1725 - acc: 0.9369 - val_loss: 0.1717 - val_acc: 0.9361\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1729 - acc: 0.9369 - val_loss: 0.1688 - val_acc: 0.9375\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1727 - acc: 0.9367 - val_loss: 0.1710 - val_acc: 0.9380\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1762 - acc: 0.9353 - val_loss: 0.1683 - val_acc: 0.9382\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1733 - acc: 0.9369 - val_loss: 0.1683 - val_acc: 0.9381\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1688 - acc: 0.9383 - val_loss: 0.1676 - val_acc: 0.9389\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1708 - acc: 0.9372 - val_loss: 0.1686 - val_acc: 0.9384\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1711 - acc: 0.9377 - val_loss: 0.1701 - val_acc: 0.9383\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1723 - acc: 0.9371 - val_loss: 0.1668 - val_acc: 0.9387\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1732 - acc: 0.9364 - val_loss: 0.1692 - val_acc: 0.9386\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1726 - acc: 0.9368 - val_loss: 0.1707 - val_acc: 0.9375\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1716 - acc: 0.9376 - val_loss: 0.1695 - val_acc: 0.9387\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1716 - acc: 0.9375 - val_loss: 0.1693 - val_acc: 0.9375\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1731 - acc: 0.9364 - val_loss: 0.1670 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1681 - acc: 0.9387 - val_loss: 0.1676 - val_acc: 0.9388\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1692 - acc: 0.9379 - val_loss: 0.1682 - val_acc: 0.9381\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1713 - acc: 0.9369 - val_loss: 0.1837 - val_acc: 0.9338\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1694 - acc: 0.9376 - val_loss: 0.1763 - val_acc: 0.9343\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1699 - acc: 0.9381 - val_loss: 0.1679 - val_acc: 0.9381\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 00040: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 8.5396 - acc: 0.8563 - val_loss: 0.8996 - val_acc: 0.8192\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.5959 - acc: 0.8922 - val_loss: 0.2927 - val_acc: 0.9136\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.3275 - acc: 0.9049 - val_loss: 0.1832 - val_acc: 0.9351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2422 - acc: 0.9165 - val_loss: 0.2582 - val_acc: 0.9228\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.2300 - acc: 0.9212 - val_loss: 0.2059 - val_acc: 0.9256\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1894 - acc: 0.9323 - val_loss: 0.1735 - val_acc: 0.9359\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1792 - acc: 0.9351 - val_loss: 0.1687 - val_acc: 0.9378\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1805 - acc: 0.9344 - val_loss: 0.1764 - val_acc: 0.9376\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1963 - acc: 0.9289 - val_loss: 0.1706 - val_acc: 0.9371\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1780 - acc: 0.9355 - val_loss: 0.1662 - val_acc: 0.9398\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1842 - acc: 0.9338 - val_loss: 0.1778 - val_acc: 0.9369\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1751 - acc: 0.9369 - val_loss: 0.1674 - val_acc: 0.9384\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1784 - acc: 0.9354 - val_loss: 0.1692 - val_acc: 0.9381\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1868 - acc: 0.9323 - val_loss: 0.1856 - val_acc: 0.9327\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1713 - acc: 0.9379 - val_loss: 0.1635 - val_acc: 0.9399\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1701 - acc: 0.9382 - val_loss: 0.1798 - val_acc: 0.9354\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1696 - acc: 0.9383 - val_loss: 0.1859 - val_acc: 0.9318\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1791 - acc: 0.9351 - val_loss: 0.1659 - val_acc: 0.9394\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1734 - acc: 0.9372 - val_loss: 0.1672 - val_acc: 0.9386\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1695 - acc: 0.9385 - val_loss: 0.1705 - val_acc: 0.9372\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1622 - acc: 0.9411 - val_loss: 0.1594 - val_acc: 0.9415\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1605 - acc: 0.9417 - val_loss: 0.1682 - val_acc: 0.9384\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1623 - acc: 0.9410 - val_loss: 0.1607 - val_acc: 0.9413\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1638 - acc: 0.9404 - val_loss: 0.1630 - val_acc: 0.9417\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1639 - acc: 0.9406 - val_loss: 0.1587 - val_acc: 0.9419\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1618 - acc: 0.9411 - val_loss: 0.1614 - val_acc: 0.9423\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1649 - acc: 0.9400 - val_loss: 0.1691 - val_acc: 0.9390\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.3947 - acc: 0.9034 - val_loss: 0.1753 - val_acc: 0.9368\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1733 - acc: 0.9366 - val_loss: 0.1647 - val_acc: 0.9404\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1682 - acc: 0.9385 - val_loss: 0.1862 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1615 - acc: 0.9411 - val_loss: 0.1595 - val_acc: 0.9418\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1602 - acc: 0.9418 - val_loss: 0.1593 - val_acc: 0.9418\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1620 - acc: 0.9406 - val_loss: 0.1720 - val_acc: 0.9361\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1610 - acc: 0.9413 - val_loss: 0.1637 - val_acc: 0.9406\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1605 - acc: 0.9413 - val_loss: 0.1587 - val_acc: 0.9419\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 00035: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 8.4108 - acc: 0.8472 - val_loss: 2.0966 - val_acc: 0.7410\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.6880 - acc: 0.8853 - val_loss: 0.2695 - val_acc: 0.9125\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.2495 - acc: 0.9177 - val_loss: 0.1933 - val_acc: 0.9333\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.1949 - acc: 0.9306 - val_loss: 0.1919 - val_acc: 0.9328\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1930 - acc: 0.9316 - val_loss: 0.2677 - val_acc: 0.8993\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1969 - acc: 0.9310 - val_loss: 0.1823 - val_acc: 0.9317\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1808 - acc: 0.9353 - val_loss: 0.1624 - val_acc: 0.9406\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1713 - acc: 0.9385 - val_loss: 0.2083 - val_acc: 0.9244\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1741 - acc: 0.9373 - val_loss: 0.1591 - val_acc: 0.9424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1732 - acc: 0.9373 - val_loss: 0.1823 - val_acc: 0.9327\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1656 - acc: 0.9398 - val_loss: 0.1861 - val_acc: 0.9302\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1653 - acc: 0.9403 - val_loss: 0.1634 - val_acc: 0.9403\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1653 - acc: 0.9404 - val_loss: 0.1597 - val_acc: 0.9431\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1610 - acc: 0.9417 - val_loss: 0.1529 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1704 - acc: 0.9384 - val_loss: 0.1603 - val_acc: 0.9419\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1639 - acc: 0.9408 - val_loss: 0.1704 - val_acc: 0.9375\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1642 - acc: 0.9409 - val_loss: 0.2004 - val_acc: 0.9263\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1648 - acc: 0.9404 - val_loss: 0.1601 - val_acc: 0.9411\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 2.6183 - acc: 0.8701 - val_loss: 0.1833 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1753 - acc: 0.9354 - val_loss: 0.1695 - val_acc: 0.9372\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1660 - acc: 0.9393 - val_loss: 0.1630 - val_acc: 0.9400\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1626 - acc: 0.9410 - val_loss: 0.1597 - val_acc: 0.9410\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1597 - acc: 0.9421 - val_loss: 0.1573 - val_acc: 0.9428\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1588 - acc: 0.9422 - val_loss: 0.1627 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 00024: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 10.8570 - acc: 0.8413 - val_loss: 0.6007 - val_acc: 0.9274\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.8534 - acc: 0.8869 - val_loss: 0.5155 - val_acc: 0.9263\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.5402 - acc: 0.8986 - val_loss: 0.3499 - val_acc: 0.9201\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2115 - acc: 0.9296 - val_loss: 0.2594 - val_acc: 0.8952\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1907 - acc: 0.9318 - val_loss: 0.2897 - val_acc: 0.8896\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1762 - acc: 0.9363 - val_loss: 0.1776 - val_acc: 0.9317\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1747 - acc: 0.9377 - val_loss: 0.1701 - val_acc: 0.9387\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1760 - acc: 0.9370 - val_loss: 0.1769 - val_acc: 0.9380\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1761 - acc: 0.9370 - val_loss: 0.1688 - val_acc: 0.9389\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1662 - acc: 0.9400 - val_loss: 0.1660 - val_acc: 0.9403\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1632 - acc: 0.9409 - val_loss: 0.2104 - val_acc: 0.9226\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1624 - acc: 0.9412 - val_loss: 0.1555 - val_acc: 0.9441\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1596 - acc: 0.9422 - val_loss: 0.1546 - val_acc: 0.9448\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1650 - acc: 0.9409 - val_loss: 0.1528 - val_acc: 0.9457\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1570 - acc: 0.9435 - val_loss: 0.1570 - val_acc: 0.9459\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1607 - acc: 0.9417 - val_loss: 0.2096 - val_acc: 0.9191\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1597 - acc: 0.9424 - val_loss: 0.1579 - val_acc: 0.9437\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1601 - acc: 0.9421 - val_loss: 0.1719 - val_acc: 0.9389\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1548 - acc: 0.9439 - val_loss: 0.2742 - val_acc: 0.9061\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1518 - acc: 0.9451 - val_loss: 0.1500 - val_acc: 0.9459\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1484 - acc: 0.9465 - val_loss: 0.1484 - val_acc: 0.9467\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1508 - acc: 0.9456 - val_loss: 0.1490 - val_acc: 0.9463\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1512 - acc: 0.9450 - val_loss: 0.1511 - val_acc: 0.9467\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1474 - acc: 0.9466 - val_loss: 0.1462 - val_acc: 0.9478\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1485 - acc: 0.9464 - val_loss: 0.1625 - val_acc: 0.9422\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1492 - acc: 0.9462 - val_loss: 0.1618 - val_acc: 0.9410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1527 - acc: 0.9442 - val_loss: 0.1490 - val_acc: 0.9471\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1536 - acc: 0.9445 - val_loss: 0.1746 - val_acc: 0.9367\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1511 - acc: 0.9454 - val_loss: 0.1501 - val_acc: 0.9462\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1459 - acc: 0.9472 - val_loss: 0.1440 - val_acc: 0.9484\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1447 - acc: 0.9476 - val_loss: 0.1473 - val_acc: 0.9477\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1453 - acc: 0.9475 - val_loss: 0.1728 - val_acc: 0.9374\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1454 - acc: 0.9475 - val_loss: 0.1493 - val_acc: 0.9474\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1446 - acc: 0.9477 - val_loss: 0.1444 - val_acc: 0.9482\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1488 - acc: 0.9460 - val_loss: 0.1457 - val_acc: 0.9480\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1421 - acc: 0.9488 - val_loss: 0.1444 - val_acc: 0.9482\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1429 - acc: 0.9483 - val_loss: 0.1455 - val_acc: 0.9482\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1425 - acc: 0.9486 - val_loss: 0.1428 - val_acc: 0.9487\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1433 - acc: 0.9484 - val_loss: 0.1489 - val_acc: 0.9468\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1429 - acc: 0.9482 - val_loss: 0.1434 - val_acc: 0.9485\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1433 - acc: 0.9482 - val_loss: 0.1421 - val_acc: 0.9483\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1425 - acc: 0.9485 - val_loss: 0.1473 - val_acc: 0.9482\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1448 - acc: 0.9476 - val_loss: 0.1665 - val_acc: 0.9381\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1422 - acc: 0.9485 - val_loss: 0.1476 - val_acc: 0.9475\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1421 - acc: 0.9486 - val_loss: 0.1470 - val_acc: 0.9471\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1457 - acc: 0.9470 - val_loss: 0.1443 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1404 - acc: 0.9492 - val_loss: 0.1520 - val_acc: 0.9459\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1414 - acc: 0.9487 - val_loss: 0.1487 - val_acc: 0.9470\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1408 - acc: 0.9489 - val_loss: 0.1465 - val_acc: 0.9480\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1408 - acc: 0.9490 - val_loss: 0.1419 - val_acc: 0.9497\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.1403 - acc: 0.9493 - val_loss: 0.1526 - val_acc: 0.9458\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.1399 - acc: 0.9497 - val_loss: 0.1424 - val_acc: 0.9489\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.1402 - acc: 0.9489 - val_loss: 0.1448 - val_acc: 0.9485\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.1414 - acc: 0.9490 - val_loss: 0.1405 - val_acc: 0.9498\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.1394 - acc: 0.9495 - val_loss: 0.1436 - val_acc: 0.9490\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.1413 - acc: 0.9488 - val_loss: 0.1454 - val_acc: 0.9484\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.1416 - acc: 0.9488 - val_loss: 0.1420 - val_acc: 0.9490\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.1390 - acc: 0.9497 - val_loss: 0.1403 - val_acc: 0.9496\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.1400 - acc: 0.9492 - val_loss: 0.1460 - val_acc: 0.9473\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.1404 - acc: 0.9495 - val_loss: 0.1532 - val_acc: 0.9437\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.1397 - acc: 0.9496 - val_loss: 0.1410 - val_acc: 0.9489\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.1397 - acc: 0.9499 - val_loss: 0.1409 - val_acc: 0.9491\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.1395 - acc: 0.9492 - val_loss: 0.1432 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.1372 - acc: 0.9505 - val_loss: 0.1405 - val_acc: 0.9496\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.1365 - acc: 0.9504 - val_loss: 0.1451 - val_acc: 0.9478\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.1364 - acc: 0.9505 - val_loss: 0.1408 - val_acc: 0.9492\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.1358 - acc: 0.9510 - val_loss: 0.1395 - val_acc: 0.9496\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.1365 - acc: 0.9509 - val_loss: 0.1405 - val_acc: 0.9500\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.1359 - acc: 0.9507 - val_loss: 0.1441 - val_acc: 0.9490\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.1357 - acc: 0.9507 - val_loss: 0.1387 - val_acc: 0.9504\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.1367 - acc: 0.9507 - val_loss: 0.1484 - val_acc: 0.9465\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.1360 - acc: 0.9507 - val_loss: 0.1413 - val_acc: 0.9491\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.1367 - acc: 0.9507 - val_loss: 0.1390 - val_acc: 0.9504\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.1361 - acc: 0.9509 - val_loss: 0.1465 - val_acc: 0.9473\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.1369 - acc: 0.9502 - val_loss: 0.1465 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.1335 - acc: 0.9517 - val_loss: 0.1379 - val_acc: 0.9504\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.1331 - acc: 0.9522 - val_loss: 0.1392 - val_acc: 0.9505\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.1335 - acc: 0.9514 - val_loss: 0.1375 - val_acc: 0.9503\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.1330 - acc: 0.9518 - val_loss: 0.1379 - val_acc: 0.9507\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.1324 - acc: 0.9521 - val_loss: 0.1380 - val_acc: 0.9506\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.1332 - acc: 0.9519 - val_loss: 0.1379 - val_acc: 0.9503\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.1327 - acc: 0.9519 - val_loss: 0.1383 - val_acc: 0.9504\n",
      "Epoch 83/200\n",
      "198/198 - 2s - loss: 0.1326 - acc: 0.9518 - val_loss: 0.1413 - val_acc: 0.9493\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 84/200\n",
      "198/198 - 2s - loss: 0.1315 - acc: 0.9521 - val_loss: 0.1381 - val_acc: 0.9503\n",
      "Epoch 85/200\n",
      "198/198 - 2s - loss: 0.1311 - acc: 0.9523 - val_loss: 0.1386 - val_acc: 0.9502\n",
      "Epoch 86/200\n",
      "198/198 - 2s - loss: 0.1314 - acc: 0.9523 - val_loss: 0.1380 - val_acc: 0.9507\n",
      "Epoch 87/200\n",
      "198/198 - 2s - loss: 0.1304 - acc: 0.9526 - val_loss: 0.1374 - val_acc: 0.9504\n",
      "Epoch 88/200\n",
      "198/198 - 2s - loss: 0.1314 - acc: 0.9523 - val_loss: 0.1368 - val_acc: 0.9510\n",
      "Epoch 89/200\n",
      "198/198 - 2s - loss: 0.1309 - acc: 0.9525 - val_loss: 0.1371 - val_acc: 0.9506\n",
      "Epoch 90/200\n",
      "198/198 - 2s - loss: 0.1302 - acc: 0.9528 - val_loss: 0.1366 - val_acc: 0.9511\n",
      "Epoch 91/200\n",
      "198/198 - 2s - loss: 0.1305 - acc: 0.9527 - val_loss: 0.1379 - val_acc: 0.9506\n",
      "Epoch 92/200\n",
      "198/198 - 2s - loss: 0.1310 - acc: 0.9525 - val_loss: 0.1379 - val_acc: 0.9508\n",
      "Epoch 93/200\n",
      "198/198 - 2s - loss: 0.1303 - acc: 0.9526 - val_loss: 0.1385 - val_acc: 0.9503\n",
      "Epoch 94/200\n",
      "198/198 - 2s - loss: 0.1304 - acc: 0.9525 - val_loss: 0.1365 - val_acc: 0.9509\n",
      "Epoch 95/200\n",
      "198/198 - 2s - loss: 0.1301 - acc: 0.9525 - val_loss: 0.1362 - val_acc: 0.9512\n",
      "Epoch 96/200\n",
      "198/198 - 2s - loss: 0.1301 - acc: 0.9528 - val_loss: 0.1372 - val_acc: 0.9508\n",
      "Epoch 97/200\n",
      "198/198 - 2s - loss: 0.1305 - acc: 0.9528 - val_loss: 0.1357 - val_acc: 0.9511\n",
      "Epoch 98/200\n",
      "198/198 - 2s - loss: 0.1296 - acc: 0.9529 - val_loss: 0.1393 - val_acc: 0.9504\n",
      "Epoch 99/200\n",
      "198/198 - 2s - loss: 0.1304 - acc: 0.9524 - val_loss: 0.1396 - val_acc: 0.9495\n",
      "Epoch 100/200\n",
      "198/198 - 2s - loss: 0.1298 - acc: 0.9527 - val_loss: 0.1382 - val_acc: 0.9499\n",
      "Epoch 101/200\n",
      "198/198 - 2s - loss: 0.1298 - acc: 0.9528 - val_loss: 0.1390 - val_acc: 0.9505\n",
      "Epoch 102/200\n",
      "198/198 - 2s - loss: 0.1294 - acc: 0.9523 - val_loss: 0.1369 - val_acc: 0.9511\n",
      "\n",
      "Epoch 00102: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 103/200\n",
      "198/198 - 2s - loss: 0.1285 - acc: 0.9532 - val_loss: 0.1362 - val_acc: 0.9508\n",
      "Epoch 104/200\n",
      "198/198 - 2s - loss: 0.1285 - acc: 0.9531 - val_loss: 0.1356 - val_acc: 0.9512\n",
      "Epoch 105/200\n",
      "198/198 - 2s - loss: 0.1285 - acc: 0.9530 - val_loss: 0.1354 - val_acc: 0.9511\n",
      "Epoch 106/200\n",
      "198/198 - 2s - loss: 0.1280 - acc: 0.9534 - val_loss: 0.1357 - val_acc: 0.9512\n",
      "Epoch 107/200\n",
      "198/198 - 2s - loss: 0.1279 - acc: 0.9535 - val_loss: 0.1401 - val_acc: 0.9499\n",
      "Epoch 108/200\n",
      "198/198 - 2s - loss: 0.1280 - acc: 0.9534 - val_loss: 0.1386 - val_acc: 0.9504\n",
      "Epoch 109/200\n",
      "198/198 - 2s - loss: 0.1283 - acc: 0.9532 - val_loss: 0.1364 - val_acc: 0.9507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/200\n",
      "198/198 - 2s - loss: 0.1282 - acc: 0.9534 - val_loss: 0.1360 - val_acc: 0.9511\n",
      "\n",
      "Epoch 00110: ReduceLROnPlateau reducing learning rate to 1.5848932274101303e-05.\n",
      "Epoch 111/200\n",
      "198/198 - 2s - loss: 0.1270 - acc: 0.9535 - val_loss: 0.1359 - val_acc: 0.9511\n",
      "Epoch 112/200\n",
      "198/198 - 2s - loss: 0.1268 - acc: 0.9537 - val_loss: 0.1355 - val_acc: 0.9509\n",
      "Epoch 113/200\n",
      "198/198 - 2s - loss: 0.1271 - acc: 0.9537 - val_loss: 0.1362 - val_acc: 0.9511\n",
      "Epoch 114/200\n",
      "198/198 - 2s - loss: 0.1270 - acc: 0.9535 - val_loss: 0.1359 - val_acc: 0.9513\n",
      "Epoch 115/200\n",
      "198/198 - 2s - loss: 0.1269 - acc: 0.9536 - val_loss: 0.1359 - val_acc: 0.9512\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 1.0000000608891671e-05.\n",
      "Epoch 00115: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 13.2152 - acc: 0.8466 - val_loss: 0.4875 - val_acc: 0.9261\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.5588 - acc: 0.9001 - val_loss: 0.1898 - val_acc: 0.9323\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.5035 - acc: 0.8976 - val_loss: 0.5204 - val_acc: 0.9190\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2461 - acc: 0.9251 - val_loss: 0.2307 - val_acc: 0.9282\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1796 - acc: 0.9368 - val_loss: 0.1531 - val_acc: 0.9437\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1672 - acc: 0.9409 - val_loss: 0.1541 - val_acc: 0.9466\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1582 - acc: 0.9439 - val_loss: 0.1458 - val_acc: 0.9483\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1593 - acc: 0.9434 - val_loss: 0.1497 - val_acc: 0.9469\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1648 - acc: 0.9412 - val_loss: 0.1541 - val_acc: 0.9461\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1549 - acc: 0.9446 - val_loss: 0.1502 - val_acc: 0.9467\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1661 - acc: 0.9413 - val_loss: 0.1488 - val_acc: 0.9459\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1603 - acc: 0.9430 - val_loss: 0.1616 - val_acc: 0.9414\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1502 - acc: 0.9464 - val_loss: 0.1448 - val_acc: 0.9487\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1504 - acc: 0.9461 - val_loss: 0.1546 - val_acc: 0.9456\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1492 - acc: 0.9468 - val_loss: 0.1582 - val_acc: 0.9423\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1503 - acc: 0.9462 - val_loss: 0.1585 - val_acc: 0.9446\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1480 - acc: 0.9473 - val_loss: 0.1415 - val_acc: 0.9496\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1500 - acc: 0.9463 - val_loss: 0.1435 - val_acc: 0.9490\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1493 - acc: 0.9470 - val_loss: 0.1435 - val_acc: 0.9491\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1535 - acc: 0.9450 - val_loss: 0.1682 - val_acc: 0.9405\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1485 - acc: 0.9468 - val_loss: 0.1419 - val_acc: 0.9497\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1489 - acc: 0.9465 - val_loss: 0.1716 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1421 - acc: 0.9491 - val_loss: 0.1411 - val_acc: 0.9496\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1439 - acc: 0.9485 - val_loss: 0.1396 - val_acc: 0.9503\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1456 - acc: 0.9475 - val_loss: 0.1420 - val_acc: 0.9494\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1427 - acc: 0.9490 - val_loss: 0.1428 - val_acc: 0.9493\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1459 - acc: 0.9478 - val_loss: 0.1412 - val_acc: 0.9494\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1471 - acc: 0.9471 - val_loss: 0.1410 - val_acc: 0.9502\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1464 - acc: 0.9474 - val_loss: 0.1389 - val_acc: 0.9505\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1455 - acc: 0.9477 - val_loss: 0.1494 - val_acc: 0.9453\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1479 - acc: 0.9467 - val_loss: 0.1638 - val_acc: 0.9422\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1491 - acc: 0.9461 - val_loss: 0.1706 - val_acc: 0.9415\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1485 - acc: 0.9472 - val_loss: 0.1441 - val_acc: 0.9484\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1466 - acc: 0.9473 - val_loss: 0.1433 - val_acc: 0.9492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1404 - acc: 0.9495 - val_loss: 0.1501 - val_acc: 0.9464\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1422 - acc: 0.9488 - val_loss: 0.1391 - val_acc: 0.9510\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1414 - acc: 0.9492 - val_loss: 0.1480 - val_acc: 0.9469\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1396 - acc: 0.9500 - val_loss: 0.1399 - val_acc: 0.9499\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1412 - acc: 0.9496 - val_loss: 0.1450 - val_acc: 0.9489\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 00039: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 6.9499 - acc: 0.8437 - val_loss: 1.7960 - val_acc: 0.8665\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.6707 - acc: 0.8912 - val_loss: 0.4332 - val_acc: 0.9280\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.2577 - acc: 0.9202 - val_loss: 0.2880 - val_acc: 0.9349\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2506 - acc: 0.9218 - val_loss: 0.1664 - val_acc: 0.9425\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1948 - acc: 0.9328 - val_loss: 0.1804 - val_acc: 0.9337\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1733 - acc: 0.9389 - val_loss: 0.1919 - val_acc: 0.9257\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1623 - acc: 0.9422 - val_loss: 0.1597 - val_acc: 0.9441\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1623 - acc: 0.9422 - val_loss: 0.1865 - val_acc: 0.9345\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1577 - acc: 0.9437 - val_loss: 0.1508 - val_acc: 0.9460\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1676 - acc: 0.9402 - val_loss: 0.1553 - val_acc: 0.9450\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1626 - acc: 0.9418 - val_loss: 0.1564 - val_acc: 0.9439\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1496 - acc: 0.9465 - val_loss: 0.1490 - val_acc: 0.9478\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1513 - acc: 0.9456 - val_loss: 0.1809 - val_acc: 0.9325\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1524 - acc: 0.9458 - val_loss: 0.1443 - val_acc: 0.9482\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1473 - acc: 0.9472 - val_loss: 0.1675 - val_acc: 0.9401\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1487 - acc: 0.9468 - val_loss: 0.1791 - val_acc: 0.9348\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1497 - acc: 0.9464 - val_loss: 0.2030 - val_acc: 0.9265\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 2.6875 - acc: 0.8793 - val_loss: 0.1618 - val_acc: 0.9423\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1535 - acc: 0.9448 - val_loss: 0.1642 - val_acc: 0.9419\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1454 - acc: 0.9482 - val_loss: 0.1484 - val_acc: 0.9475\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1434 - acc: 0.9493 - val_loss: 0.1464 - val_acc: 0.9481\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1415 - acc: 0.9496 - val_loss: 0.1451 - val_acc: 0.9483\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1419 - acc: 0.9497 - val_loss: 0.1454 - val_acc: 0.9477\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1411 - acc: 0.9499 - val_loss: 0.1445 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 00024: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 10.2263 - acc: 0.8488 - val_loss: 0.3167 - val_acc: 0.9252\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.6730 - acc: 0.8921 - val_loss: 0.2879 - val_acc: 0.9326\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.2602 - acc: 0.9217 - val_loss: 0.1741 - val_acc: 0.9375\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.1845 - acc: 0.9355 - val_loss: 0.1841 - val_acc: 0.9408\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1761 - acc: 0.9374 - val_loss: 0.2030 - val_acc: 0.9237\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1786 - acc: 0.9368 - val_loss: 0.1476 - val_acc: 0.9473\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1649 - acc: 0.9414 - val_loss: 0.2324 - val_acc: 0.9142\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1593 - acc: 0.9432 - val_loss: 0.1458 - val_acc: 0.9474\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1537 - acc: 0.9450 - val_loss: 0.1496 - val_acc: 0.9477\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1566 - acc: 0.9436 - val_loss: 0.1450 - val_acc: 0.9489\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1510 - acc: 0.9457 - val_loss: 0.1621 - val_acc: 0.9421\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1516 - acc: 0.9457 - val_loss: 0.1484 - val_acc: 0.9464\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1511 - acc: 0.9461 - val_loss: 0.1534 - val_acc: 0.9478\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1477 - acc: 0.9470 - val_loss: 0.2804 - val_acc: 0.9004\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1468 - acc: 0.9474 - val_loss: 0.1391 - val_acc: 0.9513\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1471 - acc: 0.9469 - val_loss: 0.1548 - val_acc: 0.9460\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1438 - acc: 0.9482 - val_loss: 0.1362 - val_acc: 0.9515\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1485 - acc: 0.9467 - val_loss: 0.1530 - val_acc: 0.9447\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1447 - acc: 0.9473 - val_loss: 0.1401 - val_acc: 0.9492\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1520 - acc: 0.9454 - val_loss: 0.1473 - val_acc: 0.9481\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1465 - acc: 0.9471 - val_loss: 0.1685 - val_acc: 0.9415\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1454 - acc: 0.9476 - val_loss: 0.1377 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1354 - acc: 0.9512 - val_loss: 0.1381 - val_acc: 0.9504\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1369 - acc: 0.9505 - val_loss: 0.1389 - val_acc: 0.9512\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1352 - acc: 0.9510 - val_loss: 0.1376 - val_acc: 0.9508\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1370 - acc: 0.9501 - val_loss: 0.1442 - val_acc: 0.9477\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1360 - acc: 0.9511 - val_loss: 0.1404 - val_acc: 0.9507\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 00027: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 7.9553 - acc: 0.8404 - val_loss: 0.3249 - val_acc: 0.9012\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.4603 - acc: 0.8941 - val_loss: 0.2903 - val_acc: 0.8876\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.2920 - acc: 0.9142 - val_loss: 0.1584 - val_acc: 0.9435\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2018 - acc: 0.9302 - val_loss: 0.3282 - val_acc: 0.9147\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1941 - acc: 0.9340 - val_loss: 0.1530 - val_acc: 0.9470\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1522 - acc: 0.9457 - val_loss: 0.1505 - val_acc: 0.9451\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1693 - acc: 0.9393 - val_loss: 0.1612 - val_acc: 0.9428\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1578 - acc: 0.9437 - val_loss: 0.1501 - val_acc: 0.9465\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1600 - acc: 0.9426 - val_loss: 0.1393 - val_acc: 0.9513\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1517 - acc: 0.9457 - val_loss: 0.1510 - val_acc: 0.9462\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1470 - acc: 0.9476 - val_loss: 0.1361 - val_acc: 0.9523\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1449 - acc: 0.9481 - val_loss: 0.1607 - val_acc: 0.9404\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1556 - acc: 0.9444 - val_loss: 0.2114 - val_acc: 0.9256\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1426 - acc: 0.9487 - val_loss: 0.1598 - val_acc: 0.9412\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1460 - acc: 0.9474 - val_loss: 0.1404 - val_acc: 0.9482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1430 - acc: 0.9488 - val_loss: 0.1388 - val_acc: 0.9503\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1351 - acc: 0.9510 - val_loss: 0.1327 - val_acc: 0.9527\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1383 - acc: 0.9499 - val_loss: 0.1298 - val_acc: 0.9539\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1353 - acc: 0.9514 - val_loss: 0.1274 - val_acc: 0.9544\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1360 - acc: 0.9511 - val_loss: 0.1483 - val_acc: 0.9477\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1375 - acc: 0.9501 - val_loss: 0.1272 - val_acc: 0.9541\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1341 - acc: 0.9520 - val_loss: 0.1265 - val_acc: 0.9550\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1352 - acc: 0.9513 - val_loss: 0.1257 - val_acc: 0.9545\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1361 - acc: 0.9507 - val_loss: 0.1277 - val_acc: 0.9543\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1399 - acc: 0.9490 - val_loss: 0.1386 - val_acc: 0.9502\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1370 - acc: 0.9503 - val_loss: 0.1511 - val_acc: 0.9443\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1383 - acc: 0.9502 - val_loss: 0.1295 - val_acc: 0.9533\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1380 - acc: 0.9505 - val_loss: 0.1270 - val_acc: 0.9543\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1293 - acc: 0.9533 - val_loss: 0.1281 - val_acc: 0.9542\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1299 - acc: 0.9528 - val_loss: 0.1238 - val_acc: 0.9552\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1275 - acc: 0.9539 - val_loss: 0.1238 - val_acc: 0.9555\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1315 - acc: 0.9523 - val_loss: 0.1265 - val_acc: 0.9546\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1301 - acc: 0.9530 - val_loss: 0.1423 - val_acc: 0.9494\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1303 - acc: 0.9527 - val_loss: 0.1433 - val_acc: 0.9495\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1298 - acc: 0.9531 - val_loss: 0.1262 - val_acc: 0.9545\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1258 - acc: 0.9544 - val_loss: 0.1240 - val_acc: 0.9557\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1261 - acc: 0.9541 - val_loss: 0.1221 - val_acc: 0.9558\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1263 - acc: 0.9540 - val_loss: 0.1225 - val_acc: 0.9563\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1261 - acc: 0.9540 - val_loss: 0.1229 - val_acc: 0.9554\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1266 - acc: 0.9540 - val_loss: 0.1262 - val_acc: 0.9540\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1252 - acc: 0.9542 - val_loss: 0.1247 - val_acc: 0.9558\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1270 - acc: 0.9540 - val_loss: 0.1213 - val_acc: 0.9563\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1259 - acc: 0.9540 - val_loss: 0.1351 - val_acc: 0.9504\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1282 - acc: 0.9536 - val_loss: 0.1248 - val_acc: 0.9548\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1277 - acc: 0.9534 - val_loss: 0.1223 - val_acc: 0.9560\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1252 - acc: 0.9546 - val_loss: 0.1259 - val_acc: 0.9548\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1249 - acc: 0.9547 - val_loss: 0.1290 - val_acc: 0.9541\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1230 - acc: 0.9551 - val_loss: 0.1198 - val_acc: 0.9566\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1216 - acc: 0.9557 - val_loss: 0.1221 - val_acc: 0.9560\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1230 - acc: 0.9550 - val_loss: 0.1210 - val_acc: 0.9571\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.1222 - acc: 0.9556 - val_loss: 0.1203 - val_acc: 0.9564\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.1217 - acc: 0.9553 - val_loss: 0.1228 - val_acc: 0.9552\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.1210 - acc: 0.9559 - val_loss: 0.1207 - val_acc: 0.9567\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.1193 - acc: 0.9564 - val_loss: 0.1255 - val_acc: 0.9540\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.1182 - acc: 0.9566 - val_loss: 0.1249 - val_acc: 0.9545\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.1188 - acc: 0.9565 - val_loss: 0.1205 - val_acc: 0.9566\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.1188 - acc: 0.9563 - val_loss: 0.1218 - val_acc: 0.9562\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.1193 - acc: 0.9565 - val_loss: 0.1193 - val_acc: 0.9573\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.1175 - acc: 0.9569 - val_loss: 0.1182 - val_acc: 0.9576\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.1174 - acc: 0.9569 - val_loss: 0.1210 - val_acc: 0.9564\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.1181 - acc: 0.9566 - val_loss: 0.1225 - val_acc: 0.9561\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.1178 - acc: 0.9568 - val_loss: 0.1217 - val_acc: 0.9563\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.1178 - acc: 0.9568 - val_loss: 0.1247 - val_acc: 0.9557\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.1173 - acc: 0.9569 - val_loss: 0.1202 - val_acc: 0.9569\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.1150 - acc: 0.9578 - val_loss: 0.1187 - val_acc: 0.9574\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.1152 - acc: 0.9573 - val_loss: 0.1249 - val_acc: 0.9545\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.1150 - acc: 0.9576 - val_loss: 0.1175 - val_acc: 0.9579\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.1148 - acc: 0.9576 - val_loss: 0.1208 - val_acc: 0.9565\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.1149 - acc: 0.9576 - val_loss: 0.1171 - val_acc: 0.9577\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.1133 - acc: 0.9581 - val_loss: 0.1220 - val_acc: 0.9563\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.1139 - acc: 0.9578 - val_loss: 0.1211 - val_acc: 0.9563\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.1147 - acc: 0.9576 - val_loss: 0.1162 - val_acc: 0.9584\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.1142 - acc: 0.9579 - val_loss: 0.1217 - val_acc: 0.9558\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.1136 - acc: 0.9581 - val_loss: 0.1319 - val_acc: 0.9520\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.1140 - acc: 0.9577 - val_loss: 0.1173 - val_acc: 0.9580\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.1132 - acc: 0.9583 - val_loss: 0.1170 - val_acc: 0.9583\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.1135 - acc: 0.9580 - val_loss: 0.1163 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.1109 - acc: 0.9588 - val_loss: 0.1178 - val_acc: 0.9577\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.1109 - acc: 0.9589 - val_loss: 0.1163 - val_acc: 0.9581\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.1104 - acc: 0.9589 - val_loss: 0.1157 - val_acc: 0.9584\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.1105 - acc: 0.9591 - val_loss: 0.1183 - val_acc: 0.9578\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.1102 - acc: 0.9590 - val_loss: 0.1159 - val_acc: 0.9584\n",
      "Epoch 83/200\n",
      "198/198 - 2s - loss: 0.1101 - acc: 0.9591 - val_loss: 0.1163 - val_acc: 0.9582\n",
      "Epoch 84/200\n",
      "198/198 - 2s - loss: 0.1097 - acc: 0.9594 - val_loss: 0.1168 - val_acc: 0.9579\n",
      "Epoch 85/200\n",
      "198/198 - 2s - loss: 0.1098 - acc: 0.9596 - val_loss: 0.1165 - val_acc: 0.9579\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 86/200\n",
      "198/198 - 2s - loss: 0.1084 - acc: 0.9596 - val_loss: 0.1173 - val_acc: 0.9576\n",
      "Epoch 87/200\n",
      "198/198 - 2s - loss: 0.1084 - acc: 0.9598 - val_loss: 0.1171 - val_acc: 0.9579\n",
      "Epoch 88/200\n",
      "198/198 - 2s - loss: 0.1082 - acc: 0.9600 - val_loss: 0.1174 - val_acc: 0.9574\n",
      "Epoch 89/200\n",
      "198/198 - 2s - loss: 0.1081 - acc: 0.9600 - val_loss: 0.1157 - val_acc: 0.9582\n",
      "Epoch 90/200\n",
      "198/198 - 2s - loss: 0.1081 - acc: 0.9598 - val_loss: 0.1160 - val_acc: 0.9585\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1.5848932274101303e-05.\n",
      "Epoch 91/200\n",
      "198/198 - 2s - loss: 0.1072 - acc: 0.9601 - val_loss: 0.1158 - val_acc: 0.9581\n",
      "Epoch 92/200\n",
      "198/198 - 2s - loss: 0.1069 - acc: 0.9603 - val_loss: 0.1158 - val_acc: 0.9583\n",
      "Epoch 93/200\n",
      "198/198 - 2s - loss: 0.1067 - acc: 0.9606 - val_loss: 0.1159 - val_acc: 0.9582\n",
      "Epoch 94/200\n",
      "198/198 - 2s - loss: 0.1067 - acc: 0.9603 - val_loss: 0.1152 - val_acc: 0.9587\n",
      "Epoch 95/200\n",
      "198/198 - 2s - loss: 0.1068 - acc: 0.9603 - val_loss: 0.1156 - val_acc: 0.9584\n",
      "Epoch 96/200\n",
      "198/198 - 2s - loss: 0.1067 - acc: 0.9603 - val_loss: 0.1169 - val_acc: 0.9581\n",
      "Epoch 97/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 - 2s - loss: 0.1065 - acc: 0.9602 - val_loss: 0.1157 - val_acc: 0.9585\n",
      "Epoch 98/200\n",
      "198/198 - 2s - loss: 0.1064 - acc: 0.9606 - val_loss: 0.1156 - val_acc: 0.9585\n",
      "Epoch 99/200\n",
      "198/198 - 2s - loss: 0.1065 - acc: 0.9604 - val_loss: 0.1156 - val_acc: 0.9584\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 1.0000000608891671e-05.\n",
      "Epoch 100/200\n",
      "198/198 - 2s - loss: 0.1057 - acc: 0.9608 - val_loss: 0.1153 - val_acc: 0.9586\n",
      "Epoch 101/200\n",
      "198/198 - 2s - loss: 0.1057 - acc: 0.9605 - val_loss: 0.1158 - val_acc: 0.9582\n",
      "Epoch 102/200\n",
      "198/198 - 2s - loss: 0.1055 - acc: 0.9609 - val_loss: 0.1153 - val_acc: 0.9586\n",
      "Epoch 103/200\n",
      "198/198 - 2s - loss: 0.1054 - acc: 0.9607 - val_loss: 0.1155 - val_acc: 0.9585\n",
      "Epoch 104/200\n",
      "198/198 - 2s - loss: 0.1054 - acc: 0.9607 - val_loss: 0.1155 - val_acc: 0.9586\n",
      "\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 00104: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 5.3865 - acc: 0.8530 - val_loss: 0.5134 - val_acc: 0.9041\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.5729 - acc: 0.8909 - val_loss: 0.3037 - val_acc: 0.8975\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.2362 - acc: 0.9227 - val_loss: 0.1705 - val_acc: 0.9437\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.1918 - acc: 0.9345 - val_loss: 0.1533 - val_acc: 0.9479\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1810 - acc: 0.9371 - val_loss: 0.1571 - val_acc: 0.9463\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1569 - acc: 0.9442 - val_loss: 0.1491 - val_acc: 0.9490\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1566 - acc: 0.9447 - val_loss: 0.2059 - val_acc: 0.9204\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1638 - acc: 0.9429 - val_loss: 0.1604 - val_acc: 0.9414\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1451 - acc: 0.9488 - val_loss: 0.1531 - val_acc: 0.9471\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1580 - acc: 0.9439 - val_loss: 0.1697 - val_acc: 0.9391\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1472 - acc: 0.9477 - val_loss: 0.1359 - val_acc: 0.9519\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1368 - acc: 0.9511 - val_loss: 0.1332 - val_acc: 0.9529\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1414 - acc: 0.9492 - val_loss: 0.1484 - val_acc: 0.9495\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1369 - acc: 0.9511 - val_loss: 0.1355 - val_acc: 0.9514\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1362 - acc: 0.9509 - val_loss: 0.1531 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1337 - acc: 0.9520 - val_loss: 0.1479 - val_acc: 0.9478\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1341 - acc: 0.9520 - val_loss: 0.1332 - val_acc: 0.9521\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1272 - acc: 0.9543 - val_loss: 0.1339 - val_acc: 0.9519\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1285 - acc: 0.9539 - val_loss: 0.1348 - val_acc: 0.9503\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1288 - acc: 0.9537 - val_loss: 0.1302 - val_acc: 0.9527\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1334 - acc: 0.9523 - val_loss: 0.1360 - val_acc: 0.9520\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1309 - acc: 0.9528 - val_loss: 0.1318 - val_acc: 0.9525\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1287 - acc: 0.9538 - val_loss: 0.1293 - val_acc: 0.9530\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1304 - acc: 0.9533 - val_loss: 0.1369 - val_acc: 0.9499\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1318 - acc: 0.9525 - val_loss: 0.1315 - val_acc: 0.9535\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1320 - acc: 0.9527 - val_loss: 0.1304 - val_acc: 0.9535\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1300 - acc: 0.9534 - val_loss: 0.1280 - val_acc: 0.9541\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1374 - acc: 0.9507 - val_loss: 0.1342 - val_acc: 0.9511\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1307 - acc: 0.9530 - val_loss: 0.1325 - val_acc: 0.9525\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1283 - acc: 0.9535 - val_loss: 0.1288 - val_acc: 0.9544\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1307 - acc: 0.9531 - val_loss: 0.1321 - val_acc: 0.9522\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1303 - acc: 0.9526 - val_loss: 0.1331 - val_acc: 0.9532\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 33/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 - 2s - loss: 0.1226 - acc: 0.9553 - val_loss: 0.1259 - val_acc: 0.9548\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1228 - acc: 0.9555 - val_loss: 0.1265 - val_acc: 0.9543\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1227 - acc: 0.9556 - val_loss: 0.1326 - val_acc: 0.9522\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1245 - acc: 0.9553 - val_loss: 0.1310 - val_acc: 0.9542\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1215 - acc: 0.9560 - val_loss: 0.1268 - val_acc: 0.9552\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1237 - acc: 0.9554 - val_loss: 0.1314 - val_acc: 0.9522\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1196 - acc: 0.9569 - val_loss: 0.1227 - val_acc: 0.9559\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1176 - acc: 0.9574 - val_loss: 0.1262 - val_acc: 0.9544\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1199 - acc: 0.9565 - val_loss: 0.1366 - val_acc: 0.9496\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1204 - acc: 0.9564 - val_loss: 0.1245 - val_acc: 0.9544\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1204 - acc: 0.9563 - val_loss: 0.1226 - val_acc: 0.9560\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1194 - acc: 0.9569 - val_loss: 0.1303 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1158 - acc: 0.9578 - val_loss: 0.1238 - val_acc: 0.9553\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1163 - acc: 0.9578 - val_loss: 0.1205 - val_acc: 0.9560\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1170 - acc: 0.9574 - val_loss: 0.1218 - val_acc: 0.9563\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1161 - acc: 0.9576 - val_loss: 0.1233 - val_acc: 0.9553\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1162 - acc: 0.9579 - val_loss: 0.1202 - val_acc: 0.9563\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1151 - acc: 0.9581 - val_loss: 0.1195 - val_acc: 0.9569\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.1155 - acc: 0.9583 - val_loss: 0.1234 - val_acc: 0.9549\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.1147 - acc: 0.9583 - val_loss: 0.1255 - val_acc: 0.9539\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.1175 - acc: 0.9574 - val_loss: 0.1202 - val_acc: 0.9560\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.1156 - acc: 0.9585 - val_loss: 0.1226 - val_acc: 0.9562\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.1153 - acc: 0.9583 - val_loss: 0.1196 - val_acc: 0.9569\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.1122 - acc: 0.9592 - val_loss: 0.1203 - val_acc: 0.9561\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.1117 - acc: 0.9594 - val_loss: 0.1184 - val_acc: 0.9568\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.1115 - acc: 0.9593 - val_loss: 0.1183 - val_acc: 0.9570\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.1141 - acc: 0.9582 - val_loss: 0.1212 - val_acc: 0.9563\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.1121 - acc: 0.9593 - val_loss: 0.1188 - val_acc: 0.9562\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.1115 - acc: 0.9594 - val_loss: 0.1304 - val_acc: 0.9530\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.1117 - acc: 0.9592 - val_loss: 0.1183 - val_acc: 0.9568\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.1116 - acc: 0.9591 - val_loss: 0.1216 - val_acc: 0.9558\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.1111 - acc: 0.9594 - val_loss: 0.1176 - val_acc: 0.9573\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.1115 - acc: 0.9595 - val_loss: 0.1189 - val_acc: 0.9564\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.1112 - acc: 0.9592 - val_loss: 0.1186 - val_acc: 0.9571\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.1111 - acc: 0.9596 - val_loss: 0.1233 - val_acc: 0.9544\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.1117 - acc: 0.9591 - val_loss: 0.1178 - val_acc: 0.9572\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.1116 - acc: 0.9593 - val_loss: 0.1205 - val_acc: 0.9560\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.1078 - acc: 0.9604 - val_loss: 0.1169 - val_acc: 0.9571\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.1073 - acc: 0.9606 - val_loss: 0.1180 - val_acc: 0.9572\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.1076 - acc: 0.9607 - val_loss: 0.1179 - val_acc: 0.9570\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.1069 - acc: 0.9606 - val_loss: 0.1172 - val_acc: 0.9573\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.1075 - acc: 0.9606 - val_loss: 0.1188 - val_acc: 0.9566\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.1076 - acc: 0.9605 - val_loss: 0.1169 - val_acc: 0.9571\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.1055 - acc: 0.9614 - val_loss: 0.1161 - val_acc: 0.9577\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.1048 - acc: 0.9614 - val_loss: 0.1185 - val_acc: 0.9566\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.1053 - acc: 0.9616 - val_loss: 0.1166 - val_acc: 0.9576\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.1049 - acc: 0.9613 - val_loss: 0.1199 - val_acc: 0.9563\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.1052 - acc: 0.9614 - val_loss: 0.1170 - val_acc: 0.9570\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.1048 - acc: 0.9616 - val_loss: 0.1174 - val_acc: 0.9569\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.1037 - acc: 0.9617 - val_loss: 0.1155 - val_acc: 0.9577\n",
      "Epoch 83/200\n",
      "198/198 - 2s - loss: 0.1035 - acc: 0.9618 - val_loss: 0.1167 - val_acc: 0.9573\n",
      "Epoch 84/200\n",
      "198/198 - 2s - loss: 0.1031 - acc: 0.9622 - val_loss: 0.1161 - val_acc: 0.9574\n",
      "Epoch 85/200\n",
      "198/198 - 2s - loss: 0.1033 - acc: 0.9618 - val_loss: 0.1158 - val_acc: 0.9577\n",
      "Epoch 86/200\n",
      "198/198 - 2s - loss: 0.1029 - acc: 0.9621 - val_loss: 0.1156 - val_acc: 0.9572\n",
      "Epoch 87/200\n",
      "198/198 - 2s - loss: 0.1028 - acc: 0.9620 - val_loss: 0.1164 - val_acc: 0.9572\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.5848932274101303e-05.\n",
      "Epoch 88/200\n",
      "198/198 - 2s - loss: 0.1017 - acc: 0.9626 - val_loss: 0.1153 - val_acc: 0.9575\n",
      "Epoch 89/200\n",
      "198/198 - 2s - loss: 0.1022 - acc: 0.9623 - val_loss: 0.1165 - val_acc: 0.9575\n",
      "Epoch 90/200\n",
      "198/198 - 2s - loss: 0.1016 - acc: 0.9626 - val_loss: 0.1155 - val_acc: 0.9576\n",
      "Epoch 91/200\n",
      "198/198 - 2s - loss: 0.1015 - acc: 0.9625 - val_loss: 0.1163 - val_acc: 0.9575\n",
      "Epoch 92/200\n",
      "198/198 - 2s - loss: 0.1017 - acc: 0.9624 - val_loss: 0.1171 - val_acc: 0.9568\n",
      "Epoch 93/200\n",
      "198/198 - 2s - loss: 0.1013 - acc: 0.9628 - val_loss: 0.1157 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1.0000000608891671e-05.\n",
      "Epoch 94/200\n",
      "198/198 - 2s - loss: 0.1008 - acc: 0.9629 - val_loss: 0.1154 - val_acc: 0.9577\n",
      "Epoch 95/200\n",
      "198/198 - 2s - loss: 0.1008 - acc: 0.9628 - val_loss: 0.1162 - val_acc: 0.9576\n",
      "Epoch 96/200\n",
      "198/198 - 2s - loss: 0.1007 - acc: 0.9628 - val_loss: 0.1170 - val_acc: 0.9575\n",
      "Epoch 97/200\n",
      "198/198 - 2s - loss: 0.1007 - acc: 0.9628 - val_loss: 0.1179 - val_acc: 0.9569\n",
      "Epoch 98/200\n",
      "198/198 - 2s - loss: 0.1008 - acc: 0.9628 - val_loss: 0.1165 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 00098: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 10.2506 - acc: 0.8436 - val_loss: 0.4839 - val_acc: 0.9193\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.4555 - acc: 0.8998 - val_loss: 0.1962 - val_acc: 0.9422\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.3136 - acc: 0.9128 - val_loss: 0.1684 - val_acc: 0.9438\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.1855 - acc: 0.9367 - val_loss: 0.1739 - val_acc: 0.9356\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1707 - acc: 0.9401 - val_loss: 0.1690 - val_acc: 0.9430\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1614 - acc: 0.9430 - val_loss: 0.2441 - val_acc: 0.9192\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1769 - acc: 0.9395 - val_loss: 0.1465 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1621 - acc: 0.9429 - val_loss: 0.1397 - val_acc: 0.9504\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1466 - acc: 0.9482 - val_loss: 0.1403 - val_acc: 0.9498\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1397 - acc: 0.9508 - val_loss: 0.1338 - val_acc: 0.9529\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1383 - acc: 0.9508 - val_loss: 0.1470 - val_acc: 0.9504\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1450 - acc: 0.9492 - val_loss: 0.1495 - val_acc: 0.9490\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1361 - acc: 0.9515 - val_loss: 0.1412 - val_acc: 0.9520\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1419 - acc: 0.9503 - val_loss: 0.1444 - val_acc: 0.9485\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1389 - acc: 0.9506 - val_loss: 0.1361 - val_acc: 0.9527\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1326 - acc: 0.9529 - val_loss: 0.1365 - val_acc: 0.9512\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1306 - acc: 0.9533 - val_loss: 0.1480 - val_acc: 0.9494\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1285 - acc: 0.9544 - val_loss: 0.1285 - val_acc: 0.9541\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1283 - acc: 0.9540 - val_loss: 0.1309 - val_acc: 0.9534\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1311 - acc: 0.9532 - val_loss: 0.1299 - val_acc: 0.9543\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1312 - acc: 0.9533 - val_loss: 0.1355 - val_acc: 0.9517\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1300 - acc: 0.9541 - val_loss: 0.1287 - val_acc: 0.9545\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1296 - acc: 0.9538 - val_loss: 0.1307 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1283 - acc: 0.9540 - val_loss: 0.1262 - val_acc: 0.9547\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1243 - acc: 0.9554 - val_loss: 0.1280 - val_acc: 0.9538\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1236 - acc: 0.9558 - val_loss: 0.1319 - val_acc: 0.9530\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1235 - acc: 0.9557 - val_loss: 0.1359 - val_acc: 0.9523\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1282 - acc: 0.9544 - val_loss: 0.1419 - val_acc: 0.9504\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1226 - acc: 0.9561 - val_loss: 0.1251 - val_acc: 0.9553\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1292 - acc: 0.9536 - val_loss: 0.1282 - val_acc: 0.9537\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1241 - acc: 0.9557 - val_loss: 0.1348 - val_acc: 0.9532\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1251 - acc: 0.9553 - val_loss: 0.1237 - val_acc: 0.9559\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1267 - acc: 0.9549 - val_loss: 0.1301 - val_acc: 0.9539\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1260 - acc: 0.9550 - val_loss: 0.1263 - val_acc: 0.9538\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1262 - acc: 0.9546 - val_loss: 0.1232 - val_acc: 0.9556\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1238 - acc: 0.9560 - val_loss: 0.1226 - val_acc: 0.9557\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1252 - acc: 0.9550 - val_loss: 0.2024 - val_acc: 0.9273\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1253 - acc: 0.9547 - val_loss: 0.1216 - val_acc: 0.9560\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1273 - acc: 0.9545 - val_loss: 0.1275 - val_acc: 0.9546\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1233 - acc: 0.9561 - val_loss: 0.1306 - val_acc: 0.9546\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1230 - acc: 0.9560 - val_loss: 0.1267 - val_acc: 0.9545\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1228 - acc: 0.9562 - val_loss: 0.1228 - val_acc: 0.9559\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1227 - acc: 0.9560 - val_loss: 0.1224 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1168 - acc: 0.9577 - val_loss: 0.1212 - val_acc: 0.9566\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1183 - acc: 0.9569 - val_loss: 0.1206 - val_acc: 0.9570\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1164 - acc: 0.9578 - val_loss: 0.1292 - val_acc: 0.9539\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1176 - acc: 0.9576 - val_loss: 0.1226 - val_acc: 0.9554\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1172 - acc: 0.9579 - val_loss: 0.1347 - val_acc: 0.9517\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1176 - acc: 0.9577 - val_loss: 0.1264 - val_acc: 0.9548\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1167 - acc: 0.9577 - val_loss: 0.1421 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.1141 - acc: 0.9585 - val_loss: 0.1195 - val_acc: 0.9570\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.1135 - acc: 0.9588 - val_loss: 0.1256 - val_acc: 0.9548\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.1129 - acc: 0.9593 - val_loss: 0.1166 - val_acc: 0.9578\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.1137 - acc: 0.9587 - val_loss: 0.1225 - val_acc: 0.9562\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.1135 - acc: 0.9586 - val_loss: 0.1214 - val_acc: 0.9567\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.1131 - acc: 0.9590 - val_loss: 0.1231 - val_acc: 0.9559\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.1141 - acc: 0.9585 - val_loss: 0.1181 - val_acc: 0.9578\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.1126 - acc: 0.9594 - val_loss: 0.1172 - val_acc: 0.9579\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.1101 - acc: 0.9602 - val_loss: 0.1160 - val_acc: 0.9581\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.1090 - acc: 0.9604 - val_loss: 0.1171 - val_acc: 0.9576\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.1094 - acc: 0.9602 - val_loss: 0.1193 - val_acc: 0.9576\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.1096 - acc: 0.9602 - val_loss: 0.1160 - val_acc: 0.9580\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.1093 - acc: 0.9599 - val_loss: 0.1178 - val_acc: 0.9577\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.1089 - acc: 0.9600 - val_loss: 0.1196 - val_acc: 0.9569\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.1065 - acc: 0.9611 - val_loss: 0.1175 - val_acc: 0.9569\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.1070 - acc: 0.9607 - val_loss: 0.1190 - val_acc: 0.9574\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.1067 - acc: 0.9611 - val_loss: 0.1183 - val_acc: 0.9576\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.1062 - acc: 0.9611 - val_loss: 0.1177 - val_acc: 0.9575\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.1060 - acc: 0.9613 - val_loss: 0.1155 - val_acc: 0.9583\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.1056 - acc: 0.9612 - val_loss: 0.1150 - val_acc: 0.9585\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.1064 - acc: 0.9609 - val_loss: 0.1149 - val_acc: 0.9588\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.1058 - acc: 0.9613 - val_loss: 0.1160 - val_acc: 0.9582\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.1053 - acc: 0.9613 - val_loss: 0.1165 - val_acc: 0.9583\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.1054 - acc: 0.9613 - val_loss: 0.1216 - val_acc: 0.9558\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.1049 - acc: 0.9613 - val_loss: 0.1148 - val_acc: 0.9591\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.1053 - acc: 0.9614 - val_loss: 0.1150 - val_acc: 0.9582\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.1047 - acc: 0.9618 - val_loss: 0.1165 - val_acc: 0.9583\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.1051 - acc: 0.9616 - val_loss: 0.1166 - val_acc: 0.9581\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.1043 - acc: 0.9622 - val_loss: 0.1202 - val_acc: 0.9571\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.1041 - acc: 0.9616 - val_loss: 0.1161 - val_acc: 0.9581\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.1028 - acc: 0.9621 - val_loss: 0.1140 - val_acc: 0.9588\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.1017 - acc: 0.9626 - val_loss: 0.1175 - val_acc: 0.9575\n",
      "Epoch 83/200\n",
      "198/198 - 2s - loss: 0.1019 - acc: 0.9624 - val_loss: 0.1149 - val_acc: 0.9586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/200\n",
      "198/198 - 2s - loss: 0.1019 - acc: 0.9624 - val_loss: 0.1159 - val_acc: 0.9587\n",
      "Epoch 85/200\n",
      "198/198 - 2s - loss: 0.1017 - acc: 0.9625 - val_loss: 0.1192 - val_acc: 0.9576\n",
      "Epoch 86/200\n",
      "198/198 - 2s - loss: 0.1019 - acc: 0.9624 - val_loss: 0.1156 - val_acc: 0.9579\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 87/200\n",
      "198/198 - 2s - loss: 0.1000 - acc: 0.9632 - val_loss: 0.1153 - val_acc: 0.9589\n",
      "Epoch 88/200\n",
      "198/198 - 2s - loss: 0.1001 - acc: 0.9629 - val_loss: 0.1165 - val_acc: 0.9587\n",
      "Epoch 89/200\n",
      "198/198 - 2s - loss: 0.1000 - acc: 0.9629 - val_loss: 0.1161 - val_acc: 0.9588\n",
      "Epoch 90/200\n",
      "198/198 - 2s - loss: 0.0999 - acc: 0.9630 - val_loss: 0.1144 - val_acc: 0.9592\n",
      "Epoch 91/200\n",
      "198/198 - 2s - loss: 0.0997 - acc: 0.9632 - val_loss: 0.1181 - val_acc: 0.9582\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.5848932274101303e-05.\n",
      "Epoch 00091: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 10.0385 - acc: 0.8395 - val_loss: 0.4864 - val_acc: 0.8982\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.4289 - acc: 0.9023 - val_loss: 0.1708 - val_acc: 0.9409\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.3205 - acc: 0.9137 - val_loss: 0.4881 - val_acc: 0.8413\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2041 - acc: 0.9324 - val_loss: 0.1521 - val_acc: 0.9471\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1711 - acc: 0.9403 - val_loss: 0.1568 - val_acc: 0.9473\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1694 - acc: 0.9407 - val_loss: 0.1678 - val_acc: 0.9450\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1621 - acc: 0.9431 - val_loss: 0.1402 - val_acc: 0.9517\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1755 - acc: 0.9391 - val_loss: 0.1691 - val_acc: 0.9372\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1532 - acc: 0.9459 - val_loss: 0.1405 - val_acc: 0.9517\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1427 - acc: 0.9495 - val_loss: 0.1330 - val_acc: 0.9524\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1428 - acc: 0.9489 - val_loss: 0.1349 - val_acc: 0.9520\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1461 - acc: 0.9479 - val_loss: 0.2574 - val_acc: 0.9144\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1427 - acc: 0.9497 - val_loss: 0.1775 - val_acc: 0.9318\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1429 - acc: 0.9493 - val_loss: 0.1383 - val_acc: 0.9499\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1372 - acc: 0.9512 - val_loss: 0.1354 - val_acc: 0.9522\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1307 - acc: 0.9534 - val_loss: 0.1274 - val_acc: 0.9546\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1309 - acc: 0.9534 - val_loss: 0.1309 - val_acc: 0.9541\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1308 - acc: 0.9534 - val_loss: 0.1359 - val_acc: 0.9518\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1332 - acc: 0.9524 - val_loss: 0.1296 - val_acc: 0.9536\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1381 - acc: 0.9507 - val_loss: 0.1287 - val_acc: 0.9543\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1394 - acc: 0.9502 - val_loss: 0.1330 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1277 - acc: 0.9541 - val_loss: 0.1330 - val_acc: 0.9548\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1294 - acc: 0.9534 - val_loss: 0.1402 - val_acc: 0.9513\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1269 - acc: 0.9542 - val_loss: 0.1235 - val_acc: 0.9563\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1249 - acc: 0.9549 - val_loss: 0.1237 - val_acc: 0.9560\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1252 - acc: 0.9552 - val_loss: 0.1247 - val_acc: 0.9559\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1291 - acc: 0.9537 - val_loss: 0.1245 - val_acc: 0.9563\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1317 - acc: 0.9527 - val_loss: 0.1423 - val_acc: 0.9497\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1276 - acc: 0.9543 - val_loss: 0.1357 - val_acc: 0.9522\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1243 - acc: 0.9548 - val_loss: 0.1207 - val_acc: 0.9571\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1224 - acc: 0.9557 - val_loss: 0.1250 - val_acc: 0.9554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1224 - acc: 0.9560 - val_loss: 0.1257 - val_acc: 0.9547\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1248 - acc: 0.9551 - val_loss: 0.1272 - val_acc: 0.9545\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1225 - acc: 0.9559 - val_loss: 0.1268 - val_acc: 0.9550\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1271 - acc: 0.9541 - val_loss: 0.1294 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1194 - acc: 0.9566 - val_loss: 0.1200 - val_acc: 0.9574\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1203 - acc: 0.9563 - val_loss: 0.1200 - val_acc: 0.9573\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1206 - acc: 0.9563 - val_loss: 0.1262 - val_acc: 0.9554\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1218 - acc: 0.9559 - val_loss: 0.1210 - val_acc: 0.9573\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1199 - acc: 0.9567 - val_loss: 0.1197 - val_acc: 0.9569\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1201 - acc: 0.9564 - val_loss: 0.1253 - val_acc: 0.9546\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1200 - acc: 0.9567 - val_loss: 0.1201 - val_acc: 0.9574\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1211 - acc: 0.9561 - val_loss: 0.1199 - val_acc: 0.9571\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1233 - acc: 0.9556 - val_loss: 0.1438 - val_acc: 0.9484\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1202 - acc: 0.9564 - val_loss: 0.1205 - val_acc: 0.9567\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1168 - acc: 0.9575 - val_loss: 0.1204 - val_acc: 0.9574\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1181 - acc: 0.9569 - val_loss: 0.1197 - val_acc: 0.9577\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1174 - acc: 0.9574 - val_loss: 0.1196 - val_acc: 0.9576\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1179 - acc: 0.9576 - val_loss: 0.1210 - val_acc: 0.9565\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1167 - acc: 0.9576 - val_loss: 0.1233 - val_acc: 0.9561\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.1176 - acc: 0.9575 - val_loss: 0.1193 - val_acc: 0.9576\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.1166 - acc: 0.9575 - val_loss: 0.1210 - val_acc: 0.9565\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.1178 - acc: 0.9570 - val_loss: 0.1246 - val_acc: 0.9552\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.1178 - acc: 0.9572 - val_loss: 0.1219 - val_acc: 0.9564\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.1167 - acc: 0.9580 - val_loss: 0.1331 - val_acc: 0.9517\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.1164 - acc: 0.9576 - val_loss: 0.1252 - val_acc: 0.9568\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.1140 - acc: 0.9587 - val_loss: 0.1171 - val_acc: 0.9583\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.1146 - acc: 0.9584 - val_loss: 0.1190 - val_acc: 0.9577\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.1144 - acc: 0.9584 - val_loss: 0.1292 - val_acc: 0.9537\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.1149 - acc: 0.9577 - val_loss: 0.1172 - val_acc: 0.9586\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.1133 - acc: 0.9587 - val_loss: 0.1167 - val_acc: 0.9581\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.1132 - acc: 0.9585 - val_loss: 0.1186 - val_acc: 0.9571\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.1131 - acc: 0.9583 - val_loss: 0.1217 - val_acc: 0.9557\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.1148 - acc: 0.9581 - val_loss: 0.1185 - val_acc: 0.9581\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.1128 - acc: 0.9588 - val_loss: 0.1163 - val_acc: 0.9583\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.1124 - acc: 0.9593 - val_loss: 0.1159 - val_acc: 0.9586\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.1126 - acc: 0.9592 - val_loss: 0.1216 - val_acc: 0.9564\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.1133 - acc: 0.9589 - val_loss: 0.1191 - val_acc: 0.9573\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.1141 - acc: 0.9586 - val_loss: 0.1182 - val_acc: 0.9572\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.1117 - acc: 0.9589 - val_loss: 0.1215 - val_acc: 0.9560\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.1124 - acc: 0.9591 - val_loss: 0.1171 - val_acc: 0.9575\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.1100 - acc: 0.9598 - val_loss: 0.1148 - val_acc: 0.9589\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.1095 - acc: 0.9598 - val_loss: 0.1153 - val_acc: 0.9585\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.1093 - acc: 0.9603 - val_loss: 0.1181 - val_acc: 0.9573\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.1098 - acc: 0.9599 - val_loss: 0.1159 - val_acc: 0.9588\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.1093 - acc: 0.9602 - val_loss: 0.1162 - val_acc: 0.9581\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.1092 - acc: 0.9598 - val_loss: 0.1174 - val_acc: 0.9578\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.1080 - acc: 0.9602 - val_loss: 0.1152 - val_acc: 0.9588\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.1073 - acc: 0.9605 - val_loss: 0.1149 - val_acc: 0.9588\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.1076 - acc: 0.9605 - val_loss: 0.1152 - val_acc: 0.9587\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.1082 - acc: 0.9604 - val_loss: 0.1159 - val_acc: 0.9587\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.1071 - acc: 0.9606 - val_loss: 0.1159 - val_acc: 0.9585\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 1.5848932274101303e-05.\n",
      "Epoch 00082: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 - 2s - loss: 10.4578 - acc: 0.8371 - val_loss: 1.0229 - val_acc: 0.8073\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.3374 - acc: 0.9039 - val_loss: 0.1816 - val_acc: 0.9359\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.3832 - acc: 0.9138 - val_loss: 0.1711 - val_acc: 0.9446\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2015 - acc: 0.9299 - val_loss: 0.1511 - val_acc: 0.9482\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1712 - acc: 0.9396 - val_loss: 0.1509 - val_acc: 0.9495\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1604 - acc: 0.9430 - val_loss: 0.1651 - val_acc: 0.9432\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1566 - acc: 0.9445 - val_loss: 0.1413 - val_acc: 0.9516\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1568 - acc: 0.9444 - val_loss: 0.1778 - val_acc: 0.9400\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1588 - acc: 0.9436 - val_loss: 0.1806 - val_acc: 0.9377\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1461 - acc: 0.9477 - val_loss: 0.1329 - val_acc: 0.9531\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1566 - acc: 0.9439 - val_loss: 0.1846 - val_acc: 0.9300\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1454 - acc: 0.9484 - val_loss: 0.1376 - val_acc: 0.9516\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1416 - acc: 0.9493 - val_loss: 0.1364 - val_acc: 0.9513\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1392 - acc: 0.9502 - val_loss: 0.1367 - val_acc: 0.9514\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1371 - acc: 0.9509 - val_loss: 0.1295 - val_acc: 0.9543\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1375 - acc: 0.9508 - val_loss: 0.1309 - val_acc: 0.9548\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1384 - acc: 0.9505 - val_loss: 0.1536 - val_acc: 0.9444\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1359 - acc: 0.9513 - val_loss: 0.1438 - val_acc: 0.9492\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1405 - acc: 0.9497 - val_loss: 0.1421 - val_acc: 0.9504\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1361 - acc: 0.9510 - val_loss: 0.1526 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1288 - acc: 0.9537 - val_loss: 0.1275 - val_acc: 0.9538\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1282 - acc: 0.9536 - val_loss: 0.1465 - val_acc: 0.9468\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1293 - acc: 0.9531 - val_loss: 0.1253 - val_acc: 0.9544\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1300 - acc: 0.9534 - val_loss: 0.1420 - val_acc: 0.9519\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1322 - acc: 0.9523 - val_loss: 0.1317 - val_acc: 0.9525\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1310 - acc: 0.9529 - val_loss: 0.1302 - val_acc: 0.9540\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1314 - acc: 0.9524 - val_loss: 0.1277 - val_acc: 0.9543\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1288 - acc: 0.9533 - val_loss: 0.1496 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1248 - acc: 0.9547 - val_loss: 0.1211 - val_acc: 0.9566\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1261 - acc: 0.9542 - val_loss: 0.1496 - val_acc: 0.9468\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1241 - acc: 0.9551 - val_loss: 0.1219 - val_acc: 0.9569\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1249 - acc: 0.9543 - val_loss: 0.1234 - val_acc: 0.9562\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1272 - acc: 0.9539 - val_loss: 0.1328 - val_acc: 0.9534\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1275 - acc: 0.9543 - val_loss: 0.1240 - val_acc: 0.9562\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1201 - acc: 0.9563 - val_loss: 0.1216 - val_acc: 0.9567\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1204 - acc: 0.9562 - val_loss: 0.1229 - val_acc: 0.9557\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1218 - acc: 0.9556 - val_loss: 0.1201 - val_acc: 0.9574\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1202 - acc: 0.9564 - val_loss: 0.1221 - val_acc: 0.9561\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1215 - acc: 0.9558 - val_loss: 0.1241 - val_acc: 0.9556\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1212 - acc: 0.9559 - val_loss: 0.1199 - val_acc: 0.9575\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1208 - acc: 0.9558 - val_loss: 0.1202 - val_acc: 0.9579\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1222 - acc: 0.9558 - val_loss: 0.1222 - val_acc: 0.9577\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1214 - acc: 0.9558 - val_loss: 0.1280 - val_acc: 0.9539\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1225 - acc: 0.9557 - val_loss: 0.1347 - val_acc: 0.9503\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1212 - acc: 0.9563 - val_loss: 0.1252 - val_acc: 0.9564\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1182 - acc: 0.9569 - val_loss: 0.1232 - val_acc: 0.9570\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1185 - acc: 0.9565 - val_loss: 0.1202 - val_acc: 0.9575\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1185 - acc: 0.9570 - val_loss: 0.1230 - val_acc: 0.9565\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1173 - acc: 0.9571 - val_loss: 0.1449 - val_acc: 0.9465\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1173 - acc: 0.9572 - val_loss: 0.1227 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 00050: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 8.3389 - acc: 0.8507 - val_loss: 0.4091 - val_acc: 0.9118\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.4658 - acc: 0.9063 - val_loss: 0.1786 - val_acc: 0.9386\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.3359 - acc: 0.9155 - val_loss: 0.1660 - val_acc: 0.9446\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.1917 - acc: 0.9340 - val_loss: 0.1561 - val_acc: 0.9456\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.2280 - acc: 0.9300 - val_loss: 0.1518 - val_acc: 0.9473\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1695 - acc: 0.9408 - val_loss: 0.1622 - val_acc: 0.9418\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1623 - acc: 0.9436 - val_loss: 0.1409 - val_acc: 0.9501\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1553 - acc: 0.9450 - val_loss: 0.1588 - val_acc: 0.9421\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1538 - acc: 0.9455 - val_loss: 0.1356 - val_acc: 0.9522\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1539 - acc: 0.9456 - val_loss: 0.1440 - val_acc: 0.9502\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1501 - acc: 0.9474 - val_loss: 0.1335 - val_acc: 0.9516\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1443 - acc: 0.9490 - val_loss: 0.1392 - val_acc: 0.9497\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1438 - acc: 0.9489 - val_loss: 0.1514 - val_acc: 0.9446\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1408 - acc: 0.9502 - val_loss: 0.1490 - val_acc: 0.9452\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1413 - acc: 0.9499 - val_loss: 0.1418 - val_acc: 0.9521\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1484 - acc: 0.9479 - val_loss: 0.1431 - val_acc: 0.9481\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1351 - acc: 0.9516 - val_loss: 0.1336 - val_acc: 0.9515\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1318 - acc: 0.9528 - val_loss: 0.1313 - val_acc: 0.9538\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1327 - acc: 0.9528 - val_loss: 0.1295 - val_acc: 0.9534\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1366 - acc: 0.9515 - val_loss: 0.1359 - val_acc: 0.9523\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1340 - acc: 0.9521 - val_loss: 0.1293 - val_acc: 0.9536\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1338 - acc: 0.9527 - val_loss: 0.1331 - val_acc: 0.9526\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1339 - acc: 0.9522 - val_loss: 0.1351 - val_acc: 0.9510\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1379 - acc: 0.9507 - val_loss: 0.1500 - val_acc: 0.9472\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1330 - acc: 0.9529 - val_loss: 0.1424 - val_acc: 0.9480\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1351 - acc: 0.9518 - val_loss: 0.1392 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1276 - acc: 0.9548 - val_loss: 0.1321 - val_acc: 0.9516\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1291 - acc: 0.9538 - val_loss: 0.1285 - val_acc: 0.9535\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1299 - acc: 0.9536 - val_loss: 0.1268 - val_acc: 0.9541\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1272 - acc: 0.9541 - val_loss: 0.1311 - val_acc: 0.9525\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1299 - acc: 0.9539 - val_loss: 0.1266 - val_acc: 0.9541\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1289 - acc: 0.9538 - val_loss: 0.1274 - val_acc: 0.9542\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1298 - acc: 0.9539 - val_loss: 0.1262 - val_acc: 0.9541\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1311 - acc: 0.9534 - val_loss: 0.1257 - val_acc: 0.9544\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1327 - acc: 0.9529 - val_loss: 0.1253 - val_acc: 0.9550\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1309 - acc: 0.9529 - val_loss: 0.1499 - val_acc: 0.9460\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1303 - acc: 0.9531 - val_loss: 0.1337 - val_acc: 0.9523\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1303 - acc: 0.9531 - val_loss: 0.1504 - val_acc: 0.9464\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1282 - acc: 0.9543 - val_loss: 0.1246 - val_acc: 0.9552\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1302 - acc: 0.9534 - val_loss: 0.1352 - val_acc: 0.9512\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1301 - acc: 0.9531 - val_loss: 0.1263 - val_acc: 0.9548\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1279 - acc: 0.9541 - val_loss: 0.1307 - val_acc: 0.9534\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1289 - acc: 0.9543 - val_loss: 0.1290 - val_acc: 0.9533\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1278 - acc: 0.9544 - val_loss: 0.1476 - val_acc: 0.9472\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1222 - acc: 0.9560 - val_loss: 0.1224 - val_acc: 0.9555\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1236 - acc: 0.9555 - val_loss: 0.1288 - val_acc: 0.9538\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1239 - acc: 0.9558 - val_loss: 0.1225 - val_acc: 0.9557\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1224 - acc: 0.9561 - val_loss: 0.1224 - val_acc: 0.9555\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1237 - acc: 0.9557 - val_loss: 0.1244 - val_acc: 0.9557\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1214 - acc: 0.9566 - val_loss: 0.1278 - val_acc: 0.9535\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.1190 - acc: 0.9569 - val_loss: 0.1193 - val_acc: 0.9569\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.1189 - acc: 0.9569 - val_loss: 0.1297 - val_acc: 0.9530\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.1189 - acc: 0.9570 - val_loss: 0.1190 - val_acc: 0.9570\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.1193 - acc: 0.9571 - val_loss: 0.1205 - val_acc: 0.9568\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.1187 - acc: 0.9571 - val_loss: 0.1281 - val_acc: 0.9542\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.1146 - acc: 0.9585 - val_loss: 0.1247 - val_acc: 0.9552\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.1144 - acc: 0.9586 - val_loss: 0.1174 - val_acc: 0.9572\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.1142 - acc: 0.9584 - val_loss: 0.1170 - val_acc: 0.9576\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.1140 - acc: 0.9584 - val_loss: 0.1194 - val_acc: 0.9573\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.1139 - acc: 0.9586 - val_loss: 0.1187 - val_acc: 0.9570\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.1147 - acc: 0.9581 - val_loss: 0.1171 - val_acc: 0.9576\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.1143 - acc: 0.9583 - val_loss: 0.1324 - val_acc: 0.9519\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.1136 - acc: 0.9587 - val_loss: 0.1237 - val_acc: 0.9553\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.1118 - acc: 0.9591 - val_loss: 0.1166 - val_acc: 0.9574\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.1109 - acc: 0.9596 - val_loss: 0.1191 - val_acc: 0.9564\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.1106 - acc: 0.9594 - val_loss: 0.1185 - val_acc: 0.9570\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.1095 - acc: 0.9596 - val_loss: 0.1166 - val_acc: 0.9574\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.1102 - acc: 0.9598 - val_loss: 0.1227 - val_acc: 0.9563\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.1100 - acc: 0.9599 - val_loss: 0.1185 - val_acc: 0.9568\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.1079 - acc: 0.9604 - val_loss: 0.1161 - val_acc: 0.9579\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.1076 - acc: 0.9605 - val_loss: 0.1170 - val_acc: 0.9579\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.1080 - acc: 0.9603 - val_loss: 0.1162 - val_acc: 0.9576\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.1072 - acc: 0.9608 - val_loss: 0.1179 - val_acc: 0.9575\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.1077 - acc: 0.9605 - val_loss: 0.1156 - val_acc: 0.9579\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.1072 - acc: 0.9604 - val_loss: 0.1169 - val_acc: 0.9575\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.1072 - acc: 0.9606 - val_loss: 0.1174 - val_acc: 0.9573\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.1070 - acc: 0.9607 - val_loss: 0.1155 - val_acc: 0.9581\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.1064 - acc: 0.9608 - val_loss: 0.1153 - val_acc: 0.9582\n",
      "Epoch 83/200\n",
      "198/198 - 2s - loss: 0.1068 - acc: 0.9605 - val_loss: 0.1183 - val_acc: 0.9574\n",
      "Epoch 84/200\n",
      "198/198 - 2s - loss: 0.1065 - acc: 0.9609 - val_loss: 0.1156 - val_acc: 0.9583\n",
      "Epoch 85/200\n",
      "198/198 - 2s - loss: 0.1062 - acc: 0.9608 - val_loss: 0.1162 - val_acc: 0.9576\n",
      "Epoch 86/200\n",
      "198/198 - 2s - loss: 0.1061 - acc: 0.9611 - val_loss: 0.1192 - val_acc: 0.9564\n",
      "Epoch 87/200\n",
      "198/198 - 2s - loss: 0.1058 - acc: 0.9611 - val_loss: 0.1169 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 88/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 - 2s - loss: 0.1047 - acc: 0.9613 - val_loss: 0.1153 - val_acc: 0.9586\n",
      "Epoch 89/200\n",
      "198/198 - 2s - loss: 0.1041 - acc: 0.9616 - val_loss: 0.1155 - val_acc: 0.9579\n",
      "Epoch 90/200\n",
      "198/198 - 2s - loss: 0.1039 - acc: 0.9616 - val_loss: 0.1147 - val_acc: 0.9586\n",
      "Epoch 91/200\n",
      "198/198 - 2s - loss: 0.1040 - acc: 0.9618 - val_loss: 0.1169 - val_acc: 0.9575\n",
      "Epoch 92/200\n",
      "198/198 - 2s - loss: 0.1038 - acc: 0.9618 - val_loss: 0.1162 - val_acc: 0.9579\n",
      "Epoch 93/200\n",
      "198/198 - 2s - loss: 0.1035 - acc: 0.9618 - val_loss: 0.1160 - val_acc: 0.9577\n",
      "Epoch 94/200\n",
      "198/198 - 2s - loss: 0.1035 - acc: 0.9622 - val_loss: 0.1153 - val_acc: 0.9583\n",
      "Epoch 95/200\n",
      "198/198 - 2s - loss: 0.1033 - acc: 0.9618 - val_loss: 0.1154 - val_acc: 0.9584\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 1.5848932274101303e-05.\n",
      "Epoch 96/200\n",
      "198/198 - 2s - loss: 0.1025 - acc: 0.9623 - val_loss: 0.1151 - val_acc: 0.9583\n",
      "Epoch 97/200\n",
      "198/198 - 2s - loss: 0.1023 - acc: 0.9623 - val_loss: 0.1156 - val_acc: 0.9582\n",
      "Epoch 98/200\n",
      "198/198 - 2s - loss: 0.1022 - acc: 0.9621 - val_loss: 0.1159 - val_acc: 0.9579\n",
      "Epoch 99/200\n",
      "198/198 - 2s - loss: 0.1020 - acc: 0.9624 - val_loss: 0.1156 - val_acc: 0.9584\n",
      "Epoch 100/200\n",
      "198/198 - 2s - loss: 0.1022 - acc: 0.9624 - val_loss: 0.1147 - val_acc: 0.9585\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 1.0000000608891671e-05.\n",
      "Epoch 00100: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 9.1875 - acc: 0.8238 - val_loss: 0.4205 - val_acc: 0.9229\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.4703 - acc: 0.8981 - val_loss: 1.3884 - val_acc: 0.8373\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.3926 - acc: 0.9091 - val_loss: 0.2258 - val_acc: 0.9310\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2057 - acc: 0.9302 - val_loss: 0.1737 - val_acc: 0.9442\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1800 - acc: 0.9376 - val_loss: 0.1519 - val_acc: 0.9464\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1866 - acc: 0.9374 - val_loss: 0.1653 - val_acc: 0.9442\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1638 - acc: 0.9426 - val_loss: 0.1444 - val_acc: 0.9495\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1542 - acc: 0.9461 - val_loss: 0.1485 - val_acc: 0.9487\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1487 - acc: 0.9469 - val_loss: 0.1509 - val_acc: 0.9477\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1547 - acc: 0.9455 - val_loss: 0.1513 - val_acc: 0.9466\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1520 - acc: 0.9458 - val_loss: 0.1442 - val_acc: 0.9481\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1439 - acc: 0.9490 - val_loss: 0.1344 - val_acc: 0.9526\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1451 - acc: 0.9486 - val_loss: 0.1384 - val_acc: 0.9526\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1451 - acc: 0.9485 - val_loss: 0.1432 - val_acc: 0.9495\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1387 - acc: 0.9508 - val_loss: 0.1353 - val_acc: 0.9519\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1436 - acc: 0.9493 - val_loss: 0.1894 - val_acc: 0.9332\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 2.0925 - acc: 0.8956 - val_loss: 0.1816 - val_acc: 0.9341\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1662 - acc: 0.9403 - val_loss: 0.1565 - val_acc: 0.9434\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1524 - acc: 0.9464 - val_loss: 0.1816 - val_acc: 0.9369\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1480 - acc: 0.9474 - val_loss: 0.1535 - val_acc: 0.9446\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1472 - acc: 0.9478 - val_loss: 0.1422 - val_acc: 0.9499\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1424 - acc: 0.9497 - val_loss: 0.1602 - val_acc: 0.9431\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 00022: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 8.6172 - acc: 0.8351 - val_loss: 0.6048 - val_acc: 0.9075\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.8196 - acc: 0.8802 - val_loss: 0.5174 - val_acc: 0.8653\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.2469 - acc: 0.9217 - val_loss: 0.1806 - val_acc: 0.9404\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.1872 - acc: 0.9350 - val_loss: 0.2306 - val_acc: 0.9200\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1824 - acc: 0.9379 - val_loss: 0.1596 - val_acc: 0.9456\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1872 - acc: 0.9361 - val_loss: 0.1717 - val_acc: 0.9424\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1642 - acc: 0.9424 - val_loss: 0.1445 - val_acc: 0.9502\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1572 - acc: 0.9449 - val_loss: 0.1472 - val_acc: 0.9484\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1545 - acc: 0.9457 - val_loss: 0.1505 - val_acc: 0.9488\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1551 - acc: 0.9452 - val_loss: 0.1391 - val_acc: 0.9509\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1517 - acc: 0.9465 - val_loss: 0.2065 - val_acc: 0.9226\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1543 - acc: 0.9452 - val_loss: 0.1374 - val_acc: 0.9505\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1462 - acc: 0.9479 - val_loss: 0.1457 - val_acc: 0.9488\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1515 - acc: 0.9457 - val_loss: 0.1497 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1440 - acc: 0.9485 - val_loss: 0.1593 - val_acc: 0.9418\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1447 - acc: 0.9483 - val_loss: 0.1373 - val_acc: 0.9519\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1398 - acc: 0.9503 - val_loss: 0.1535 - val_acc: 0.9483\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1409 - acc: 0.9495 - val_loss: 0.1344 - val_acc: 0.9524\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1447 - acc: 0.9488 - val_loss: 0.1436 - val_acc: 0.9489\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1398 - acc: 0.9503 - val_loss: 0.1390 - val_acc: 0.9515\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1398 - acc: 0.9500 - val_loss: 0.1336 - val_acc: 0.9528\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1392 - acc: 0.9502 - val_loss: 0.1363 - val_acc: 0.9522\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1365 - acc: 0.9507 - val_loss: 0.1312 - val_acc: 0.9537\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1373 - acc: 0.9510 - val_loss: 0.1382 - val_acc: 0.9511\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1430 - acc: 0.9488 - val_loss: 0.2467 - val_acc: 0.9225\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 2.6167 - acc: 0.8856 - val_loss: 0.1573 - val_acc: 0.9443\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1542 - acc: 0.9445 - val_loss: 0.1498 - val_acc: 0.9470\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1473 - acc: 0.9471 - val_loss: 0.1441 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1418 - acc: 0.9496 - val_loss: 0.1369 - val_acc: 0.9519\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1375 - acc: 0.9507 - val_loss: 0.1393 - val_acc: 0.9507\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1365 - acc: 0.9512 - val_loss: 0.1358 - val_acc: 0.9520\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1356 - acc: 0.9518 - val_loss: 0.1339 - val_acc: 0.9528\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1353 - acc: 0.9518 - val_loss: 0.1321 - val_acc: 0.9528\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 00033: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 12.7169 - acc: 0.8267 - val_loss: 0.6683 - val_acc: 0.8970\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.6917 - acc: 0.8850 - val_loss: 0.4355 - val_acc: 0.9114\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.3231 - acc: 0.9153 - val_loss: 0.2689 - val_acc: 0.9088\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2755 - acc: 0.9197 - val_loss: 0.1784 - val_acc: 0.9440\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.1882 - acc: 0.9362 - val_loss: 0.1800 - val_acc: 0.9413\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.1820 - acc: 0.9370 - val_loss: 0.1695 - val_acc: 0.9422\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1637 - acc: 0.9428 - val_loss: 0.1634 - val_acc: 0.9427\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.2158 - acc: 0.9322 - val_loss: 0.1849 - val_acc: 0.9420\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1829 - acc: 0.9384 - val_loss: 0.1491 - val_acc: 0.9488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1566 - acc: 0.9453 - val_loss: 0.1458 - val_acc: 0.9489\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1621 - acc: 0.9436 - val_loss: 0.1445 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1484 - acc: 0.9479 - val_loss: 0.1408 - val_acc: 0.9502\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1556 - acc: 0.9454 - val_loss: 0.1405 - val_acc: 0.9495\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1439 - acc: 0.9490 - val_loss: 0.1706 - val_acc: 0.9403\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1501 - acc: 0.9468 - val_loss: 0.1366 - val_acc: 0.9515\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1497 - acc: 0.9464 - val_loss: 0.1390 - val_acc: 0.9514\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1435 - acc: 0.9491 - val_loss: 0.1746 - val_acc: 0.9369\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1503 - acc: 0.9467 - val_loss: 0.1461 - val_acc: 0.9481\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1438 - acc: 0.9488 - val_loss: 0.2094 - val_acc: 0.9251\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1450 - acc: 0.9480 - val_loss: 0.1410 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1359 - acc: 0.9513 - val_loss: 0.1393 - val_acc: 0.9505\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1356 - acc: 0.9513 - val_loss: 0.1870 - val_acc: 0.9316\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1355 - acc: 0.9513 - val_loss: 0.1302 - val_acc: 0.9532\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1353 - acc: 0.9515 - val_loss: 0.1695 - val_acc: 0.9412\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1348 - acc: 0.9519 - val_loss: 0.1446 - val_acc: 0.9478\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1364 - acc: 0.9513 - val_loss: 0.1315 - val_acc: 0.9532\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1400 - acc: 0.9496 - val_loss: 0.1510 - val_acc: 0.9475\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1381 - acc: 0.9507 - val_loss: 0.1446 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1299 - acc: 0.9538 - val_loss: 0.1294 - val_acc: 0.9537\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1295 - acc: 0.9534 - val_loss: 0.1327 - val_acc: 0.9530\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1323 - acc: 0.9523 - val_loss: 0.1298 - val_acc: 0.9534\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1318 - acc: 0.9527 - val_loss: 0.1313 - val_acc: 0.9529\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1334 - acc: 0.9518 - val_loss: 0.1377 - val_acc: 0.9511\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1301 - acc: 0.9529 - val_loss: 0.1284 - val_acc: 0.9535\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1313 - acc: 0.9530 - val_loss: 0.1271 - val_acc: 0.9538\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1314 - acc: 0.9526 - val_loss: 0.1591 - val_acc: 0.9437\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1340 - acc: 0.9515 - val_loss: 0.1312 - val_acc: 0.9528\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1321 - acc: 0.9527 - val_loss: 0.1313 - val_acc: 0.9528\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1314 - acc: 0.9524 - val_loss: 0.1279 - val_acc: 0.9535\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1306 - acc: 0.9527 - val_loss: 0.1364 - val_acc: 0.9517\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1265 - acc: 0.9541 - val_loss: 0.1238 - val_acc: 0.9552\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1274 - acc: 0.9538 - val_loss: 0.1231 - val_acc: 0.9555\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1263 - acc: 0.9547 - val_loss: 0.1271 - val_acc: 0.9543\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1266 - acc: 0.9541 - val_loss: 0.1315 - val_acc: 0.9528\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1259 - acc: 0.9548 - val_loss: 0.1264 - val_acc: 0.9545\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1262 - acc: 0.9546 - val_loss: 0.1321 - val_acc: 0.9523\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1264 - acc: 0.9541 - val_loss: 0.1351 - val_acc: 0.9511\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1227 - acc: 0.9554 - val_loss: 0.1248 - val_acc: 0.9544\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1225 - acc: 0.9554 - val_loss: 0.1236 - val_acc: 0.9554\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1233 - acc: 0.9553 - val_loss: 0.1231 - val_acc: 0.9552\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.1239 - acc: 0.9552 - val_loss: 0.1247 - val_acc: 0.9547\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.1229 - acc: 0.9553 - val_loss: 0.1314 - val_acc: 0.9528\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 00052: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 - 2s - loss: 9.6024 - acc: 0.8356 - val_loss: 0.2823 - val_acc: 0.9267\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.4638 - acc: 0.8928 - val_loss: 0.4645 - val_acc: 0.8629\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.2802 - acc: 0.9158 - val_loss: 0.1732 - val_acc: 0.9390\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2246 - acc: 0.9257 - val_loss: 0.2279 - val_acc: 0.9158\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.2158 - acc: 0.9287 - val_loss: 0.1673 - val_acc: 0.9386\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.2223 - acc: 0.9279 - val_loss: 0.1974 - val_acc: 0.9231\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1628 - acc: 0.9418 - val_loss: 0.1553 - val_acc: 0.9456\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1533 - acc: 0.9454 - val_loss: 0.1505 - val_acc: 0.9466\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1590 - acc: 0.9436 - val_loss: 0.1553 - val_acc: 0.9452\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1553 - acc: 0.9453 - val_loss: 0.1452 - val_acc: 0.9495\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1481 - acc: 0.9469 - val_loss: 0.1500 - val_acc: 0.9462\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1508 - acc: 0.9463 - val_loss: 0.1628 - val_acc: 0.9405\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1493 - acc: 0.9469 - val_loss: 0.1724 - val_acc: 0.9375\n",
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1484 - acc: 0.9476 - val_loss: 0.1390 - val_acc: 0.9505\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1448 - acc: 0.9483 - val_loss: 0.1390 - val_acc: 0.9499\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1466 - acc: 0.9477 - val_loss: 0.1376 - val_acc: 0.9510\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1442 - acc: 0.9489 - val_loss: 0.1432 - val_acc: 0.9499\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1468 - acc: 0.9474 - val_loss: 0.1646 - val_acc: 0.9420\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1463 - acc: 0.9483 - val_loss: 0.1452 - val_acc: 0.9483\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1376 - acc: 0.9508 - val_loss: 0.1400 - val_acc: 0.9510\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1415 - acc: 0.9493 - val_loss: 0.1426 - val_acc: 0.9486\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1328 - acc: 0.9522 - val_loss: 0.1426 - val_acc: 0.9492\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1325 - acc: 0.9522 - val_loss: 0.1339 - val_acc: 0.9524\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1351 - acc: 0.9515 - val_loss: 0.1387 - val_acc: 0.9507\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1361 - acc: 0.9512 - val_loss: 0.1309 - val_acc: 0.9533\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1340 - acc: 0.9518 - val_loss: 0.1448 - val_acc: 0.9468\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1366 - acc: 0.9509 - val_loss: 0.1345 - val_acc: 0.9520\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1333 - acc: 0.9525 - val_loss: 0.1338 - val_acc: 0.9522\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1359 - acc: 0.9514 - val_loss: 0.1483 - val_acc: 0.9462\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1346 - acc: 0.9517 - val_loss: 0.1334 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1301 - acc: 0.9532 - val_loss: 0.1312 - val_acc: 0.9541\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1285 - acc: 0.9541 - val_loss: 0.1349 - val_acc: 0.9547\n",
      "Epoch 33/200\n",
      "198/198 - 2s - loss: 0.1317 - acc: 0.9528 - val_loss: 0.1374 - val_acc: 0.9506\n",
      "Epoch 34/200\n",
      "198/198 - 2s - loss: 0.1291 - acc: 0.9533 - val_loss: 0.1335 - val_acc: 0.9529\n",
      "Epoch 35/200\n",
      "198/198 - 2s - loss: 0.1309 - acc: 0.9528 - val_loss: 0.1304 - val_acc: 0.9542\n",
      "Epoch 36/200\n",
      "198/198 - 2s - loss: 0.1281 - acc: 0.9542 - val_loss: 0.1301 - val_acc: 0.9539\n",
      "Epoch 37/200\n",
      "198/198 - 2s - loss: 0.1287 - acc: 0.9535 - val_loss: 0.1336 - val_acc: 0.9532\n",
      "Epoch 38/200\n",
      "198/198 - 2s - loss: 0.1294 - acc: 0.9531 - val_loss: 0.1312 - val_acc: 0.9530\n",
      "Epoch 39/200\n",
      "198/198 - 2s - loss: 0.1349 - acc: 0.9520 - val_loss: 0.1368 - val_acc: 0.9511\n",
      "Epoch 40/200\n",
      "198/198 - 2s - loss: 0.1290 - acc: 0.9538 - val_loss: 0.1325 - val_acc: 0.9524\n",
      "Epoch 41/200\n",
      "198/198 - 2s - loss: 0.1302 - acc: 0.9533 - val_loss: 0.1329 - val_acc: 0.9527\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 42/200\n",
      "198/198 - 2s - loss: 0.1251 - acc: 0.9549 - val_loss: 0.1417 - val_acc: 0.9491\n",
      "Epoch 43/200\n",
      "198/198 - 2s - loss: 0.1255 - acc: 0.9544 - val_loss: 0.1323 - val_acc: 0.9540\n",
      "Epoch 44/200\n",
      "198/198 - 2s - loss: 0.1271 - acc: 0.9543 - val_loss: 0.1299 - val_acc: 0.9544\n",
      "Epoch 45/200\n",
      "198/198 - 2s - loss: 0.1257 - acc: 0.9546 - val_loss: 0.1323 - val_acc: 0.9527\n",
      "Epoch 46/200\n",
      "198/198 - 2s - loss: 0.1252 - acc: 0.9549 - val_loss: 0.1288 - val_acc: 0.9544\n",
      "Epoch 47/200\n",
      "198/198 - 2s - loss: 0.1254 - acc: 0.9544 - val_loss: 0.1304 - val_acc: 0.9540\n",
      "Epoch 48/200\n",
      "198/198 - 2s - loss: 0.1257 - acc: 0.9544 - val_loss: 0.1267 - val_acc: 0.9550\n",
      "Epoch 49/200\n",
      "198/198 - 2s - loss: 0.1255 - acc: 0.9547 - val_loss: 0.1300 - val_acc: 0.9535\n",
      "Epoch 50/200\n",
      "198/198 - 2s - loss: 0.1244 - acc: 0.9551 - val_loss: 0.1342 - val_acc: 0.9518\n",
      "Epoch 51/200\n",
      "198/198 - 2s - loss: 0.1240 - acc: 0.9553 - val_loss: 0.1267 - val_acc: 0.9549\n",
      "Epoch 52/200\n",
      "198/198 - 2s - loss: 0.1264 - acc: 0.9544 - val_loss: 0.1307 - val_acc: 0.9525\n",
      "Epoch 53/200\n",
      "198/198 - 2s - loss: 0.1252 - acc: 0.9547 - val_loss: 0.1313 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 54/200\n",
      "198/198 - 2s - loss: 0.1214 - acc: 0.9558 - val_loss: 0.1240 - val_acc: 0.9558\n",
      "Epoch 55/200\n",
      "198/198 - 2s - loss: 0.1202 - acc: 0.9565 - val_loss: 0.1269 - val_acc: 0.9544\n",
      "Epoch 56/200\n",
      "198/198 - 2s - loss: 0.1204 - acc: 0.9565 - val_loss: 0.1334 - val_acc: 0.9522\n",
      "Epoch 57/200\n",
      "198/198 - 2s - loss: 0.1204 - acc: 0.9563 - val_loss: 0.1278 - val_acc: 0.9548\n",
      "Epoch 58/200\n",
      "198/198 - 2s - loss: 0.1192 - acc: 0.9568 - val_loss: 0.1274 - val_acc: 0.9541\n",
      "Epoch 59/200\n",
      "198/198 - 2s - loss: 0.1209 - acc: 0.9562 - val_loss: 0.1278 - val_acc: 0.9547\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 60/200\n",
      "198/198 - 2s - loss: 0.1172 - acc: 0.9571 - val_loss: 0.1294 - val_acc: 0.9534\n",
      "Epoch 61/200\n",
      "198/198 - 2s - loss: 0.1178 - acc: 0.9570 - val_loss: 0.1238 - val_acc: 0.9559\n",
      "Epoch 62/200\n",
      "198/198 - 2s - loss: 0.1163 - acc: 0.9580 - val_loss: 0.1308 - val_acc: 0.9537\n",
      "Epoch 63/200\n",
      "198/198 - 2s - loss: 0.1170 - acc: 0.9575 - val_loss: 0.1305 - val_acc: 0.9537\n",
      "Epoch 64/200\n",
      "198/198 - 2s - loss: 0.1167 - acc: 0.9575 - val_loss: 0.1259 - val_acc: 0.9548\n",
      "Epoch 65/200\n",
      "198/198 - 2s - loss: 0.1168 - acc: 0.9574 - val_loss: 0.1274 - val_acc: 0.9540\n",
      "Epoch 66/200\n",
      "198/198 - 2s - loss: 0.1168 - acc: 0.9573 - val_loss: 0.1246 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 67/200\n",
      "198/198 - 2s - loss: 0.1146 - acc: 0.9581 - val_loss: 0.1238 - val_acc: 0.9555\n",
      "Epoch 68/200\n",
      "198/198 - 2s - loss: 0.1132 - acc: 0.9585 - val_loss: 0.1253 - val_acc: 0.9551\n",
      "Epoch 69/200\n",
      "198/198 - 2s - loss: 0.1139 - acc: 0.9585 - val_loss: 0.1245 - val_acc: 0.9556\n",
      "Epoch 70/200\n",
      "198/198 - 2s - loss: 0.1131 - acc: 0.9586 - val_loss: 0.1234 - val_acc: 0.9560\n",
      "Epoch 71/200\n",
      "198/198 - 2s - loss: 0.1131 - acc: 0.9585 - val_loss: 0.1233 - val_acc: 0.9559\n",
      "Epoch 72/200\n",
      "198/198 - 2s - loss: 0.1132 - acc: 0.9587 - val_loss: 0.1236 - val_acc: 0.9558\n",
      "Epoch 73/200\n",
      "198/198 - 2s - loss: 0.1136 - acc: 0.9586 - val_loss: 0.1223 - val_acc: 0.9564\n",
      "Epoch 74/200\n",
      "198/198 - 2s - loss: 0.1129 - acc: 0.9588 - val_loss: 0.1225 - val_acc: 0.9563\n",
      "Epoch 75/200\n",
      "198/198 - 2s - loss: 0.1126 - acc: 0.9582 - val_loss: 0.1247 - val_acc: 0.9549\n",
      "Epoch 76/200\n",
      "198/198 - 2s - loss: 0.1123 - acc: 0.9588 - val_loss: 0.1222 - val_acc: 0.9566\n",
      "Epoch 77/200\n",
      "198/198 - 2s - loss: 0.1127 - acc: 0.9585 - val_loss: 0.1220 - val_acc: 0.9567\n",
      "Epoch 78/200\n",
      "198/198 - 2s - loss: 0.1126 - acc: 0.9588 - val_loss: 0.1359 - val_acc: 0.9513\n",
      "Epoch 79/200\n",
      "198/198 - 2s - loss: 0.1121 - acc: 0.9588 - val_loss: 0.1238 - val_acc: 0.9554\n",
      "Epoch 80/200\n",
      "198/198 - 2s - loss: 0.1118 - acc: 0.9597 - val_loss: 0.1256 - val_acc: 0.9552\n",
      "Epoch 81/200\n",
      "198/198 - 2s - loss: 0.1115 - acc: 0.9591 - val_loss: 0.1244 - val_acc: 0.9555\n",
      "Epoch 82/200\n",
      "198/198 - 2s - loss: 0.1118 - acc: 0.9590 - val_loss: 0.1222 - val_acc: 0.9563\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 83/200\n",
      "198/198 - 2s - loss: 0.1091 - acc: 0.9599 - val_loss: 0.1220 - val_acc: 0.9563\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 - 2s - loss: 0.1092 - acc: 0.9598 - val_loss: 0.1231 - val_acc: 0.9560\n",
      "Epoch 85/200\n",
      "198/198 - 2s - loss: 0.1088 - acc: 0.9598 - val_loss: 0.1226 - val_acc: 0.9568\n",
      "Epoch 86/200\n",
      "198/198 - 2s - loss: 0.1088 - acc: 0.9600 - val_loss: 0.1211 - val_acc: 0.9567\n",
      "Epoch 87/200\n",
      "198/198 - 2s - loss: 0.1083 - acc: 0.9601 - val_loss: 0.1227 - val_acc: 0.9561\n",
      "Epoch 88/200\n",
      "198/198 - 2s - loss: 0.1082 - acc: 0.9598 - val_loss: 0.1232 - val_acc: 0.9563\n",
      "Epoch 89/200\n",
      "198/198 - 2s - loss: 0.1078 - acc: 0.9602 - val_loss: 0.1215 - val_acc: 0.9566\n",
      "Epoch 90/200\n",
      "198/198 - 2s - loss: 0.1085 - acc: 0.9601 - val_loss: 0.1210 - val_acc: 0.9567\n",
      "Epoch 91/200\n",
      "198/198 - 2s - loss: 0.1078 - acc: 0.9602 - val_loss: 0.1218 - val_acc: 0.9567\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 92/200\n",
      "198/198 - 2s - loss: 0.1062 - acc: 0.9609 - val_loss: 0.1211 - val_acc: 0.9568\n",
      "Epoch 93/200\n",
      "198/198 - 2s - loss: 0.1060 - acc: 0.9609 - val_loss: 0.1229 - val_acc: 0.9564\n",
      "Epoch 94/200\n",
      "198/198 - 2s - loss: 0.1059 - acc: 0.9610 - val_loss: 0.1221 - val_acc: 0.9562\n",
      "Epoch 95/200\n",
      "198/198 - 2s - loss: 0.1061 - acc: 0.9608 - val_loss: 0.1221 - val_acc: 0.9565\n",
      "Epoch 96/200\n",
      "198/198 - 2s - loss: 0.1057 - acc: 0.9609 - val_loss: 0.1213 - val_acc: 0.9566\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1.5848932274101303e-05.\n",
      "Epoch 97/200\n",
      "198/198 - 2s - loss: 0.1048 - acc: 0.9613 - val_loss: 0.1216 - val_acc: 0.9562\n",
      "Epoch 98/200\n",
      "198/198 - 2s - loss: 0.1048 - acc: 0.9613 - val_loss: 0.1217 - val_acc: 0.9562\n",
      "Epoch 99/200\n",
      "198/198 - 2s - loss: 0.1047 - acc: 0.9613 - val_loss: 0.1208 - val_acc: 0.9570\n",
      "Epoch 100/200\n",
      "198/198 - 2s - loss: 0.1046 - acc: 0.9613 - val_loss: 0.1207 - val_acc: 0.9570\n",
      "Epoch 101/200\n",
      "198/198 - 2s - loss: 0.1045 - acc: 0.9614 - val_loss: 0.1216 - val_acc: 0.9568\n",
      "Epoch 102/200\n",
      "198/198 - 2s - loss: 0.1043 - acc: 0.9616 - val_loss: 0.1209 - val_acc: 0.9570\n",
      "Epoch 103/200\n",
      "198/198 - 2s - loss: 0.1042 - acc: 0.9616 - val_loss: 0.1212 - val_acc: 0.9567\n",
      "Epoch 104/200\n",
      "198/198 - 2s - loss: 0.1040 - acc: 0.9613 - val_loss: 0.1215 - val_acc: 0.9565\n",
      "\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 1.0000000608891671e-05.\n",
      "Epoch 105/200\n",
      "198/198 - 2s - loss: 0.1034 - acc: 0.9619 - val_loss: 0.1208 - val_acc: 0.9569\n",
      "Epoch 106/200\n",
      "198/198 - 2s - loss: 0.1033 - acc: 0.9617 - val_loss: 0.1208 - val_acc: 0.9571\n",
      "Epoch 107/200\n",
      "198/198 - 2s - loss: 0.1033 - acc: 0.9618 - val_loss: 0.1209 - val_acc: 0.9570\n",
      "Epoch 108/200\n",
      "198/198 - 2s - loss: 0.1032 - acc: 0.9618 - val_loss: 0.1209 - val_acc: 0.9567\n",
      "Epoch 109/200\n",
      "198/198 - 2s - loss: 0.1033 - acc: 0.9617 - val_loss: 0.1209 - val_acc: 0.9566\n",
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 110/200\n",
      "198/198 - 2s - loss: 0.1030 - acc: 0.9618 - val_loss: 0.1212 - val_acc: 0.9568\n",
      "Epoch 00110: early stopping\n",
      "signal_1 data shape: (173270, 50, 3)\n",
      "signal_2 data shape: (155841, 50, 3)\n",
      "shape of X: (329111, 150)\n",
      "shape of Y: (329111,)\n",
      "Weight for background: 0.95\n",
      "Weight for signal: 1.06\n",
      "Finished preprocessing\n",
      "shape of X: (329111, 50, 3)\n",
      "shape of Y: (329111,)\n",
      "Model summary:\n",
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "198/198 - 2s - loss: 10.6520 - acc: 0.8373 - val_loss: 4.9211 - val_acc: 0.7643\n",
      "Epoch 2/200\n",
      "198/198 - 2s - loss: 0.7780 - acc: 0.8885 - val_loss: 0.3553 - val_acc: 0.9194\n",
      "Epoch 3/200\n",
      "198/198 - 2s - loss: 0.2835 - acc: 0.9205 - val_loss: 0.2744 - val_acc: 0.9182\n",
      "Epoch 4/200\n",
      "198/198 - 2s - loss: 0.2557 - acc: 0.9225 - val_loss: 0.1641 - val_acc: 0.9460\n",
      "Epoch 5/200\n",
      "198/198 - 2s - loss: 0.2966 - acc: 0.9192 - val_loss: 0.9086 - val_acc: 0.8342\n",
      "Epoch 6/200\n",
      "198/198 - 2s - loss: 0.2975 - acc: 0.9197 - val_loss: 0.1691 - val_acc: 0.9429\n",
      "Epoch 7/200\n",
      "198/198 - 2s - loss: 0.1638 - acc: 0.9426 - val_loss: 0.1504 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "198/198 - 2s - loss: 0.1555 - acc: 0.9449 - val_loss: 0.1568 - val_acc: 0.9460\n",
      "Epoch 9/200\n",
      "198/198 - 2s - loss: 0.1612 - acc: 0.9430 - val_loss: 0.1611 - val_acc: 0.9457\n",
      "Epoch 10/200\n",
      "198/198 - 2s - loss: 0.1525 - acc: 0.9461 - val_loss: 0.1419 - val_acc: 0.9507\n",
      "Epoch 11/200\n",
      "198/198 - 2s - loss: 0.1474 - acc: 0.9474 - val_loss: 0.1553 - val_acc: 0.9464\n",
      "Epoch 12/200\n",
      "198/198 - 2s - loss: 0.1543 - acc: 0.9456 - val_loss: 0.1438 - val_acc: 0.9500\n",
      "Epoch 13/200\n",
      "198/198 - 2s - loss: 0.1484 - acc: 0.9476 - val_loss: 0.1434 - val_acc: 0.9506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/200\n",
      "198/198 - 2s - loss: 0.1458 - acc: 0.9485 - val_loss: 0.1479 - val_acc: 0.9497\n",
      "Epoch 15/200\n",
      "198/198 - 2s - loss: 0.1440 - acc: 0.9487 - val_loss: 0.1739 - val_acc: 0.9382\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 16/200\n",
      "198/198 - 2s - loss: 0.1384 - acc: 0.9505 - val_loss: 0.1491 - val_acc: 0.9468\n",
      "Epoch 17/200\n",
      "198/198 - 2s - loss: 0.1383 - acc: 0.9506 - val_loss: 0.1370 - val_acc: 0.9523\n",
      "Epoch 18/200\n",
      "198/198 - 2s - loss: 0.1374 - acc: 0.9508 - val_loss: 0.1412 - val_acc: 0.9499\n",
      "Epoch 19/200\n",
      "198/198 - 2s - loss: 0.1380 - acc: 0.9509 - val_loss: 0.1416 - val_acc: 0.9518\n",
      "Epoch 20/200\n",
      "198/198 - 2s - loss: 0.1358 - acc: 0.9515 - val_loss: 0.1440 - val_acc: 0.9492\n",
      "Epoch 21/200\n",
      "198/198 - 2s - loss: 0.1385 - acc: 0.9504 - val_loss: 0.1333 - val_acc: 0.9531\n",
      "Epoch 22/200\n",
      "198/198 - 2s - loss: 0.1402 - acc: 0.9497 - val_loss: 0.1354 - val_acc: 0.9531\n",
      "Epoch 23/200\n",
      "198/198 - 2s - loss: 0.1351 - acc: 0.9512 - val_loss: 0.1413 - val_acc: 0.9493\n",
      "Epoch 24/200\n",
      "198/198 - 2s - loss: 0.1417 - acc: 0.9491 - val_loss: 0.1407 - val_acc: 0.9506\n",
      "Epoch 25/200\n",
      "198/198 - 2s - loss: 0.1370 - acc: 0.9509 - val_loss: 0.1359 - val_acc: 0.9521\n",
      "Epoch 26/200\n",
      "198/198 - 2s - loss: 0.1368 - acc: 0.9510 - val_loss: 0.1560 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 27/200\n",
      "198/198 - 2s - loss: 0.1302 - acc: 0.9530 - val_loss: 0.1379 - val_acc: 0.9513\n",
      "Epoch 28/200\n",
      "198/198 - 2s - loss: 0.1312 - acc: 0.9530 - val_loss: 0.1347 - val_acc: 0.9530\n",
      "Epoch 29/200\n",
      "198/198 - 2s - loss: 0.1355 - acc: 0.9512 - val_loss: 0.1368 - val_acc: 0.9516\n",
      "Epoch 30/200\n",
      "198/198 - 2s - loss: 0.1335 - acc: 0.9522 - val_loss: 0.1309 - val_acc: 0.9538\n",
      "Epoch 31/200\n",
      "198/198 - 2s - loss: 0.1321 - acc: 0.9521 - val_loss: 0.1310 - val_acc: 0.9548\n",
      "Epoch 32/200\n",
      "198/198 - 2s - loss: 0.1314 - acc: 0.9529 - val_loss: 0.1309 - val_acc: 0.9539\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for signal_1, signal_2 in zip(signal_1_recons,signal_2_recons):\n",
    "    result.append(train_pfn(signal_1, signal_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3697547",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_res = []\n",
    "for i, (pfn, hist, data) in enumerate(result):\n",
    "    preds = pfn.predict(data[1][2], batch_size=10000)\n",
    "    pfn_fp, pfn_tp, threshs = roc_curve(data[1][-1][:,1], preds[:,1])\n",
    "    roc_res.append([pfn_fp, pfn_tp, threshs])\n",
    "    auc = roc_auc_score(data[1][-1][:,1], preds[:,1])\n",
    "    print('reconstruct PFN AUC(beta = {:.2f}):{}'.format(new_betas[i],auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dbe3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"auc.npz\", beta= new_betas, roc=roc_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
